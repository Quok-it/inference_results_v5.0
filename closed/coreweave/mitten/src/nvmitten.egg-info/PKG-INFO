Metadata-Version: 2.2
Name: nvmitten
Version: 0.2.0
Summary: MLPerf Inference Test Bench from NVIDIA
Author: A. Cheng, NVIDIA
Author-email: alicheng@nvidia.com
License: Apache 2.0 License
Requires-Python: >=3.8
Description-Content-Type: text/markdown; charset=UTF-8
License-File: LICENSE
Requires-Dist: graphlib_backport>=1.0.3; python_version < "3.9"
Requires-Dist: requests>=2.28.1
Requires-Dist: tqdm>=4.65.0
Requires-Dist: numpy<1.24.0,>=1.22.0
Requires-Dist: GitPython>=3.1.31
Requires-Dist: pandas
Requires-Dist: opencv-python
Requires-Dist: pybind11
Requires-Dist: onnx
Provides-Extra: test
Requires-Dist: pytest>=6.2.4; extra == "test"
Requires-Dist: pycodestyle; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Requires-Dist: pylint; extra == "test"
Requires-Dist: pyfakefs; extra == "test"
Requires-Dist: mock; extra == "test"

# MLPerf Inference Test Bench

**M**LPerf **I**nference **T**es**t** B**en**ch, or Mitten, is a framework by NVIDIA to run the [MLPerf Inference
benchmark](https://github.com/mlcommons/inference).


## Features

Mitten, while more optimized for NVIDIA GPU-based systems, is a generic framework that supports arbitrary systems. Some
of the things that Mitten handles are:

- System hardware detection
- Describing and running a benchmark as a pipeline
- Building TRT engines from various sources
- Executing C++ or other compiled executables as a pipeline operation inside Python via pybind11
- Automatic debugging logs and artifacts


### Planned Features

- Easier method of configuring pipelines
- Server-client system for benchmarking workloads over a network connection
