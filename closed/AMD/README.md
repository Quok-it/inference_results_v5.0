# MLPerf Inference v5.0 AMD-Optimized Implementations
This is a repository of AMD-optimized implementations for the [MLPerf](https://mlcommons.org/en/) Inference Benchmark.

## Llama2-70b 
Instructions are here for [Offline](AMD/measurements/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline/README.md) and [Server](AMD/measurements/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline/README.md). Please execute from this directory.

## SD-XL
Instructions are [here](AMD/code/stable-diffusion-xl).

## More Information
You may want the following useful: [How to Reproduce](https://rocm.blogs.amd.com/artificial-intelligence/reproducing-amd-mlperf-inference-submission/README.html) and [More Technical Information](https://rocm.blogs.amd.com/artificial-intelligence/mi325x-accelerates-mlperf-inference/README.html). 
