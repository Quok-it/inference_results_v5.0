diff --git a/include/dgl/runtime/ndarray.h b/include/dgl/runtime/ndarray.h
index 40bbbed3..e31f62da 100644
--- a/include/dgl/runtime/ndarray.h
+++ b/include/dgl/runtime/ndarray.h
@@ -30,6 +30,7 @@
 
 // forward declaration
 inline std::ostream& operator<<(std::ostream& os, DGLDataType t);
+typedef BFloat16 bfloat16;
 
 namespace dgl {
 
@@ -55,6 +56,7 @@ GEN_DGLDATATYPETRAITS_FOR(int32_t, kDGLInt, 32);
 GEN_DGLDATATYPETRAITS_FOR(int64_t, kDGLInt, 64);
 // XXX(BarclayII) most DL frameworks do not support unsigned int and long
 // arrays, so I'm just converting uints to signed DTypes.
+GEN_DGLDATATYPETRAITS_FOR(uint16_t, kDGLInt, 16);
 GEN_DGLDATATYPETRAITS_FOR(uint32_t, kDGLInt, 32);
 GEN_DGLDATATYPETRAITS_FOR(uint64_t, kDGLInt, 64);
 #ifdef DGL_USE_CUDA
@@ -62,6 +64,8 @@ GEN_DGLDATATYPETRAITS_FOR(__half, kDGLFloat, 16);
 #if BF16_ENABLED
 GEN_DGLDATATYPETRAITS_FOR(__nv_bfloat16, kDGLBfloat, 16);
 #endif  // BF16_ENABLED
+#else
+GEN_DGLDATATYPETRAITS_FOR(BFloat16, kDGLBfloat, 16);
 #endif  // DGL_USE_CUDA
 GEN_DGLDATATYPETRAITS_FOR(float, kDGLFloat, 32);
 GEN_DGLDATATYPETRAITS_FOR(double, kDGLFloat, 64);
diff --git a/python/dgl/dataloading/neighbor_sampler.py b/python/dgl/dataloading/neighbor_sampler.py
index 7aa7d8b7..ece2f862 100644
--- a/python/dgl/dataloading/neighbor_sampler.py
+++ b/python/dgl/dataloading/neighbor_sampler.py
@@ -1,5 +1,4 @@
 """Data loading components for neighbor sampling"""
-
 from .. import backend as F
 from ..base import EID, NID
 from ..heterograph import DGLGraph
@@ -151,6 +150,8 @@ class NeighborSampler(BlockSampler):
 
     def sample_blocks(self, g, seed_nodes, exclude_eids=None):
         output_nodes = seed_nodes
+        previous_edges = {}
+        previous_seed_nodes = seed_nodes
         blocks = []
         # sample_neighbors_fused function requires multithreading to be more efficient
         # than sample_neighbors
@@ -193,17 +194,40 @@ class NeighborSampler(BlockSampler):
                 output_device=self.output_device,
                 exclude_edges=exclude_eids,
             )
-            block = to_block(frontier, seed_nodes)
-            # If sampled from graphbolt-backed DistGraph, `EID` may not be in
-            # the block. If not exists, we should remove it from the block.
-            if EID in frontier.edata.keys():
-                block.edata[EID] = frontier.edata[EID]
-            else:
-                del block.edata[EID]
-            seed_nodes = block.srcdata[NID]
+
+            # Before we add the edges, we need to first record the source nodes (of the current seed nodes)
+            # so that other edges' source nodes will not be included as next layer's seed nodes. 
+            temp = to_block(frontier, previous_seed_nodes, include_dst_in_src=False)
+            seed_nodes = temp.srcdata[NID]
+
+            # We add all previously accumulated edges to this subgraph
+            for etype in previous_edges:
+                frontier.add_edges(*previous_edges[etype], etype=etype)
+
+            # This subgraph now contains all its new edges 
+            # and previously accumulated edges
+            # so we add them
+            previous_edges = {}
+            for etype in frontier.etypes:
+                previous_edges[etype] = frontier.edges(etype=etype)
+
+            eid = frontier.edata[EID]
+            # Convert this subgraph to a message flow graph.
+            # we need to turn on the include_dst_in_src
+            # so that we get compatibility with DGL's OOTB GATConv. 
+            block = to_block(frontier, previous_seed_nodes, include_dst_in_src=True)
+            block.edata[EID] = eid
+
+            # for this layers seed nodes - 
+            # they will be our next layers' destination nodes
+            # so we add them to the collection of previous seed nodes. 
+            previous_seed_nodes = block.srcdata[NID]
+
+            # we insert the block to our list of blocks
             blocks.insert(0, block)
+            input_nodes = seed_nodes
 
-        return seed_nodes, output_nodes, blocks
+        return input_nodes, output_nodes, blocks
 
 
 MultiLayerNeighborSampler = NeighborSampler
diff --git a/python/dgl/distgnn/__init__.py b/python/dgl/distgnn/__init__.py
index bc220c59..396fc4bd 100644
--- a/python/dgl/distgnn/__init__.py
+++ b/python/dgl/distgnn/__init__.py
@@ -2,3 +2,13 @@
 This package contains DistGNN and Libra based graph partitioning tools.
 """
 from . import partition, tools
+
+mode = 'orig'
+def set_mode(mode_):
+    global mode
+    mode = mode_
+
+def get_mode():
+    return mode
+
+from . hels import DGLBlockPush
diff --git a/python/dgl/distgnn/communicate.py b/python/dgl/distgnn/communicate.py
new file mode 100644
index 00000000..7359b732
--- /dev/null
+++ b/python/dgl/distgnn/communicate.py
@@ -0,0 +1,166 @@
+import os, psutil, sys, gc, time, random
+from math import ceil, floor
+import dgl
+import torch as th
+from torch import nn
+from torch.nn import functional as F
+import torch.distributed as dist
+
+class communicate():
+    def __init__(self, rank, num_parts):
+        self.rank = rank
+        self.num_parts = num_parts
+
+    #########
+    def all_to_all_s(self, send_list):
+        send_sr = []
+        recv_sr = [th.zeros(1, dtype=th.int64) for i in range(0, self.num_parts)]
+        for i in range(self.num_parts):
+            t_ = th.tensor([send_list[i]], dtype=th.int64)
+            send_sr.append(t_)
+
+        sync_req = dist.all_to_all(recv_sr, send_sr, async_op=True)   # make it async
+        sync_req.wait()
+
+        return send_sr, recv_sr
+
+    ###########
+    def all_to_all_v(self, vector):
+        return
+
+
+def mpi_allreduce(sten, ops=None):
+    op = dist.ReduceOp.SUM
+    if ops == 'max':
+        op = dist.ReduceOp.MAX
+    elif ops == 'min':
+        op = dist.ReduceOp.MIN
+
+    if th.is_tensor(sten):
+        dist.all_reduce(sten, op)
+    else:
+        sten = th.tensor(sten)
+        dist.all_reduce(sten, op)
+
+    return sten.tolist()
+
+def alltoall_s(send_tensor):  ## input is a tensor
+    if not th.is_tensor(send_tensor):
+        send_tensor = th.tensor(send_tensor, dtype=th.int64)
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=th.int64)
+    else:
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=send_tensor.dtype)
+    scount = [1 for i in range(send_tensor.shape[0])]
+    rcount = [1 for i in range(send_tensor.shape[0])]
+    sync_req = dist.all_to_all_single(recv_tensor, send_tensor, rcount, scount, async_op=True)
+    sync_req.wait()
+
+    return send_tensor, recv_tensor
+
+def alltoall_s_async(send_tensor):  ## input is a tensor
+    if not th.is_tensor(send_tensor):
+        send_tensor = th.tensor(send_tensor, dtype=th.int64)
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=th.int64)
+    else:
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=send_tensor.dtype)
+    scount = [1 for i in range(send_tensor.shape[0])]
+    rcount = [1 for i in range(recv_tensor.shape[0])]
+    sync_req = dist.all_to_all_single(recv_tensor, send_tensor, rcount, scount, async_op=True)
+    #sync_req.wait()
+
+    return sync_req, send_tensor, recv_tensor
+
+
+def alltoall_s_exp(send_tensor):  ## input is a tensor
+    if not th.is_tensor(send_tensor):
+        send_tensor = th.tensor(send_tensor, dtype=th.int64)
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=th.int64)
+    else:
+        recv_tensor = th.empty(send_tensor.shape[0], dtype=send_tensor.dtype)
+    scount = [1 for i in range(send_tensor.shape[0])]
+    rcount = [1 for i in range(send_tensor.shape[0])]
+    sync_req = dist.all_to_all_single(recv_tensor, send_tensor, rcount, scount, async_op=True)
+
+    return send_tensor, recv_tensor, sync_req
+
+###########
+def alltoall_v(rdata, sdata, rcount, scount):
+    sync_req = dist.all_to_all_single(rdata, sdata, rcount, scount, async_op=True)
+    sync_req.wait()
+
+def alltoall_v_sync(ten, num_ranks):
+    nsend = ten.shape[0]
+    send_size = [nsend for i in range(num_ranks)]
+    send_sr, recv_sr = alltoall_s(send_size)
+ 
+    tsend, trecv = sum(send_sr), sum(recv_sr)  ##recv data
+    assert trecv >= 0
+ 
+    send_data = th.empty(tsend, dtype=ten.dtype)
+    recv_data = th.empty(trecv, dtype=ten.dtype)
+ 
+    offset = 0
+    for i in range(num_ranks):
+        send_data[offset:offset + nsend] = ten
+        offset += nsend
+ 
+    send_sr, recv_sr = send_sr.tolist(), recv_sr.tolist()
+    dist.all_to_all_single(recv_data, send_data, recv_sr, send_sr)
+    return recv_data
+
+def alltoall_v_async(rdata, sdata, rcount, scount):
+    sync_req = dist.all_to_all_single(rdata, sdata, rcount, scount, async_op=True)
+    return sync_req
+
+
+def barrier():
+    dist.barrier()
+
+
+def communicate_data(params):
+    neigh, send_size, xnbn, xrbn,num_parts, rank = params
+
+    piggy_bag = 0
+    send_sr, recv_sr = alltoall_s(send_size)
+
+    tsend, trecv = sum(send_sr), sum(recv_sr)  ##recv data
+    assert trecv >= 0
+
+    feat_size = neigh.shape[0]
+    send_feat = th.empty(tsend * (feat_size + piggy_bag), dtype=neigh.dtype)
+    recv_feat = th.empty(trecv * (feat_size + piggy_bag), dtype=neigh.dtype)
+    send_nodes = th.empty(tsend, dtype=th.int64)
+    recv_nodes = th.empty(trecv, dtype=th.int64)
+
+    offsetv, offseti = 0, 0
+    for np in range(num_parts):
+        index = xnbn[np].int()
+        if int(send_sr[np]) != index.shape[0]:
+            print(np, ' ', int(send_sr[np]),' ' ,index.shape[0], ' ', xnbn[np].shape[0])
+            sys.exit(1)
+
+        assert send_feat.shape[0] >= index.shape[0] * (feat_size + piggy_bag) + offsetv
+        if rank != np:
+            send_feat = gnn_utils.gather_fetatures(neigh, index, offseti, send_feat, offsetv, None, 0)
+        offsetv += int(send_sr[np]) * (feat_size + piggy_bag)
+        scount[np] = int(send_sr[np]) * (feat_size + piggy_bag)
+        rcount[np] = int(recv_sr[np]) * (feat_size + piggy_bag)
+
+    piggy_bag = 0
+    offseti, offsetv = 0, 0
+    for np in range(num_parts):
+        index = th.arange(xrbn[np].shape[0])
+        if rank != np:
+            send_nodes = gather_features(xrbn[np].long(), index, offseti, send_nodes, offsetv, None, piggy_bag)
+        offsetv += int(send_sr[np])
+        scount_nodes[np] = int(send_sr[np])
+        rcount_nodes[np] = int(recv_sr[np])
+
+
+    req2 = dist.all_to_all_single(recv_feat, send_feat, rcount, scount, async_op=True)
+    req2.wait()
+
+    req1 = dist.all_to_all_single(recv_nodes, send_nodes, rcount_nodes, scount_nodes, async_op=True)
+    req1.wait()
+
+    return recv_feat, rcount, recv_nodes, rcount_nodes
diff --git a/python/dgl/distgnn/hels.py b/python/dgl/distgnn/hels.py
new file mode 100644
index 00000000..a76c1325
--- /dev/null
+++ b/python/dgl/distgnn/hels.py
@@ -0,0 +1,923 @@
+## Historical Embedding Protocol Implementation
+## Author: Md. Vasmiuddin <vasimuddin.md@intel.com>
+##
+
+import os, psutil, sys, gc, time, random
+from math import ceil, floor
+import dgl
+import torch as th
+from torch import nn
+from torch.nn import functional as F
+import torch.distributed as dist
+from .. import function as fn
+from ..utils import expand_as_pair, check_eq_shape
+from ..heterograph import DGLGraph as DGLHeteroGraph
+
+from .communicate import *
+from .hels_cache import hels_cache
+from . import set_mode
+
+from .queue import gqueue, gstack
+from tpp_pytorch_extension.gnn.common import gnn_utils
+
+## hist emb comms
+heq_comm1 = gqueue()
+heq_comm2 = gqueue()
+heq_st1 = gqueue()
+heq_st2 = gqueue()
+## fresh emb comms
+q_comm1 = gqueue()
+q_comm2 = gqueue()
+q_st1 = gqueue()
+q_st2 = gqueue()
+hels_stk_comm = gstack()
+
+pq_comm1 = gqueue()
+pq_comm2 = gqueue()
+pq_st1 = gqueue()
+pq_st2 = gqueue()
+
+
+validation=False
+debug = False
+checks=False
+
+class csr_main:
+    def __init__(self):
+        self.s = 0
+        self.d = 0
+        self.indptr = 0
+        self.data = None
+
+## new csr format (defined above) having same interface as csr in spmm
+## for use in tppfying mapped_spmm_emb
+class csr_main_v2:
+    def __init__(self):
+        self.indices = 0
+        self.dst = 0
+        self.indptr = 0
+        self.num_rows = 0
+        self.data = None
+
+
+class csr_():
+    def __init__(self):
+        self.indptr, self.indices = [0], []
+        self.dt = {}
+        self.acc = 0
+
+class cache_stats():
+    def __init__(self, nlayers):
+        self.hits = 0
+        self.exact_hits = 0
+        self.reqs = 0
+        self.mb_nodes, self.mb_bn, self.nmb = 0, 0, 0
+        self.nlayer = nlayers
+        self.nodes_recv, self.nodes_send = 0, 0
+
+    def reset(self):
+        self.hits = 0
+        self.exact_hits = 0
+        self.reqs = 0
+        self.mb_nodes, self.mb_bn, self.nmb = 0, 0, 0
+        self.nodes_recv, self.nodes_send = 0, 0
+
+    def display(self):
+        print("\t| hitrate {:.2f} | he_hits: {} | exact_hits: {} | tot_reqs: {} | "
+        "#send nodes: {} | #recv nodes: {} |".
+              format(self.hits*1.0/(self.reqs + 1), self.hits, self.exact_hits, self.reqs,
+                     self.nodes_send, self.nodes_recv))
+        print('\t \t| Avg, #level nodes: {:.2f} | #level bn nodes: {:.2f} |'.
+              format(self.mb_nodes*1.0/(self.nmb+1), self.mb_bn*1.0/(self.nmb+1)))
+
+
+class timer():
+    def __init__(self, rank, num_parts):
+        self.rank, self.num_parts = rank, num_parts
+        self.convert_tim = 0.0
+        self.init_tim = 0.0
+        self.hels_tim, self.hels_init_tim = 0.0, 0.0
+        self.hels_wait_tim, self.cache_wait_tim = 0.0, 0.0
+        self.local_agg_tim = 0.0
+        self.total_agg_tim = 0.0
+
+        self.cache_in_tim, self.cache_out_tim = 0.0, 0.0
+        self.gather_tim, self.async_comm_tim = 0.0, 0.0
+        self.cache_in_inner_tim, self.map_tim = 0.0, 0.0
+        self.halo_tim, self.r1_tim, self.r2_tim = 0.0, 0.0, 0.0
+        self.fn_tim, self.map_in_tim, self.map_in_tim2 = 0.0, 0.0, 0.0
+        self.alltoall_sync, self.halo_tim = 0.0, 0.0
+        self.cache_out_scatter_tim, self.cache_out_lu_tim = 0.0, 0.0
+        self.he_store_tim = 0.0
+
+    def reset(self):
+        self.convert_tim = 0.0
+        self.init_tim = 0.0
+        self.hels_tim, self.hels_init_tim = 0.0, 0.0
+        self.hels_wait_tim, self.cache_wait_tim = 0.0, 0.0
+        self.local_agg_tim = 0.0
+        self.total_agg_tim = 0.0
+        self.cache_in_tim, self.cache_out_tim = 0.0, 0.0
+        self.cache_in_inner_tim, self.gather_tim = 0.0, 0.0
+        self.async_comm_tim, self.map_tim = 0.0, 0.0
+        self.halo_tim, self.r1_tim, self.r2_tim = 0.0, 0.0, 0.0
+        self.fn_tim, self.map_in_tim, self.map_in_tim2 = 0.0, 0.0, 0.0
+        self.alltoall_sync, self.halo_tim = 0.0, 0.0
+        self.cache_out_scatter_tim, self.cache_out_lu_tim = 0.0, 0.0
+        self.he_store_tim = 0.0
+
+
+    def display(self):
+        print('\n\tconvert time: {:.4f}'.format(self.convert_tim))
+        print('\thels init time: {:.4f}'.format(self.init_tim))
+        print('\ths create halo db time: {:.4f}'.format(self.halo_tim))
+        print()
+        print('\t>> preproc: {:.4f}, hels comm time: {:.4f}'.format(self.hels_init_tim, self.hels_tim))
+        print('\t>> hels r1 (scatter) time: {:.4f}, hels_wait_tim: {:.4f}'.
+              format(self.r1_tim, self.hels_wait_tim))
+        print('\tlocal agg time: {:.4f}'.format(self.local_agg_tim))
+        print('\tfind halo time: {:.4f}'.format(self.halo_tim))
+        print('\t>> he cache send agg time: \t{:.4f}'.format(self.cache_in_tim))
+        print('\t>>> he cache (inner) mapping time: {:.4f}'.format(self.map_tim))
+        print('\t>>>> he cache (inner) map_in time: {:.4f}, {:.4f}, {:.4f}'.format(self.fn_tim, self.map_in_tim, self.map_in_tim2))
+        print('\t>>> he cache (inner) node sampling time: {:.4f}'.format(self.cache_in_inner_tim))
+        print('\t>>> he cache (inner) alltoall barrier time: {:.4f}'.format(self.alltoall_sync))
+        print('\t>>> he cache (inner) gather agg time: {:.4f}'.format(self.gather_tim))
+        print('\t>>> he cache (inner) async comm time: {:.4f}'.format(self.async_comm_tim))
+        print('\t>> he wait time: {:.4f}'.format(self.cache_wait_tim))
+        print('\t>> he recv (scatter) time: {:.4f}'.format(self.r2_tim))
+        print('\t>>> he store  time: {:.4f}'.format(self.he_store_tim))
+        print('\t>> he cache out agg time: {:.4f}'.format(self.cache_out_tim))
+        print('\t>>> he cache out lu time: {:.4f}'.format(self.cache_out_lu_tim))
+        print('\t>>> he cache out scatter time: {:.4f}'.format(self.cache_out_scatter_tim))
+        print('\ttotal agg time: {:.4f}'.format(self.total_agg_tim))
+
+        print(flush=True)
+
+    def dist_gather(self, my_tensor):
+        tensor = th.tensor([self.rank], dtype=th.float32)
+        output = [tensor.clone() for _ in range(self.num_parts)]
+        if self.rank == 0:
+            dist.gather(tensor=my_tensor, gather_list = output, dst=0)
+        else:
+            dist.gather(tensor=my_tensor, gather_list = [], dst=0)
+
+        output = th.tensor(output)
+        return output
+
+    def dis_all(self):
+        cache_out_tim = th.tensor(self.cache_out_tim)
+        cache_in_tim = th.tensor(self.cache_in_tim)
+        alltoall_sync = th.tensor(self.alltoall_sync)
+        r2_tim = th.tensor(self.r2_tim)
+        local_agg_tim = th.tensor(self.local_agg_tim)
+        map_tim = th.tensor(self.map_tim)
+
+        output = self.dist_gather(r2_tim)
+        if self.rank == 0:
+            print()
+            print('he scatter (r2):  | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+        output = self.dist_gather(cache_out_tim)
+        if self.rank == 0:
+            print('cache_out_tim:    | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+        output = self.dist_gather(local_agg_tim)
+        if self.rank == 0:
+            print('local_agg_tim:    | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+        output = self.dist_gather(cache_in_tim)
+        if self.rank == 0:
+            print('cache_in_tim:     | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+        output = self.dist_gather(map_tim)
+        if self.rank == 0:
+            print('map_tim:          | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+        output = self.dist_gather(alltoall_sync)
+        if self.rank == 0:
+            print('alltoall_sync:    | min: {:.4f}, avg: {:.4f}, max: {:.4f}'.
+                  format(float(output.min()), float(output.mean()), float(output.max()) ) )
+
+            print()
+
+
+################################################################
+def hels_master_func(pb, args):
+    hels_obj = hels_master(pb.graph._graph, pb.graph._ntypes, pb.graph._etypes, \
+                                pb.graph._node_frames, pb.graph._edge_frames)
+
+    hels_obj.hels_init(pb, args)
+    return hels_obj
+
+
+class hels_master(DGLHeteroGraph):
+    def hels_init(self, pb, args):
+        set_mode('hels')
+        self.part_method = "metis"
+        self.N = pb.g_orig.number_of_nodes()      ## total nodes in original graph
+        self.rank      = args.rank
+        self.num_parts = pb.num_parts
+        self.nlayers   = args.n_layers
+        self.timer = timer(self.rank, self.num_parts)
+
+        self.feats = pb.node_feats['feat']
+        self.feat_size = self.feats.shape[1]
+
+        self.ncache = args.ncache  ##-5 means cache inf/all cacheable items
+        self.min_life = 0   #min_cahe line life,  Note: comms delay/aync is a factor here
+        self.max_life = args.max_life   ## max cache life
+        self.dl = args.dl   ## delay in push model
+
+        self.layer_itr = 0
+        self.epoch = []
+        self.o2l_map = []
+        self.ocomms = []
+        self.mb_nodes, self.mb_bn = [], []
+        for i in range(self.nlayers):
+            if args.b2b:
+                self.ocomms.append(th.tensor([-100 for j in range(self.N)], dtype=th.int32))
+                self.o2l_map.append(th.tensor([-100 for j in range(self.N)], dtype=th.int64))
+            self.epoch.append(0)
+
+        ## To maintain async history
+        self.cache_size = args.cache_size
+        ncache_lim = ceil(float(args.cache_size)/self.num_parts)
+        assert self.ncache > 0, "ncache should be > 0"
+        assert self.ncache <= ncache_lim, "ncache should be < cache_size/num_parts!"
+
+        self.hels, self.hels_stats = [], []
+        for i in range(self.nlayers):
+            self.hels.append(hels_cache(self, i))  ## s/w managed cache
+            self.hels_stats.append(cache_stats(self.nlayers))
+
+        self.create_sn_bitmap()
+
+        self.enable_hec = args.enable_hec
+        self.b2b =args.b2b
+        if args.enable_hec and self.rank == 0:
+            print('######### Historical embedding usage ENABLED.')
+        elif args.enable_hec == False and self.rank == 0:
+            print('######### Historical embedding usage DISABLED.')
+        if args.b2b and self.rank == 0:
+            print('######### b2b usage ENABLED.')
+        elif args.b2b == False and self.rank == 0:
+            print('######### b2b usage DISABLED.')
+
+
+    def create_sn_bitmap(self):
+        """
+        """
+        self.ndata['degs'] = self.in_degrees()
+        self.deg_max = self.ndata['degs'].max()
+        self.deg_min = self.ndata['degs'].min()
+        dist.all_reduce(self.deg_max, op=dist.ReduceOp.MAX)
+        dist.all_reduce(self.deg_min, op=dist.ReduceOp.MIN)
+        self.deg_max_lg = int(th.log2(self.deg_max.float()))
+
+        self.ndata['solid'] = th.zeros(self.number_of_nodes(), dtype=th.int32)  ## make it bool
+        db = th.tensor([-100 for i in range(self.N)], dtype=th.int64)
+
+        ninners = (self.ndata['inner_node'] == 1).sum()
+        db[self.ndata['orig'][:ninners]] = th.arange(ninners)
+        self.communicate_halo_nodes_orig()
+        self.sn_db = []
+        offset = 0
+        for np in range(self.num_parts):
+            try:
+                nodes = self.recv_bn[offset : offset + self.recv_bn_count[np]]
+            except:
+                print("Error: recv_bn is not created in communicate_halo_nodes_orig(), exiting..")
+                sys.exit(1)
+
+            if np == self.rank:
+                assert nodes.shape[0] == 0
+            db_nodes = db[nodes.long()]
+            ind = th.nonzero(db_nodes != -100, as_tuple=True)[0]
+            db_nodes = db_nodes[ind]
+            self.ndata['solid'][db_nodes.long()] = 1
+
+            ## Create remote halo nodes database as marker in orig tensor
+            if False:
+                ten = th.zeros(self.N, dtype=th.bool)
+                ten[nodes.long()] = True
+            else:
+                ten = th.zeros(self.N, dtype=th.int32)
+                ten[nodes.long()] = 1
+            self.sn_db.append(ten)  ## change the name to sn_db
+
+        assert self.ndata['solid'][ninners:].sum() == 0
+        self.recv_bn = None
+        self.recv_bn_count = None
+
+
+    def create_sn_bitmap_ext(self):
+        self.communicate_halo_nodes_orig()
+        orig2part = th.tensor([-100 for i in range(self.N)], dtype=th.int64)
+        ninners = (self.ndata['inner_node'] == 1).sum()
+        orig2part[self.ndata['orig'][:ninners]] = th.arange(ninners)
+
+        self.sn_part_db = []
+        offset = 0
+        for np in range(self.num_parts):
+            try:
+                onodes = self.recv_bn[offset : offset + self.recv_bn_count[np]]
+            except:
+                print("Error: recv_bn is not created in communicate_halo_nodes_orig(), exiting..")
+                sys.exit(1)
+
+            if np == self.rank: assert onodes.shape[0] == 0
+            pnodes = orig2part[onodes.long()]
+            ind = th.nonzero(onodes != -100, as_tuple=True)[0]
+            ten = th.tensor([-1 for i in range(self.number_of_nodes())], dtype=th.int32)
+            ten[pnodes[ind]] = onodes[ind]
+            self.sn_part_db.append(ten)
+
+    def printname(self):
+        print('hels file name:', os.path.basename(__file__))
+
+    def reset(self):
+        for i in range(self.nlayers):
+            self.epoch[i] = 0
+            self.hels[i].cache_reset()
+            self.hels_stats[i].reset()
+
+        self.layer_itr = 0
+        self.timer.reset()
+        print('comm size: ', pq_comm1.size())
+        assert pq_comm1.size() == 0
+        assert pq_comm2.size() == 0
+        assert pq_st1.size() == 0
+        assert pq_st1.size() == 0
+        assert pq_st2.size() == 0
+        assert pq_st2.size() == 0
+
+        return
+
+    def profile(self):
+        ## timers
+        if self.rank == 0:
+            self.timer.display()
+            ## stats
+            for i in range(self.nlayers):
+                self.hels_stats[i].display()
+
+        #self.timer.dis_all()
+
+    def finalize(self):
+        if self.rank == 0:
+            print('Cache settings: ')
+            print('hs cache size: ', self.cache_size)
+            print('hs limit on caching per iteration: ', self.ncache, flush=True)
+
+        return
+
+    def find_halo_nodes_orig(self):
+        bn_inner = self.ndata['inner_node']
+        bn_idx = th.nonzero(~(bn_inner.bool()), as_tuple=True)[0]
+
+        bn_part = bn_idx    ## part node id for border nodes
+        bn_orig = self.ndata['orig'][bn_part]    ## orig id for border nodes
+        return bn_orig, bn_part
+
+
+    def communicate_halo_nodes_orig(self):
+        bn_orig, bn_part = self.find_halo_nodes_orig()
+
+        vsize = bn_orig.shape[0]
+        send_size = [vsize for i in range(self.num_parts)]
+        send_size[self.rank] = 0
+        send_sr, recv_sr = alltoall_s(send_size)
+
+        scount = [0 for i in range(self.num_parts)]
+        self.recv_bn_count = [0 for i in range(self.num_parts)]
+        tsend, trecv = sum(send_sr), sum(recv_sr)  ##recv data
+
+        send_bn = th.empty(tsend, dtype=th.int32)
+        self.recv_bn = th.empty(trecv, dtype=th.int32)
+        offset = 0
+        for np in range(self.num_parts):
+            if np != self.rank:
+                send_bn[offset: offset + send_sr[np]] = bn_orig.to(th.int32)
+            else:
+                assert int(send_sr[np]) == 0
+                assert int(recv_sr[np]) == 0
+
+            scount[np] = int(send_sr[np])
+            self.recv_bn_count[np] = int(recv_sr[np])
+            offset += scount[np]
+
+        req = dist.all_to_all_single(self.recv_bn, send_bn,
+                                     self.recv_bn_count, scount,
+                                     async_op=True)
+        req.wait()
+        return req
+
+
+#############################################################################################
+
+class hels_mini_batch(DGLHeteroGraph):
+    def find_solid_nodes_orig(self):
+        lnodes = self.level_nodes
+        assert lnodes.shape[0] > 0
+        #'''
+        sn_solid = self.part.ndata['solid'][lnodes]
+
+        sn_idx = th.nonzero(sn_solid == 1, as_tuple=True)[0]
+
+        sn_part = lnodes[sn_idx]    ## part node id for halo nodes
+
+        sn_orig = self.part.ndata['orig'][sn_part]    ## orig id for halo nodes
+        # sn_batch = self.dstnodes()[sn_idx]
+        sn_batch = self.srcnodes()[sn_idx]
+        '''
+        sn_orig, sn_batch, sn_part = gnn_utils.find_nodes(
+            self.part.ndata['solid'],
+            self.part.ndata['orig'],
+            self.srcnodes(), lnodes, 'solid'
+        )
+        '''
+        return sn_orig, sn_batch, sn_part
+
+    ## Finds different node ids (batch, part-local, part-orig) of layered halo nodes
+    ## Output:
+    ## bn_orig - orig node id of halo nodes in this layer
+    ## bn_part - local node id for halo nodes in this layer
+    ## bn_batch - batch node id for halo nodes in this layer
+    def find_halo_nodes_orig(self):
+        #bn_map = th.zeros(self.part.N, dtype=th.int32) ## partition #nodes
+        lnodes = self.level_nodes
+        #'''
+        bn_inner = self.part.ndata['inner_node'][lnodes]
+        bn_idx = th.nonzero(bn_inner == 0, as_tuple=True)[0]
+
+        bn_part = lnodes[bn_idx]    ## part node id for halo nodes
+        bn_orig = self.part.ndata['orig'][bn_part]    ## orig id for halo nodes
+        # bn_batch = self.dstnodes()[bn_idx]
+        bn_batch = self.srcnodes()[bn_idx]
+        '''
+        bn_orig, bn_batch, bn_part = gnn_utils.find_nodes(
+                self.part.ndata['inner_node'].to(th.int32),
+                self.part.ndata['orig'],
+                self.srcnodes(), lnodes, 'halo'
+        )
+        '''
+        ## validation
+        if validation:
+            assert th.equal(bn_batch, bn_idx) == True
+
+        return bn_orig, bn_part, bn_batch
+
+
+    ## For a given level, bcast the orig id (not bitmap) of my halo nodes
+    def communicate_halo_nodes_orig(self):
+        bn_orig, bn_part, bn_batch = self.find_halo_nodes_orig()
+
+        vsize = bn_orig.shape[0]
+        assert vsize <= self.level_nodes.shape[0]
+        send_size = [vsize for i in range(self.part.num_parts)]
+        send_size[self.rank] = 0
+        send_sr, recv_sr = alltoall_s(send_size)
+
+        #tic = time.time()
+        scount = [0 for i in range(self.part.num_parts)]
+        self.recv_bn_count = [0 for i in range(self.part.num_parts)]
+        tsend = sum(send_sr)
+        trecv = sum(recv_sr)  ##recv data
+
+        ## stats
+        self.hels_stats.reqs += bn_orig.shape[0]
+
+        send_bn = th.empty(tsend, dtype=th.int32)
+        self.recv_bn = th.empty(trecv, dtype=th.int32)
+        offset = 0
+        for np in range(self.part.num_parts):
+            if np != self.rank:
+                send_bn[offset: offset + send_sr[np]] = bn_orig.to(th.int32)
+                if checks:
+                    oid = self.o2l_map[send_bn[offset: offset + send_sr[np]].long()]
+                    ind = th.nonzero(oid == -100, as_tuple=True)[0]
+                    assert ind.shape[0] == 0
+            else:
+                assert int(send_sr[np]) == 0
+                assert int(recv_sr[np]) == 0
+
+            scount[np] = int(send_sr[np])
+            self.recv_bn_count[np] = int(recv_sr[np])
+            offset += scount[np]
+
+        req = dist.all_to_all_single(self.recv_bn, send_bn,
+                                     self.recv_bn_count, scount,
+                                     async_op=True)
+        req.wait()
+        #return req
+
+
+    ## Finds xn of local bn (oid) w/ recv bn (oid)
+    ## Here remote id is original node id
+    def r2l_mapping_orig(self):
+        self.xnbn = []; self.xrbn = []
+        p_node_map = 0; offset = 0
+        for np in range(self.part.num_parts):
+            rbn_orig = self.recv_bn[offset : offset + self.recv_bn_count[np]]
+
+            if False:  ## python code for C/C++ ext code in else
+                l_lid = self.o2l_map[rbn_orig.long()]
+                index = (l_lid != -100).nonzero(as_tuple=True)[0]
+                r_lid2 = rbn_orig[index]
+                l_lid2 = l_lid[index]
+            else:
+                r_lid2, l_lid2 = gnn_utils.r2l_map(self.o2l_map, rbn_orig.long())
+
+            self.xrbn.append(r_lid2.long())  ## remote orig node ids
+            self.xnbn.append(l_lid2.long())  ## corr. part node id
+            offset += self.recv_bn_count[np]
+
+
+    def db_r2l_mapping_orig(self):
+        self.xnbn = []; self.xrbn = []; self.xlbn = []
+        p_node_map = 0; offset = 0
+        tic = time.time()
+        sn_orig, sn_batch, sn_part = self.find_solid_nodes_orig()
+        toc = time.time()
+        self.part.timer.fn_tim += toc - tic
+
+        timer = th.tensor([0], dtype=th.int64)
+
+        for np in range(self.part.num_parts):
+            db_t = self.part.sn_db[np]
+            tic = time.time()
+            r_lid2 = th.empty(0, dtype=th.int64)
+            b_lid2 = th.empty(0, dtype=th.int64)
+            l_lid2 = th.empty(0, dtype=th.int64)
+            '''
+            l_lid = db_t[sn_orig.long()]   #.to(th.device('cpu'))
+
+            index = th.nonzero(l_lid == True, as_tuple=True)[0]
+            r_lid2 = sn_orig[index]
+            b_lid2 = sn_batch[index]
+            l_lid2 = sn_part[index]
+            '''
+            if np != self.rank:
+                r_lid2, b_lid2, l_lid2 = gnn_utils.db_r2l_map(db_t, sn_orig, sn_batch, sn_part)
+
+            toc = time.time()
+            self.part.timer.map_in_tim2 += toc - tic
+
+            if debug and self.rank == np:
+                assert r_lid2.shape[0] == 0
+
+            self.xrbn.append(r_lid2.long())  ## remote local id
+            self.xnbn.append(b_lid2.long())  ## corr. local batch node id
+            self.xlbn.append(l_lid2.long())  ## corr. local node id
+
+        self.part.timer.map_in_tim += (timer[0] *1.0)/2.4/1e9
+        return self.xnbn, self.xrbn, self.xlbn
+
+
+    def hels_to_remote_cache_send(self):
+        h_emb = self.hels
+        piggy_bag = 0
+        #neigh = self.srcdata['h']
+        neigh = self.feat
+        feat_size = neigh.shape[1]
+
+        scount = [0 for i in range(self.num_parts)]
+        rcount = [0 for i in range(self.num_parts)]
+        scount_nodes = [0 for i in range(self.num_parts)]
+        rcount_nodes = [0 for i in range(self.num_parts)]
+
+        tic = time.time()
+        # xn of remote part nodes and current nodes orig
+        self.db_r2l_mapping_orig()
+
+        toc = time.time()
+        self.part.timer.map_tim += toc - tic
+
+        xnbn, xrbn, xlbn = self.xnbn, self.xrbn, self.xlbn
+        hil, hi, lo = self.part.deg_max_lg, self.part.deg_max, self.part.deg_min
+        thres = self.part.ncache
+
+        timer = th.tensor([0], dtype=th.int64)
+
+        tic = time.time()
+        offset = 0
+        send_size = []
+        for np in range(self.num_parts):
+            if xnbn[np].shape[0] > self.part.ncache:
+                degs = self.part.ndata['degs'][self.xlbn[np]]
+                x, y = gnn_utils.node_sampling(degs, xnbn[np].long(), xrbn[np].long(), hil, thres)
+                xnbn[np] = x
+                xrbn[np] = y
+
+            send_size.append(xnbn[np].shape[0])
+
+        toc = time.time()
+        self.part.timer.cache_in_inner_tim += toc - tic
+
+        tic = time.time()
+        send_size[self.rank] = 0
+        send_sr, recv_sr, sync_req = alltoall_s_exp(send_size)
+
+        toc = time.time()
+        self.part.timer.alltoall_sync += toc - tic
+
+        tic = time.time()
+        tsend= sum(send_sr)  ##recv data
+        send_feat = th.empty(tsend * (feat_size + piggy_bag), dtype=neigh.dtype)
+        send_nodes = th.empty(tsend, dtype=th.int64)
+
+        index = th.tensor([], dtype=th.int64)
+        offsetv, offseti = 0, 0
+        for np in range(self.part.num_parts):
+            offsetv += int(send_sr[np]) * (feat_size + piggy_bag)
+            scount[np] = int(send_sr[np]) * (feat_size + piggy_bag)
+            rcount[np] = int(recv_sr[np]) * (feat_size + piggy_bag)
+
+            index = th.cat((index,xnbn[np].long()))
+        assert scount[self.rank] == 0
+        send_feat_ = gnn_utils.gather_features(neigh, index)
+        send_feat = send_feat_.view(send_feat_.shape[0] * send_feat_.shape[1])
+
+        piggy_bag = 0
+        offseti, offsetv = 0, 0
+        for np in range(self.part.num_parts):
+            index = th.arange(xrbn[np].shape[0])
+            if self.rank != np:
+                gnn_utils.gather_n_store_offset(xrbn[np].long(), index, send_nodes, offseti, offsetv)
+            offsetv += int(send_sr[np])
+            scount_nodes[np] = int(send_sr[np])
+            rcount_nodes[np] = int(recv_sr[np])
+            self.hels_stats.nodes_send += index.shape[0]
+
+        assert scount_nodes[self.rank] == 0
+        toc = time.time()
+        self.part.timer.gather_tim += toc - tic
+
+        tic = time.time()
+        sync_req.wait()
+        trecv = sum(recv_sr)
+        assert trecv >= 0
+        recv_feat = th.empty(trecv * (feat_size + piggy_bag), dtype=neigh.dtype)
+        recv_nodes = th.empty(trecv, dtype=th.int64)
+        for np in range(self.part.num_parts):
+            rcount[np] = int(recv_sr[np]) * (feat_size + piggy_bag)
+            rcount_nodes[np] = int(recv_sr[np])
+
+        req2 = dist.all_to_all_single(recv_feat, send_feat, rcount, scount, async_op=True)
+        pq_comm2.push(req2)
+        pq_st2.push(recv_feat)
+        pq_st2.push(rcount)
+
+        req1 = dist.all_to_all_single(recv_nodes, send_nodes, rcount_nodes, scount_nodes, async_op=True)
+        pq_comm1.push(req1)
+        pq_st1.push(recv_nodes)
+        pq_st1.push(rcount_nodes)
+
+        toc = time.time()
+        self.part.timer.async_comm_tim += toc - tic
+
+
+    def hels_to_remote_cache_recv(self):
+        h_emb = self.hels
+        piggy_bag = 0
+        #neigh = self.srcdata['h']
+        neigh = self.feat
+        feat_size = neigh.shape[1]
+
+        if pq_comm1.empty() or pq_comm2.empty():
+            print(self.part.epoch[self.part.layer_itr], ' ', self.part.layer_itr,
+                  ' Error: Empty queue: q_comm!', ' ', pq_comm1.empty(), ' ', pq_comm2.empty())
+            sys.exit(1)
+
+        tic = time.time()
+        req = pq_comm2.pop()
+        req.wait()
+        req = pq_comm1.pop()
+        req.wait()
+        recv_feat  = pq_st2.pop()
+        rcount  = pq_st2.pop()
+        recv_nodes = pq_st1.pop()
+        rcount_nodes = pq_st1.pop()
+        self.part.timer.cache_wait_tim += time.time() - tic
+
+        if validation:
+            assert th.unique(recv_nodes).shape[0] == recv_nodes.shape[0]
+
+        h_emb.inc_age()
+        offsetn, offsetf = 0, 0
+        for np in range(self.part.num_parts):
+            if self.rank != np:
+                ptr_nodes = recv_nodes[offsetn : offsetn + rcount_nodes[np]]
+                offsetn += rcount_nodes[np]
+                ptr_feat = recv_feat[offsetf : offsetf + rcount[np]]
+                ptr_feat = ptr_feat.view(rcount_nodes[np], feat_size)
+                offsetf += rcount[np]
+
+                data = ptr_feat, ptr_nodes.long(), None
+                tic = time.time()
+                h_emb.cache_store(data)
+                toc = time.time()
+                self.part.timer.he_store_tim += toc - tic
+                self.hels_stats.nodes_recv += rcount_nodes[np]
+            else:
+                assert rcount_nodes[np] == 0
+
+
+    def hels_from_local_cache(self):
+        h_emb = self.hels
+        bn_oid, bn_lid, bn_bid = self.find_halo_nodes_orig()
+        tic = time.time()
+        hits_index, hits_feats, hits_feat_size = \
+            h_emb.fused_lookup_load(bn_oid)  ## piggy_bag is enabled
+        toc = time.time()
+        self.part.timer.cache_out_lu_tim += toc - tic
+
+        self.hels_stats.mb_bn += bn_bid.shape[0]
+        self.hels_stats.reqs += bn_bid.shape[0]
+
+        assert hits_feat_size != 0
+        if hits_index.shape[0] == 0:
+            return
+        if hits_feats.shape[0] == 0:
+            return
+        bn_bid = bn_bid[hits_index]
+
+        feats = hits_feats.view(hits_index.shape[0], hits_feat_size)
+
+        reduction = 0
+        index = bn_bid
+        assert index.shape[0] == feats.shape[0]
+        self.hels_stats.hits += index.shape[0]
+
+        tic = time.time()
+        #gnn_utils.scatter_features(feats, index, self.srcdata['h'], reduction)
+        gnn_utils.scatter_features(feats, index, self.feat, reduction)
+        toc = time.time()
+        self.part.timer.cache_out_scatter_tim += toc - tic
+
+        #self.hels_stats.mb_nodes += self.srcdata['h'].shape[0]
+        self.hels_stats.mb_nodes += self.feat.shape[0]
+        self.hels_stats.nmb += 1
+
+    def o2l_reset(elf):
+        if self.part.b2b:
+            self.o2l_map[self.level_nodes_orig] = -100
+            if checks:
+                val = (self.o2l_map != -100).sum()
+                assert val == 0, "o2l_map is not reset back to -100 completely."
+
+
+
+    def apply_hs_func(self, max_steps, feat_src):
+        self.feat = feat_src
+        if self.part.enable_hec:
+            if self.part.epoch[self.level] >= self.part.dl:
+                assert self.part.layer_itr == self.level
+                tic = time.time()
+                self.hels_to_remote_cache_recv()  ## Async recv to fill local HECs
+                toc = time.time()
+                self.part.timer.r2_tim += toc - tic
+
+                tic_ = time.time()
+                self.hels_from_local_cache()   ## search and get from loca HEC
+                toc_ = time.time()
+                self.part.timer.cache_out_tim += toc_ - tic_
+
+            if self.part.epoch[self.level] < max_steps - self.part.dl:
+                tic_ = time.time()
+                self.hels_to_remote_cache_send()  ## async send to remote HECs
+                toc_ = time.time()
+                self.part.timer.cache_in_tim += toc_ - tic_
+
+        self.part.epoch[self.part.layer_itr] += 1
+        self.part.layer_itr = (self.part.layer_itr + 1) % self.part.nlayers
+
+## ---------------------------------------------------------------------------------------------
+class DGLBlockPush(hels_mini_batch):
+    """Subclass that signifies the graph is a block created from
+    :func:`dgl.to_block`.
+    """
+    # (BarclayII) I'm making a subclass because I don't want to make another version of
+    # serialization that contains the is_block flag.
+    is_block = True
+
+    def init(self, part, input_nodes, level):
+        self.part = part
+        self.input_nodes = input_nodes
+        self.level = level
+        self.rank = part.rank
+        self.num_parts = part.num_parts
+
+        self.level_nodes = input_nodes[self.srcnodes()]  ## level nodes represent lid
+        self.level_nodes_orig = part.ndata['orig'][self.level_nodes]
+
+        self.hels = part.hels[level]   ## hec
+        self.hels_stats = part.hels_stats[level]  ## hec stats
+
+        if self.part.b2b:
+            self.o2l_map = part.o2l_map[level]       ## running hashmap, only one map would do all
+            self.ocomms = part.ocomms[level]
+            self.o2l_map[self.level_nodes_orig] = self.srcnodes()
+
+            tic = time.time()
+            self.create_halo_db_v2()   ## old csr for using coo for s and d
+            self.part.timer.halo_tim += time.time() - tic
+
+
+    def __repr__(self):
+        if len(self.srctypes) == 1 and len(self.dsttypes) == 1 and len(self.etypes) == 1:
+            ret = 'Block(num_src_nodes={srcnode}, num_dst_nodes={dstnode}, num_edges={edge})'
+            return ret.format(
+                srcnode=self.number_of_src_nodes(),
+                dstnode=self.number_of_dst_nodes(),
+                edge=self.number_of_edges())
+        else:
+            ret = ('Block(num_src_nodes={srcnode},\n'
+                   '      num_dst_nodes={dstnode},\n'
+                   '      num_edges={edge},\n'
+                   '      metagraph={meta})')
+            nsrcnode_dict = {ntype : self.number_of_src_nodes(ntype)
+                             for ntype in self.srctypes}
+            ndstnode_dict = {ntype : self.number_of_dst_nodes(ntype)
+                             for ntype in self.dsttypes}
+            nedge_dict = {etype : self.number_of_edges(etype)
+                          for etype in self.canonical_etypes}
+            meta = str(self.metagraph().edges(keys=True))
+            return ret.format(
+                srcnode=nsrcnode_dict, dstnode=ndstnode_dict, edge=nedge_dict, meta=meta)
+
+
+
+    def create_halo_db(self):
+        src = self.srcnodes()
+        ind = th.nonzero(self.part.ndata['inner_node'][self.input_nodes[src]] == 0, as_tuple=True)[0]
+        csr = csr_()
+        pos, acc = 0, 0
+
+        src = src[ind]
+        dout = self.out_edges(src)
+        out, count = th.unique_consecutive(dout[0], return_counts = True)
+        assert out.shape[0] > 0
+        oid = self.part.ndata['orig'][self.input_nodes[out]]
+        csr.indices = dout[1]
+        pos = 0
+        for i in range(out.shape[0]):
+            csr.dt[int(oid[i])] = i
+
+        z = th.zeros(1, dtype=count.dtype)
+        cum = th.cumsum(count, 0)
+        csr.indptr = th.cat((z, cum) )
+
+        self.csr = csr
+
+    def create_halo_db_v2(self):
+        src = self.srcnodes()
+        ind = th.nonzero(self.part.ndata['inner_node'][self.input_nodes[src]] == 0, as_tuple=True)[0]
+        csr = csr_main()
+        src = src[ind]
+        dout = self.out_edges(src)
+        do, index = dout[1].sort()
+        so = dout[0][index]
+        so = self.part.ndata['orig'][self.input_nodes[so]]
+
+        uniq, counts = do.unique_consecutive(return_counts=True)
+        uniq2 = do.unique()
+        assert uniq.shape[0] == uniq2.shape[0]
+
+        indptr = [0]
+        cum = th.cumsum(counts, 0)
+        indptr += cum.tolist()
+        csr.s, csr.d, csr.indptr = so, do, th.tensor(indptr, dtype=th.int64)
+
+        self.csr = csr
+
+    ### New version fo create_halo_db_v3
+    ## in drpa code creates halo db for tppfied mapped_spmm_emb
+    def create_halo_db_v3(self):
+        src = self.srcnodes()
+        ind = th.nonzero(self.part.ndata['inner_node'][self.input_nodes[src]] == 0, as_tuple=True)[0]
+        csr = csr_main_v2()
+        src = src[ind]
+        dout = self.out_edges(src)
+        do, index = dout[1].sort()
+        so = dout[0][index]
+        so = self.part.ndata['orig'][self.input_nodes[so]]
+
+        uniq, counts = do.unique_consecutive(return_counts=True)
+        if checks:
+            uniq2 = do.unique()
+            assert uniq.shape[0] == uniq2.shape[0]
+
+        indptr = [0]
+        cum = th.cumsum(counts, 0)
+        indptr += cum.tolist()
+        assert len(indptr) - 1 == uniq.shape[0]
+        csr.s, csr.d, csr.indptr = so, uniq, th.tensor(indptr, dtype=th.int64)
+
+        self.csrv2 = csr
diff --git a/python/dgl/distgnn/hels_cache.py b/python/dgl/distgnn/hels_cache.py
new file mode 100644
index 00000000..f3954642
--- /dev/null
+++ b/python/dgl/distgnn/hels_cache.py
@@ -0,0 +1,180 @@
+import os, psutil, sys, gc, time, random
+from math import ceil, floor
+import dgl
+import torch as th
+from torch import nn
+from torch.nn import functional as F
+import torch.distributed as dist
+from .. import function as fn
+from ..utils import expand_as_pair, check_eq_shape
+from ..heterograph import DGLGraph as DGLHeteroGraph
+from .communicate import alltoall_s
+from tpp_pytorch_extension.gnn.common import gnn_utils
+
+
+## Cache feats
+class hels_cache:
+    def __init__(self, dgmb, level):
+        self.rank = dgmb.rank
+        #self.num_parts = dgmb.num_parts
+        self.min_life = dgmb.min_life
+        self.life = dgmb.max_life   #5      ## expiry
+        self.cache_size = dgmb.cache_size ## 10
+        self.max_feat_size = 512    ## make a parameter from partition book (pb)
+        self.N = dgmb.N
+        self.feat_size = dgmb.feats.shape[1]
+        self.feat_dtype = dgmb.feats.dtype
+
+        self.piggy_bag = 1
+        self.cache_ptr = 0
+        self.level = level
+        if dgmb.part_method == "metis":
+            self.piggy_bag = 0
+
+        self.batch_id = 0
+        self.cachemap = th.tensor([-200 for i in range(dgmb.N)], dtype=th.int64)
+
+        max_age = self.life + 1
+        self.storage_aux = th.tensor([-99 for i in range(self.cache_size)], dtype = self.feat_dtype)
+        self.storage_deg  =self.storage_aux
+        #self.storage_nodes = th.empty(self.cache_size, dtype = th.int32)
+        self.buf_feats = th.empty(self.cache_size * self.max_feat_size, dtype = self.feat_dtype)
+
+        self.wt = th.tensor([max_age for i in range(self.cache_size)], dtype = th.int32)
+        ## reverse pointer to cachemap which contains oid pointer to cache buffer
+        self.rptr = th.tensor([-1 for i in range(self.cache_size)], dtype = th.int32)
+
+        self.reset_timers()
+
+
+    def reset_timers(self):
+        self.batch_id = 0
+        self.store_tim = 0
+        self.load_tim = 0
+
+    def display_timers(self):
+        print('\tCache, load time: {:.4f} store_time: {:.4f}'.format(self.load_tim, self.store_tim))
+
+    def inc_age(self):
+        self.batch_id += 1
+        self.wt = self.wt + 1
+
+    def sort(self):
+        self.age_index = th.empty([i for i in range(self.cache_size)], dtype = th.int32)
+
+    def set_fs(self, feat_size):
+        self.feat_size = feat_size
+        assert self.feat_size <= self.max_feat_size
+        newl = self.cache_size * self.feat_size
+        self.buf_feats = self.buf_feats[:newl].view(self.cache_size, self.feat_size)
+
+    def check_eq(self, a, b):
+        assert a == b
+
+    def cache_reset(self):
+        if self.rank == 0:
+            self.display_timers()
+
+        self.feat_size = 0
+        max_age = self.life + 1
+        self.wt = th.tensor([max_age for i in range(self.cache_size)], dtype = th.int32)
+        self.rptr = th.tensor([-1 for i in range(self.cache_size)], dtype = th.int32)
+        self.storage_deg = th.tensor([-99 for i in range(self.cache_size)],
+                                     dtype = self.feat_dtype)
+
+        ind = th.nonzero(self.cachemap != -200, as_tuple=True)[0]
+        if self.rank == 0:
+            print('\t> hs cache fill, avg occ: {:.4f}, %occ: {:.2f}'.
+                  format(ind.shape[0], ind.shape[0]*1.0/self.cache_size))
+
+        self.cachemap[ind] = -200
+
+        ## validation
+        ind = th.nonzero(self.cachemap != -200, as_tuple=True)[0]
+        assert ind.shape[0] == 0
+        self.reset_timers()
+
+    def cache_store(self, data):
+        feats, nodes, degs = data
+        tic = time.time()
+        if self.feat_size == 0:
+            self.set_fs(feats.shape[1])
+        else:
+            assert feats.shape[1] == self.feat_size
+
+        cache_ptr_t = th.tensor([self.cache_ptr], dtype=th.int32)
+        hval = -200
+        rval = -1
+        size = self.cache_size - self.cache_ptr
+        if size < 0:
+            self.cache_ptr = 0
+            size = feats.size(0)
+
+        if nodes.shape[0] > 0:
+            cache_data = self.cachemap, self.rptr, self.wt, nodes, self.buf_feats,\
+                     feats, feats[:size][:], feats[size:][:],\
+                     cache_ptr_t, int(self.cache_size), int(hval), int(rval)
+            gnn_utils.cache_store(cache_data)
+            self.cache_ptr = int(cache_ptr_t)
+
+        toc = time.time()
+        self.store_tim += toc - tic
+
+    def cache_lookup(self, oid):
+        if self.feat_size == 0:
+            print('Cache empty....')
+            return
+        tic = time.time()
+        bitval_oid = self.cachemap[oid.long()]
+        inda = th.nonzero(bitval_oid != -200, as_tuple=True)[0]
+        bitval = bitval_oid[inda]
+
+        if self.level == 0:
+            lookup_loc = bitval
+            oid_index = inda
+        else:
+            age = self.wt[bitval]
+            indb = th.nonzero(((age >= self.min_life) & (age <= self.life)), as_tuple=True)[0]
+            lookup_loc = bitval[indb]
+            oid_index = inda[indb]   ## oid with non -200 and life < self.life
+
+        toc = time.time()
+        self.load_tim += toc - tic
+        return oid_index, lookup_loc
+
+
+    def cache_load(self, lookup_loc):
+        if self.feat_size == 0:
+            print('Cache empty....')
+            return
+
+        if lookup_loc.shape[0] > 0:
+            assert self.feat_size != 0
+            feats = self.buf_feats.view(self.cache_size, self.feat_size)
+            gat_feats =  gnn_utils.gather_features(feats, lookup_loc)
+
+            return gat_feats
+        return None
+
+    def fused_lookup_load(self, oid):
+        if self.feat_size == 0:
+            print('Cache empty....')
+            return
+        tic = time.time()
+
+        assert self.feat_size != 0
+        feats = self.buf_feats.view(self.cache_size, self.feat_size)
+        if self.level == 0:
+            oid_index, gat_data = gnn_utils.cache_load(self.cachemap, oid, feats)
+        else:
+            oid_index, gat_data = gnn_utils.cache_load(self.cachemap, oid, feats,\
+                self.wt, self.level, self.min_life, self.life)
+
+        #oid_index2, loc = cache_lookup(oid)
+        #gat_data2 =  gnn_utils.gather_features(feats, lookup_loc)
+        #assert th.allclose(g_orig.ndata['feat'][input_nodes], batch_inputs, atol=0.01) == Tr
+
+        toc = time.time()
+        self.load_tim += toc - tic
+
+        return oid_index, gat_data, self.feat_size
diff --git a/python/dgl/distgnn/iels.py b/python/dgl/distgnn/iels.py
new file mode 100644
index 00000000..2385686a
--- /dev/null
+++ b/python/dgl/distgnn/iels.py
@@ -0,0 +1,629 @@
+## Distributed  mini-batch sampling core codebase
+## Author: Md. Vasmiuddin <vasimuddin.md@intel.com>
+## Parallel Computing Lab, Intel Corporation
+##
+
+import os, psutil, sys, time, random
+from math import ceil, floor
+import numpy as np
+import dgl
+import torch as th
+from torch import nn
+from torch.nn import functional as F
+import torch.distributed as dist
+from .. import function as fn
+from dgl import DGLHeteroGraph
+
+from .communicate import *
+
+from .queue import gqueue, gstack
+from tpp_pytorch_extension.gnn.common import gnn_utils
+from .iels_cache import feat_cache
+
+debug = False
+stats = False
+
+class emb_buffer():
+    def __init__(self, feat_size, dtype):
+        self.feat_size = feat_size
+        self.dtype = dtype
+        self.buf_size = 1000
+        self.feat_buf = th.empty([buf_size, self.feat_size], dtype = self.dtype)
+        self.sn_db_ptr = th.tensor([-1 for i in range(bsize)])
+
+class timer():
+    def __init__(self, rank, num_parts):
+        self.rank, self.num_parts = rank, num_parts
+        self.reset()
+
+    def reset(self):
+        self.ltl, self.cn, self.cf, self.ltr, self.bf = 0.0, 0.0, 0.0, 0.0, 0.0
+        self.find_bn, self.cf_gf, self.imb = 0.0, 0.0, 0.0
+        self.cn_comm, self.cn_wait, self.lgf, self.lsf = 0.0, 0.0, 0.0, 0.0
+        self.cn_comm0, self.cn_wait0 = 0.0, 0.0
+        self.cf_imb, self.cf_comm, self.cf_wait, self.setup = 0.0, 0.0, 0.0, 0.0
+        self.lsetup = 0.0
+        self.nbnodes, self.nmb = 0, 0
+        self.cntypes = [0 for i in range(4)]
+
+    def display(self):
+        print('load tensor local time: {:.4f}'.format(self.ltl))
+        print(' -local setup time: {:.4f}'.format(self.lsetup))
+        print(' -local gf time: {:.4f}'.format(self.lgf))
+        print(' -local sf time: {:.4f}'.format(self.lsf))
+        print('communicate nodes time: {:.4f}'.format(self.cn))
+        print(' -cn comm0 time:       {:.4f}'.format(self.cn_comm0))
+        print(' -cn wait0 time:       {:.4f}'.format(self.cn_wait0))
+        print(' -cn time (async):   {:.4f}'.format(self.cn_comm))
+        print(' -cn_wait time: {:.4f}'.format(self.cn_wait))        
+        print('communicate feat time: {:.4f}'.format(self.cf))
+        print(' - cf_gf time: {:.4f}'.format(self.cf_gf))
+        print(' - cf_imb time: {:.4f}'.format(self.cf_imb))
+        print(' - cf_comm time: {:.4f}'.format(self.cf_comm))
+        print(' - cf_wait time: {:.4f}'.format(self.cf_wait))
+        print('load tensor remote time: {:.4f}'.format(self.ltr))
+        print('buffering feat time: {:.4f}'.format(self.bf))
+        print()
+        print('Number of MB: ', self.nmb)
+        print('MB nodes: ', self.nbnodes)
+
+    def display_glob(self):
+        if self.rank == 0:
+            print('Global times: >>>>>>>>>>>>>>>>>>>>>>')
+        
+        mx, avg = mpi_allreduce(self.cn, 'max'), mpi_allreduce(self.cn)
+        if self.rank == 0:
+            print('comms time (bi, nodes/feats): avg: {:.2f}, max: {:.2f}'.
+                  format(avg/self.num_parts, mx))
+            
+        mx, avg = mpi_allreduce(self.cf_imb, 'max'), mpi_allreduce(self.cf_imb)
+        if self.rank == 0:
+            print('comms time (bi, nodes/feats): avg: {:.2f}, max: {:.2f}'.
+                  format(avg/self.num_parts, mx))
+
+        if self.rank == 0:            
+            print()
+        if self.rank == 0:
+            print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')
+
+
+def iels_master_func(pb, args):
+    iels_obj = iels_master() 
+    
+    iels_obj.iels_init(pb, args)
+    return iels_obj
+
+class iels_master(): 
+    def __init__(self):
+        pass
+    
+    def iels_init(self, pb, args):
+        self.N = pb.g_orig.number_of_nodes() ## total nodes in original graph
+        self.rank      = args.rank
+        self.num_parts = pb.num_parts
+        self.ndata = pb.node_feats
+
+        self.ndtypes = pb.g_orig.ntypes
+        self.ntypes_id, self.acc_onodes, offset = {}, {}, 0
+        for keys, nt in enumerate(pb.g_orig.ntypes):
+            self.ntypes_id[nt] = keys
+            self.acc_onodes[nt] = offset
+            offset += pb.g_orig.num_nodes(nt)
+        self.pn = (self.ndata['inner_node'] == 1).sum()
+        self.feats = self.ndata['feat'][0:self.pn][:]
+        self.scf = self.ndata['scf'][0:self.pn][:] if self.feats.dtype == th.int8 else None
+        self.dtype = self.feats.dtype
+        self.feat_size = self.feats.shape[1]
+        if self.scf is not None:
+            self.scf_size = self.scf.shape[1]
+        self.part_sn_onid = self.ndata['orig'][:self.pn]
+        self.part_sn_gnid = th.tensor(list(th.arange(self.pn)), dtype=th.int32)
+
+        if debug and self.rank == 0:
+            print("sn_onid size: ", self.part_sn_onid.size())
+            print("sn_gnid size: ", self.part_sn_gnid.size())
+
+        ## database to know in which partition the remote ndoes are residing
+        rr = self.create_remote_sn_db_commence()
+        ## local feats db - orig node id to index in the feat table
+        self.create_local_sn_db(self.part_sn_onid, self.part_sn_gnid)  ## at partition level
+        self.create_remote_sn_db(rr)
+
+        ## remote embedding buffering data structures
+        self.enable_iec = args.enable_iec
+        if self.enable_iec:
+            self.buf_thres = args.ncache
+            self.buf_size = args.cache_size
+
+        self.buf_offset, self.buf_base = 0, self.pn
+        if self.enable_iec:
+            self.cache = feat_cache(pb, self.local_sn_db, args)
+            self.feats = self.cache.fuse_buf(self.feats)
+
+        self.timer = timer(self.rank, self.num_parts)
+        ##stats
+        self.sdata, self.rdata, self.snodes, self.rnodes = 0, 0, 0, 0
+
+
+    def printname(self):
+        print('dms file name:', os.path.basename(__file__))
+
+    def profile(self):
+        self.timer.display_glob()
+        if self.rank == 0:
+            self.timer.display()
+            print("#########################################")
+            print("Below numbers are accumulate across all the batches")
+            print('Total send data (elements): ', self.sdata)
+            print('Total recv data (elements): ', self.rdata)
+            print('Total solid  nodes: ', self.snodes)
+            print('Total remote nodes: ', self.rnodes)
+            print("#########################################")
+            if self.enable_iec:
+                self.cache.display()
+
+        snodes_max, snodes_avg = mpi_allreduce(self.snodes, 'max'), mpi_allreduce(self.snodes)
+        rnodes_max, rnodes_avg = mpi_allreduce(self.rnodes, 'max'), mpi_allreduce(self.rnodes)
+        if self.rank == 0:
+            print("################### Global ###")
+            print('solid nodes:  avg: {:.4f}, max: {}'.format(snodes_avg/self.num_parts, snodes_max))
+            print('remote nodes: avg: {:.4f}, max: {}'.format(rnodes_avg/self.num_parts, rnodes_max))
+            print("#########################################")
+
+    def reset(self):
+        self.timer.reset()
+        self.sdata, self.rdata, self.snodes, self.rnodes = 0, 0, 0, 0
+
+    def create_local_sn_db(self, onid, gnid):
+        self.local_sn_db = th.full([self.N], -1,  dtype=th.int32)
+        self.local_sn_db[onid] = gnid
+
+    def create_remote_sn_db_commence(self):
+        return self.communicate_solid_nodes()
+
+    def create_remote_sn_db(self, rr):
+        self.onid_map = th.full([self.N], -1,  dtype=th.int32)
+        self.pid_map = th.full([self.N], -1,  dtype=th.int32)
+
+        rr.wait()
+        offset = 0
+        for np in range(self.num_parts):
+            try:
+                nodes_onid = self.recv_sn_onid[offset : offset + self.recv_sn_count[np]]
+                nodes_gnid = self.recv_sn_gnid[offset : offset + self.recv_sn_count[np]]
+            except:
+                print("Error: recv_bn is not created in communicate_halo_nodes_orig(), exiting..")
+                sys.exit(1)
+
+            if np == self.rank:
+                assert nodes_onid.shape[0] == 0
+
+            self.onid_map[nodes_onid.long()] = nodes_gnid   ## stores gnid
+            self.pid_map[nodes_onid.long()] = np
+            offset += self.recv_sn_count[np]
+
+    def find_solid_nodes(self):
+        bn_part = self.part_sn_gnid  ## part node id for border nodes
+        bn_orig = self.part_sn_onid  ## orig id for border nodes
+        return bn_orig, bn_part
+
+    def communicate_solid_nodes(self):
+        sn_orig, sn_part = self.find_solid_nodes()
+
+        send_size = [sn_part.shape[0] for i in range(self.num_parts)]
+        send_size[self.rank] = 0
+        req, send_sr, recv_sr = alltoall_s_async(send_size)
+
+        send_sn_count = [0 for i in range(self.num_parts)]
+        self.recv_sn_count = [0 for i in range(self.num_parts)]
+        tsend = sum(send_sr)
+
+        send_sn_onid = th.empty(tsend, dtype=th.int32)
+        send_sn_gnid = th.empty(tsend, dtype=th.int32)
+
+        offset = 0
+        for np in range(self.num_parts):
+            if np != self.rank:
+                send_sn_onid[offset: offset + send_sr[np]] = sn_orig.to(th.int32)
+                send_sn_gnid[offset: offset + send_sr[np]] = sn_part.to(th.int32)
+            else:
+                assert int(send_sr[np]) == 0
+
+            send_sn_count[np] = int(send_sr[np])
+            offset += send_sn_count[np]
+
+        req.wait()  ## wait on alltoall_s_async
+
+        for np in range(self.num_parts):
+            self.recv_sn_count[np] = int(recv_sr[np])
+
+        trecv = sum(recv_sr)
+        self.recv_sn_onid = th.empty(trecv, dtype=th.int32)
+        self.recv_sn_gnid = th.empty(trecv, dtype=th.int32)
+
+        assert send_sn_gnid.max() < self.pn
+        req = dist.all_to_all_single(self.recv_sn_gnid, send_sn_gnid,
+                                     self.recv_sn_count, send_sn_count,
+                                     async_op=True)
+        req = dist.all_to_all_single(self.recv_sn_onid, send_sn_onid,
+                                     self.recv_sn_count, send_sn_count,
+                                     async_op=True)
+        return req
+
+    def finalize(self):
+        pass
+
+class ielspp_batch():
+    def __init__(self, part, input_nodes):
+        self.part = part
+        self.input_nodes = input_nodes
+        self.rank = part.rank
+        if self.part.enable_iec:
+            self.buf_thres = part.buf_thres
+            self.buf_size = part.buf_size
+        self.buf_offset = part.buf_offset
+        self.buf_base = part.buf_base
+        self.timer = part.timer
+        self.acc_onodes = part.acc_onodes
+        self.batch_feats = {}
+        self.batch_scf = {}
+        for nt in self.part.ndtypes:
+            self.batch_feats[nt] = None
+            self.batch_scf[nt] = None
+        self.queue = gqueue()
+        self.queue0 = gqueue()
+        self.queue1 = gqueue()
+
+    def dist_sampling(self):
+        pass
+
+    def finalize(self):
+        assert self.queue.size() == 0
+        assert self.queue0.size() == 0
+        assert self.queue1.size() == 0
+
+    def start_comm(self):
+        self.queue.reset()
+        self.queue0.reset()
+        self.queue1.reset()
+
+        tic = time.time()
+        for nt in self.part.ndtypes:
+            self.communicate_remote_bnodesv2(nt)
+        toc = time.time()
+        self.timer.cn += toc - tic
+        
+        for nt in self.part.ndtypes:
+            self.start_comm_(nt)
+
+    def end_comm(self):
+        for nt in self.part.ndtypes:
+            self.end_comm_(nt)
+
+        self.finalize()
+
+    def start_comm_(self, ntype):
+        t0 = time.time()
+
+        ## sandwiching
+        t1 = time.time()
+        self.load_tensor_local_one(ntype)
+        t2 = time.time()
+        self.timer.ltl += t2 - t1
+
+        t0 = time.time()
+        self.communicate_remote_bnodes_featsv2()
+        t1 = time.time()
+        self.timer.cf += t1 - t0
+
+    def end_comm_(self, ntype):
+        #if self.rank == 0 and debug:
+        #    print('batch_feats:<< ', self.batch_feats)
+        #    print('batch_scf:<< ', self.batch_scf)
+
+        send_rn_bnid = self.queue1.pop()
+        send_rn_onid = self.queue1.pop()
+        recv_feats = self.queue.pop()
+        recv_count = self.queue.pop()
+        req = self.queue.pop()
+        if self.part.scf is not None:
+            recv_scfs = self.queue.pop()
+            recv_scf_count = self.queue.pop()
+            reqscf = self.queue.pop()
+        tic = time.time()
+        req.wait()
+        if self.part.scf is not None:
+            reqscf.wait()
+        toc = time.time()
+        self.timer.cf_wait += toc - tic
+        
+        self.part.rdata += sum(recv_count)
+        if self.part.scf is not None:
+            self.part.rdata += sum(recv_scf_count)
+        t2 = time.time()
+        if self.part.scf is None: recv_scfs = None
+        self.load_tensor_remote(recv_feats, recv_scfs, send_rn_bnid, ntype)
+        if self.rank == 0  and debug:
+            print('send rn bnid:> ', send_rn_bnid)
+            print('send rn onid:> ', send_rn_onid)
+            print('batch_feats:> ', self.batch_feats)
+            print('batch_scf:> ', self.batch_scf)
+        t3 = time.time()
+        self.timer.ltr += t3 - t2
+
+        ## buffer remote embeddings for future use
+        if self.part.enable_iec:
+            self.buffer_remote_feats(recv_feats, send_rn_onid)
+        t4 = time.time()
+        self.timer.bf += t4 - t3
+
+    def find_remote_bnodes(self, ntype):
+        if isinstance(self.input_nodes, dict):
+            offset = self.acc_onodes[ntype]
+            rn_batch = th.nonzero(
+                         self.part.local_sn_db[ \
+                             offset + self.input_nodes[ntype] \
+                         ] == -1, as_tuple=True)[0]
+            rn_orig = offset + self.input_nodes[ntype][rn_batch]    ## part node id for halo nodes   
+        else:
+            rn_batch = th.nonzero(
+                         self.part.local_sn_db[ \
+                             self.input_nodes \
+                         ] == -1, as_tuple=True)[0]
+            rn_orig = self.input_nodes[rn_batch]    ## part node id for halo nodes
+
+        return rn_orig, rn_batch
+
+    def load_tensor_remote(self, recv_feats, recv_scfs, send_rn_bnid, ntype):
+        if isinstance(self.input_nodes, dict):
+            rf = recv_feats.view(-1, self.part.feat_size)
+            gnn_utils.scatter_features(rf, send_rn_bnid, self.batch_feats[ntype], 0)
+            if recv_scfs is not None:
+                rscf = recv_scfs.view(-1, self.part.scf_size)
+                gnn_utils.scatter_features(rscf, send_rn_bnid, self.batch_scf[ntype], 0)
+        else:
+            rf = recv_feats.view(-1, self.part.feat_size)
+            gnn_utils.scatter_features(rf, send_rn_bnid, self.batch_feats[ntype], 0)
+            if recv_scfs is not None:
+                rscf = recv_scfs.view(-1, self.part.scf_size)
+                gnn_utils.scatter_features(rscf, send_rn_bnid, self.batch_scf[ntype], 0)
+            
+            if self.rank == 0 and stats:
+                print('recv_feats: ',  recv_feats)
+                print('send_rn_bid: ', send_rn_bnid)
+                print('batch_feats: ', self.batch_feats)
+                print()
+                assert self.lsize + send_rn_bnid.shape[0] == self.input_nodes.shape[0]
+                self.lut[send_rn_bnid] = 1
+                assert (self.lut == -1).sum() == 0
+
+    def buffer_remote_feats(self, recv_feats, send_rn_onid):
+        assert self.buf_thres < self.buf_size
+
+        num_nodes = send_rn_onid.shape[0]
+        ## plain thresholding
+        if send_rn_onid.shape[0] > self.buf_thres:
+            recv_feats = recv_feats[:self.buf_thres, :]
+            send_rn_onid = send_rn_onid[ :self.buf_thres]
+            num_nodes = self.buf_thres
+
+        recv_feats = recv_feats.view(num_nodes, self.part.feat_size)
+        self.part.cache.cache_store(recv_feats, send_rn_onid)
+
+    def get_buffer_lines(self, num_nodes_to_buf):
+        if self.buf_size - self.buf_offset > num_nodes_to_buf:
+            poffset = self.buf_offset
+            self.buf_offset += num_nodes_to_buf
+            index = th.tensor([ i for i in range(poffset, self.buf_offset)], dtype=th.int32)
+        else:
+            index1 = th.tensor([ i for i in range(self.buf_offset, self.buf_size)], dtype=th.int32)
+            self.buf_offset = num_nodes_to_buf - (self.buf_size - self.buf_offset)
+            index2 = th.tensor([ i for i in range(self.buf_offset)], dtype=th.int32)
+            index = th.cat((index1, index2), 0)
+
+        return index
+
+    def load_tensor_local_one(self, ntype):
+        self.timer.nmb += 1
+        if isinstance(self.input_nodes, dict):
+            nnt = self.input_nodes[ntype].shape[0]
+            self.timer.nbnodes += nnt
+            self.batch_feats[ntype] = th.empty([nnt, self.part.feat_size], dtype=self.part.dtype)
+            if self.part.scf is not None:
+                self.batch_scf[ntype] = th.empty([nnt, self.part.scf_size], dtype=th.float32)
+            
+            tic = time.time()
+            offset = self.acc_onodes[ntype]
+            gnid = self.part.local_sn_db[offset + self.input_nodes[ntype]]
+            index = th.nonzero(gnid != -1, as_tuple=True)[0]
+            toc = time.time()
+            self.timer.setup += toc - tic
+            
+            self.part.snodes += int(index.shape[0])
+            tic = time.time()
+            buff = gnn_utils.gather_features(self.part.feats, gnid[index].to(th.long))
+            if self.part.scf is not None:
+                scf = gnn_utils.gather_features(self.part.scf, gnid[index].to(th.long))
+            toc = time.time()            
+            self.timer.lgf += toc - tic
+            
+            tic = time.time()
+            gnn_utils.scatter_features(buff, index.long(), self.batch_feats[ntype], 0)
+            if self.part.scf is not None:
+                gnn_utils.scatter_features(scf, index.long(), self.batch_scf[ntype], 0)
+            toc = time.time()
+            self.timer.lsf += toc - tic
+        else:
+            nnt = self.input_nodes.shape[0]
+            self.timer.nbnodes += nnt
+            self.batch_feats[ntype] = th.empty([nnt, self.part.feat_size], dtype=self.part.dtype)
+            
+            tic = time.time()
+            offset = self.acc_onodes[ntype]
+            assert offset == 0
+            gnid = self.part.local_sn_db[offset + self.input_nodes]
+            index = th.nonzero(gnid != -1, as_tuple=True)[0]
+            toc = time.time()
+            self.timer.setup += toc - tic
+            
+            self.part.snodes += int(index.shape[0])
+            tic = time.time()
+            buff = gnn_utils.gather_features(self.part.feats, gnid[index].to(th.long))
+            if self.part.scf is not None:
+                scf = gnn_utils.gather_features(self.part.scf, gnid[index].to(th.long))
+            toc = time.time()            
+            self.timer.lgf += toc - tic
+            
+            tic = time.time()
+            gnn_utils.scatter_features(buff, index.long(), self.batch_feats[ntype], 0)
+            if self.part.scf is not None:
+                gnn_utils.scatter_features(scf, index.long(), self.batch_scf[ntype], 0)
+            toc = time.time()
+            self.timer.lsf += toc - tic
+
+    def communicate_remote_bnodesv2(self, ntype):
+        tic = time.time()
+        rn_onid, rn_bnid = self.find_remote_bnodes(ntype)
+        toc = time.time()
+        self.timer.find_bn += toc - tic
+
+        ## stats
+        self.part.rnodes += int(rn_onid.shape[0])
+        #if self.rank == 0 and debug:
+        #    print("halo bid: ", rn_bnid.sort())
+
+        rn_gnid = self.part.onid_map[rn_onid]
+        rn_pid = self.part.pid_map[rn_onid]
+
+        if debug:
+            assert th.max(rn_pid) <= self.part.num_parts
+
+        send_rn_count = []
+        send_rn_gnid = th.empty(rn_gnid.shape[0], dtype=self.part.onid_map.dtype)
+        send_rn_bnid = th.empty(rn_gnid.shape[0], dtype=rn_bnid.dtype)
+        send_rn_onid = th.empty(rn_gnid.shape[0], dtype=rn_onid.dtype)
+        mask_lst = []
+        for np in range(self.part.num_parts):
+            if self.rank != np:
+                mask = th.nonzero(rn_pid == np, as_tuple=True)[0]
+                ssize = int(mask.shape[0])
+                send_rn_count.append(ssize)
+                mask_lst.append(mask)
+            else:
+                mask = th.nonzero(rn_pid == np, as_tuple=True)[0]  ## put under debug
+                assert mask.shape[0] == 0
+                ssize = 0
+                send_rn_count.append(ssize)
+                mask_lst.append(mask)
+
+        assert send_rn_gnid.shape[0] == sum(send_rn_count)
+        req, send_sr, recv_sr = alltoall_s_async(send_rn_count)      
+
+        offset, ssize = 0, 0
+        for np in range(self.part.num_parts):
+            if self.rank != np:
+                mask = mask_lst[np] 
+                ssize = int(mask.shape[0])
+
+                send_rn_gnid[offset: offset + ssize] = rn_gnid[mask]
+                send_rn_bnid[offset: offset + ssize] = rn_bnid[mask]
+                send_rn_onid[offset: offset + ssize] = rn_onid[mask]
+            else:
+                mask = mask_lst[np] 
+                assert mask.shape[0] == 0
+                ssize = 0
+
+            offset += ssize
+            
+        toc = time.time()
+        self.timer.cn_comm0 += toc - tic
+
+        tic = time.time()
+        req.wait()
+        tsend, trecv = sum(send_sr), sum(recv_sr)  ##recv data
+        toc = time.time()
+        self.timer.cn_wait0 += toc - tic
+
+        recv_rn_count = []
+        for np in range(self.part.num_parts):
+            recv_rn_count.append(int(recv_sr[np]))
+
+        recv_rn_gnid = th.empty(trecv, dtype=self.part.onid_map.dtype)
+
+        tic = time.time()        
+        req = dist.all_to_all_single(recv_rn_gnid, send_rn_gnid,
+                                     recv_rn_count, send_rn_count,
+                                     async_op=True)
+        self.queue0.push(req)
+        self.queue0.push(recv_rn_gnid)
+        self.queue0.push(recv_rn_count)
+        self.queue0.push(send_rn_count)
+        toc = time.time()
+        self.timer.cn_comm += toc - tic
+
+        self.queue1.push(send_rn_bnid)
+        self.queue1.push(send_rn_onid)
+
+    def communicate_remote_bnodes_featsv2(self):
+
+        tic = time.time()
+        req  =  self.queue0.pop()
+        req.wait()
+        recv_rn_gnid  =  self.queue0.pop()
+        recv_rn_count =  self.queue0.pop()
+        send_rn_count =  self.queue0.pop()
+        toc = time.time()
+        self.timer.cn_wait += toc - tic
+        
+        tic = time.time()
+        sf = gnn_utils.gather_features(self.part.feats, recv_rn_gnid.long())
+        send_feats = sf.view(recv_rn_gnid.shape[0]* self.part.feat_size)
+        if self.part.scf is not None:
+            sscf = gnn_utils.gather_features(self.part.scf, recv_rn_gnid.long())
+            send_scfs = sscf.view(recv_rn_gnid.shape[0]* self.part.scf_size)
+        toc = time.time()
+        self.timer.cf_gf += toc - tic
+
+        tic = time.time()
+        send_count = []
+        recv_count = []
+        send_scf_count = []
+        recv_scf_count = []
+        for np in range(self.part.num_parts):
+            ele = recv_rn_count[np]
+            send_count.append(ele * self.part.feat_size)
+            recv_count.append(send_rn_count[np] * self.part.feat_size)
+
+        if self.part.scf is not None:
+            for np in range(self.part.num_parts):
+                ele = recv_rn_count[np]
+                send_scf_count.append(ele * self.part.scf_size)
+                recv_scf_count.append(send_rn_count[np] * self.part.scf_size)
+
+        recv_feats = th.empty(sum(recv_count), dtype=self.part.dtype)
+        if self.part.scf is not None:
+            recv_scfs = th.empty(sum(recv_scf_count), dtype=th.float32)
+
+        req = dist.all_to_all_single(recv_feats, send_feats,
+                                     recv_count, send_count,
+                                     async_op=True)
+        self.part.sdata += sum(send_count)
+        recv_feats = recv_feats.view(sum(send_rn_count), self.part.feat_size)
+
+        if self.part.scf is not None:
+            reqscf = dist.all_to_all_single(recv_scfs, send_scfs,
+                                         recv_scf_count, send_scf_count,
+                                         async_op=True)
+            self.part.sdata += sum(send_scf_count)
+            recv_scfs = recv_scfs.view(sum(send_rn_count), self.part.scf_size)
+
+        self.queue.push(recv_feats)
+        self.queue.push(recv_count)
+        self.queue.push(req)
+        if self.part.scf is not None:
+            self.queue.push(recv_scfs)
+            self.queue.push(recv_scf_count)
+            self.queue.push(reqscf)
+        toc = time.time()
+        self.timer.cf_comm += toc - tic
+        
diff --git a/python/dgl/distgnn/iels_cache.py b/python/dgl/distgnn/iels_cache.py
new file mode 100644
index 00000000..55c98784
--- /dev/null
+++ b/python/dgl/distgnn/iels_cache.py
@@ -0,0 +1,143 @@
+import os, psutil, sys, gc, time, random
+from math import ceil, floor
+import dgl
+import torch as th
+from torch import nn
+from torch.nn import functional as F
+import torch.distributed as dist
+from tpp_pytorch_extension.gnn.common import gnn_utils
+
+stats=False
+chk = True
+
+class feat_cache:
+    def __init__(self, pb, sn_db, args):
+        self.rank = args.rank
+        self.num_parts = pb.num_parts
+        self.cache_size = args.cache_size ## 10
+        self.N = pb.g_orig.number_of_nodes()      ## total nodes in original graph
+        self.feat_size = pb.node_feats['feat'].shape[1]
+        self.cur_itr = 0
+
+        self.cache_offset, self.base = 0, 0
+        self.n = (pb.graph.ndata['inner_node'] == 1).sum() ##pb.node_feats['feat'].shape[0]
+        self.feat_dtype = pb.node_feats['feat'].dtype
+
+        if sn_db == None:
+            #self.cachemap = th.tensor([-1 for i in range(self.N)], dtype=th.int64)
+            self.cachemap = th.tensor([-1 for i in range(self.N)], dtype=th.int32)
+        else:
+            self.cachemap = sn_db  ##resuse cachemap created by dms
+            if chk:
+                self.immutable = sn_db.clone()
+
+        self.buf_feats = th.empty([self.cache_size, self.feat_size], dtype = self.feat_dtype)
+        self.wt = th.zeros(self.cache_size, dtype=th.int32)
+        ## reverse pointer to cachemap which contains oid pointer to cache buffer
+        self.rptr = th.tensor([-1 for i in range(self.cache_size)], dtype = th.int64)
+
+        self.reset()
+        self.cache_occ = -1
+
+
+    ## fused cache buffer with partition local features
+    def fuse_buf(self, feats):
+        self.buf_feats = th.cat((feats, self.buf_feats), 0)
+        self.base = self.n
+        self.fused = True
+        return self.buf_feats
+
+    def reset(self):
+        self.load_tim, self.store_tim = 0, 0
+
+
+    def display(self):
+        print("load time: {:.4f} ".format(self.load_tim))
+        print("save time: {:.4f}".format( self.store_tim))
+        print('cache size: {:.4f}, occ: {:.4f}'.format(self.cache_size, self.cache_occ))
+        print("############################################")
+
+    def get_cache_lines(self, num_nodes_to_cache):
+        if self.cache_size - self.cache_offset > num_nodes_to_cache:
+            poffset = self.cache_offset
+            self.cache_offset += num_nodes_to_cache
+            index = th.tensor([ i for i in range(poffset, self.cache_offset)], dtype=th.int32)
+        else:
+            index1 = th.tensor([ i for i in range(self.cache_offset, self.cache_size)], dtype=th.int32)
+            self.cache_offset = num_nodes_to_cache - (self.cache_size - self.cache_offset)
+            index2 = th.tensor([ i for i in range(self.cache_offset)], dtype=th.int32)
+            index = th.cat((index1, index2), 0)
+
+        return index
+
+
+    def sample_nodes(self, nodes, n, sample='random'):
+        assert nodes.shape[0] >= n
+        if sample == 'naive':
+            return nodes[:n], th.arange(0, n)
+        elif sample == 'random':
+            #random_values = random.sample(range(nodes.shape[0]), n)
+            random_values = th.randint(0, nodes.shape[0], (n,))
+            return nodes[random_values], random_values
+
+
+    def cache_store(self, feats, nodes, policy='naive'):
+        assert nodes.shape[0] < self.cache_size, 'Insufficient cache size.'
+
+        tic = time.time()
+        if policy == 'naive':
+            clines = self.get_cache_lines(nodes.shape[0])
+
+        elif policy == 'lru':
+            ## lru
+            val, ind = th.sort(self.wt)
+            clines = ind[:nodes.shape[0]]
+        else:
+            print('Error: Incorrect cache store policy!')
+
+        ## reset selected clines cachemap
+        rptr = self.rptr[clines]
+        ind =  th.nonzero(rptr != -1, as_tuple=True)[0]
+        rptr = rptr[ind]
+        self.cachemap[rptr] = -1
+
+        if chk:
+            assert (self.immutable[rptr] == -1).sum() == rptr.shape[0]
+        #    print((self.cachemap != -1).sum(), ' ', self.n)
+        #    assert (self.cachemap != -1).sum() == self.n
+
+        ## set reverse pointer from clines to cachemap
+        self.rptr[clines] = nodes
+        self.wt[clines] = self.cur_itr
+        self.cur_itr += 1
+
+        ## set cachemap entries
+        if self.fused:
+            clines += self.base
+
+        self.cachemap[nodes] = clines ##.long()
+        if stats:
+            self.cache_occ += (self.cachemap != -1).sum()
+        if chk:
+            assert (self.immutable[nodes] == -1).sum() == nodes.shape[0]
+        #    assert (self.cachemap != -1).sum() == self.n + clines.shape[0]
+
+        # scatter feats to the cache buffer
+        gnn_utils.scatter_features(feats, clines.long(), self.buf_feats, 0)
+        toc = time.time()
+        self.store_tim += toc - tic
+
+    def cache_load(self, nodes):
+        tic = time.time()
+        clines_all = self.cachemap[nodes.long()]
+        ind = th.nonzero(clines_all != -1, as_tuple=True)[0]
+        clines = clines_all[ind]
+        feats = gnn_utils.gather_features(self.buf_feats, clines.to(th.long))
+
+        if self.fused:
+            clines -= self.base
+
+        self.wt[clines] = self.cur_itr
+        self.cur_itr += 1
+        toc = time.time()
+        self.load_tim += toc - tic
diff --git a/python/dgl/distgnn/mpi.py b/python/dgl/distgnn/mpi.py
new file mode 100644
index 00000000..8367e7e3
--- /dev/null
+++ b/python/dgl/distgnn/mpi.py
@@ -0,0 +1,37 @@
+import os, sys
+import torch as th
+import torch.distributed as dist
+
+def init_mpi(dist_backend, dist_url):
+    world_size = -1
+    if dist_backend == 'ccl':
+        try:
+            import oneccl_bindings_for_pytorch
+        except:
+            print("CCL backend requested but import oneccl_bindings_for_pytorch failed")
+            raise
+    elif dist_backend == 'mpi':
+        if not th.distributed.is_mpi_available():
+            try:
+                import torch_mpi
+                print("imported torch_mpi.........")
+            except:
+                print("MPI backend requested but not available try installing torch_mpi module")
+                raise
+        else:
+            raise ValueError(f"{dist_backend} backend requested but not supported")
+
+    if dist_url == "env://" and world_size == -1:
+        world_size = int(os.environ.get("PMI_SIZE", -1))
+        if world_size == -1: world_size = int(os.environ["WORLD_SIZE"])
+
+
+    distributed = world_size > 1
+    if distributed:
+        rank = int(os.environ.get("PMI_RANK", -1))
+        if rank == -1: rank = int(os.environ["RANK"])
+        dist.init_process_group(backend=dist_backend, init_method=dist_url,
+                                world_size=world_size, rank=rank)
+    if rank == 0:
+        print("Rank: ", rank ," World_size: ", world_size)
+    return rank, world_size
diff --git a/python/dgl/distgnn/partitions.py b/python/dgl/distgnn/partitions.py
new file mode 100644
index 00000000..2df7e3c9
--- /dev/null
+++ b/python/dgl/distgnn/partitions.py
@@ -0,0 +1,399 @@
+import os, sys, psutil, json, argparse, time
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.nn.functional as F
+import dgl
+from dgl import DGLGraph
+from dgl.data.utils import load_tensors, load_graphs
+from dgl.data import register_data_args, load_data
+
+import torch.optim as optim
+import torch.multiprocessing as mp
+import torch.distributed as dist
+from dgl.distgnn.communicate import mpi_allreduce
+import time
+import os
+
+debug = False
+
+class partition_book:
+    def __init__(self, p):
+        g_orig, node_feats, node_map, num_parts, n_classes, dle, etypes = p
+        self.g_orig = g_orig
+        self.node_feats = node_feats
+        self.node_map = node_map
+        self.num_parts = num_parts
+        self.n_classes = n_classes
+        self.dle = dle
+        self.etypes = etypes
+
+def standardize_metis_parts(graph, node_feats, rank, resize=False):
+    N = graph.number_of_nodes()
+    E = graph.number_of_edges()
+
+    if resize:
+        nlocal = (graph.ndata['inner_node'] == 1).sum()
+        try:
+            feat = node_feats['_N/features']
+        except:
+            feat = node_feats['_N/feat']
+
+        try:
+            label = node_feats['_N/label'].clone().resize_(N)
+        except:
+            label = node_feats['_N/labels'].clone().resize_(N)
+        train = node_feats['_N/train_mask'].clone().resize_(N)
+        val = node_feats['_N/val_mask'].clone().resize_(N)
+        test = node_feats['_N/test_mask'].clone().resize_(N)
+
+        ten = th.zeros(N-feat.shape[0], feat.shape[1], dtype=feat.dtype)
+        feat_ = th.cat((feat, ten), 0)
+
+        train[nlocal: ] = 0
+        test[nlocal: ] = 0
+        val[nlocal: ] = 0
+        node_feats['feat'] = feat_
+        node_feats['label'] = label
+        node_feats['train_mask'] = train
+        node_feats['test_mask'] = test
+        node_feats['val_mask'] = val
+
+        graph.ndata['orig'] = graph.ndata['orig_id']
+        del graph.ndata['orig_id']
+        ntrain = (node_feats['train_mask'] == 1).sum()
+        tot_train = mpi_allreduce(ntrain)
+
+        ntest = (node_feats['test_mask'] == 1).sum()
+        tot_test = mpi_allreduce(ntest)
+
+        nval = (node_feats['val_mask'] == 1).sum()
+        tot_val = mpi_allreduce(nval)
+
+        tot_nodes = mpi_allreduce(N)
+        tot_edges = mpi_allreduce(E)
+
+        if rank == 0:
+            print('tot_train nodes: ', tot_train)
+            print('tot_test nodes: ', tot_test)
+            print('tot_val nodes: ', tot_val)
+
+    else:
+        nlocal = (graph.ndata['inner_node'] == 1).sum()
+        try:
+            feat = node_feats['_N/features']
+        except:
+            feat = node_feats['_N/feat']
+
+        try:
+            label = node_feats['_N/label']
+        except:
+            label = node_feats['_N/labels']
+        train = node_feats['_N/train_mask']
+        val = node_feats['_N/val_mask']
+        test = node_feats['_N/test_mask']
+
+        node_feats['feat'] = feat
+        node_feats['label'] = label
+        node_feats['train_mask'] = train
+        node_feats['test_mask'] = test
+        node_feats['val_mask'] = val
+        node_feats['orig'] = graph.ndata['orig_id']
+        node_feats['inner_node'] = graph.ndata['inner_node']
+        node_feats['orig'] = graph.ndata['orig_id']
+
+        ntrain = (node_feats['train_mask'] == 1).sum()
+        tot_train = mpi_allreduce(ntrain)
+
+        ntest = (node_feats['test_mask'] == 1).sum()
+        tot_test = mpi_allreduce(ntest)
+
+        nval = (node_feats['val_mask'] == 1).sum()
+        tot_val = mpi_allreduce(nval)
+
+        tot_nodes = mpi_allreduce(N)
+        tot_edges = mpi_allreduce(E)
+
+        if rank == 0:
+            print('tot_train nodes: ', tot_train)
+            print('tot_test nodes: ', tot_test)
+            print('tot_val nodes: ', tot_val)
+
+    try:
+        del node_feats['_N/feat']
+    except:
+        del node_feats['_N/features']
+    try:
+        del node_feats['_N/label']
+    except:
+        del node_feats['_N/labels']
+    del node_feats['_N/train_mask']
+    del node_feats['_N/test_mask'], node_feats['_N/val_mask']
+
+
+def load_GNNdataset(args):
+    dlist = ['ogbn-products', 'ogbn-papers100M']
+    if args.dataset in dlist:
+        assert os.path.isdir(args.path) == True
+        filename = os.path.join(args.path, "struct.graph")
+        tfilename = os.path.join(args.path, "tensor.pt")
+
+        if os.path.isfile(filename) and os.path.isfile(tfilename):
+            data, _ = dgl.load_graphs(filename)
+            g_orig = data[0]
+            n_classes = int(th.load(tfilename))
+        else:
+            def load_ogb(name):
+                from ogb.nodeproppred import DglNodePropPredDataset
+            
+                data = DglNodePropPredDataset(name=name, root='./dataset')
+                splitted_idx = data.get_idx_split()
+                graph, labels = data[0]
+                labels = labels[:, 0]
+            
+                graph.ndata['features'] = graph.ndata['feat']
+                graph.ndata['labels'] = labels
+                in_feats = graph.ndata['features'].shape[1]
+                num_labels = len(th.unique(labels[th.logical_not(th.isnan(labels))]))
+            
+                # Find the node IDs in the training, validation, and test set.
+                train_nid, val_nid, test_nid = splitted_idx['train'], splitted_idx['valid'], splitted_idx['test']
+                train_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)
+                train_mask[train_nid] = True
+                val_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)
+                val_mask[val_nid] = True
+                test_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)
+                test_mask[test_nid] = True
+                graph.ndata['train_mask'] = train_mask
+                graph.ndata['val_mask'] = val_mask
+                graph.ndata['test_mask'] = test_mask
+                return graph, num_labels
+
+            try:
+                g_orig, n_classes = load_ogb(args.dataset)
+                if not debug:
+                    try:
+                        del g_orig.ndata['feat']
+                        del g_orig.ndata['features']
+                    except:
+                        pass
+                g_orig = dgl.add_reverse_edges(g_orig)
+                if args.rank == 0 and not debug:
+                    dgl.save_graphs(filename, [g_orig])
+                    th.save(th.tensor(n_classes), tfilename)
+            except Exception as e:
+                print(e)
+                n_classes = -1
+                g_orig = None
+
+    elif args.dataset == 'IGBH':
+        path = os.path.join(args.path, args.data, args.dataset_size)
+        filename = os.path.join(
+                     path, 
+                     str(args.world_size)+args.token, 
+                     "struct.graph"
+                   )
+        assert(os.path.isfile(filename)) == True
+        n_classes = args.n_classes
+        g_orig = dgl.load_graphs(filename)[0][0]
+    else:
+        print(">>>>>>>>>> Error: dataset {} not found! exiting...".format(dataset))
+        sys.exit(1)
+
+    return g_orig, n_classes
+
+def partition_book_random(args, part_config, category='', resize_data=False):
+
+    num_parts = args.world_size
+
+    dls = time.time()
+    g_orig, n_classes = load_GNNdataset(args)
+    ntypes = g_orig.ntypes
+    g_orig = None
+
+    part_config_g = part_config
+    di = str(num_parts) + args.token
+    part_config_g = os.path.join(part_config_g, di)
+    fjson = args.dataset + '.json'
+    part_config = os.path.join(part_config_g, fjson)
+
+    try:
+        with open(part_config) as conf_f:
+            part_metadata = json.load(conf_f)
+    except:
+        print(">>>>> Error: Partition data for {} not found!! at {}".
+              format(args.dataset, part_config))
+        sys.exit(1)
+
+    prefix = part_config_g + "/"
+    part_files = part_metadata['part-{}'.format(args.rank)]
+
+    dle = time.time() - dls
+
+    graph = load_graphs(prefix + part_files['part_graph'])[0][0]
+    nsn = int(graph.ndata['inner_node'].sum())
+
+    train_mask = th.zeros(nsn, dtype=th.uint8)
+    val_mask = th.zeros(nsn, dtype=th.uint8)
+    labels = th.zeros(nsn, dtype=th.long)
+
+    pnid = {}
+
+    mask_key = None
+    for k, nt in enumerate(ntypes):
+        pnid[k] = (graph.ndata['_TYPE'][:nsn] == k).nonzero(as_tuple=True)[0]
+        if nt == category:
+            mask_key = k
+        
+    inner_node = graph.ndata['inner_node']
+    orig_id = graph.ndata['orig_id']
+    _TYPE = graph.ndata['_TYPE']
+                
+    graph = None
+
+    if args.rank == 0:
+        process = psutil.Process(os.getpid())
+        print(f'Loaded graph partition data. Mem used: {process.memory_info().rss/1e9} GB', flush=True)
+
+    acc, acc_labels = 0, 0
+    part_files = part_metadata['part-{}'.format(args.rank)]
+    node_feats = load_tensors(prefix + part_files['node_feats'])
+    if len(ntypes) > 1:
+        train_mask[pnid[mask_key]] = node_feats[category+'/train_mask']
+        val_mask[pnid[mask_key]] = node_feats[category+'/val_mask']
+        labels[pnid[mask_key]] = node_feats[category+'/label']
+    else:
+        train_mask[pnid[mask_key]] = node_feats['train_mask']
+        val_mask[pnid[mask_key]] = node_feats['val_mask']
+        labels[pnid[mask_key]] = node_feats['label']
+
+    node_feats['train_mask'] = train_mask
+    node_feats['val_mask'] = val_mask
+    node_feats['labels'] = labels
+    node_feats['inner_node'] = inner_node
+    node_feats['orig'] = orig_id
+    node_feats['_TYPE'] = _TYPE
+    del node_feats[category+'/train_mask']
+    del node_feats[category+'/val_mask']
+    del node_feats[category+'/test_mask']
+    del node_feats[category+'/label']
+
+    node_feats['feat'] = None
+    node_feats['scf'] = None
+    if len(ntypes) > 1:
+        for nt in ntypes:
+            if node_feats['feat'] is None:
+                node_feats['feat'] = node_feats[nt+'/feat']
+                del node_feats[nt+'/feat']
+            else:
+                node_feats['feat'] = th.cat((node_feats['feat'], node_feats[nt+'/feat']),0)
+                del node_feats[nt+'/feat']
+
+            if args.use_int8:
+                if node_feats['scf'] is None:
+                    node_feats['scf'] = node_feats[nt+'/scf']
+                    del node_feats[nt+'/scf']
+                else:
+                    node_feats['scf'] = th.cat((node_feats['scf'], node_feats[nt+'/scf']), 0)
+                    del node_feats[nt+'/scf']
+
+    if not args.use_int8:
+        if args.use_bf16 and (not node_feats['feat'].dtype == th.bfloat16): 
+            node_feats['feat'] = node_feats['feat'].to(th.bfloat16)
+        elif not args.use_bf16 and node_feats['feat'].dtype == th.bfloat16:
+            node_feats['feat'] = node_feats['feat'].to(th.float32)
+
+    dls = time.time()
+    if args.rank == 0:
+        process = psutil.Process(os.getpid())
+        print(f'Loaded graph partition data. Mem used: {process.memory_info().rss/1e9} GB')
+
+    g_orig, n_classes = load_GNNdataset(args)
+    dle = dle + (time.time() - dls)
+
+    node_feats['train_samples'] = g_orig.ndata['train_mask']['paper'].sum()
+    node_feats['eval_samples'] = g_orig.ndata['val_mask']['paper'].sum()
+    g_orig.ndata['test_mask']['paper'] = None
+    g_orig.ndata['train_mask']['paper'] = None
+    g_orig.ndata['val_mask']['paper'] = None
+
+    if args.rank == 0:
+        process = psutil.Process(os.getpid())
+        print(f'Loaded full graph struct. Mem used: {process.memory_info().rss/1e9} GB')
+    etypes = g_orig.etypes
+
+    #node_map = part_metadata['node_map']
+    node_map = None
+    d = g_orig, node_feats, node_map, num_parts, n_classes, dle, etypes
+    pb = partition_book(d)
+    return pb
+
+def partition_book_metis(args, part_config, resize_ndata=False):
+
+    num_parts = args.world_size
+
+    dls = time.time()
+    g_orig, n_classes = load_GNNdataset(args)
+    ntypes = g_orig.ntypes
+    etypes = g_orig.canonical_etypes
+
+    part_config_g = part_config
+
+    di = args.dataset + "-" + str(num_parts) + args.token + "-balance-train"
+    part_config_g = os.path.join(part_config_g, di)
+    fjson = args.dataset + '.json'
+    part_config = os.path.join(part_config_g, fjson)
+
+    #if args.rank == 0:
+    #    print("Dataset/partition location: ", part_config)
+
+    try:
+        with open(part_config) as conf_f:
+            part_metadata = json.load(conf_f)
+    except:
+        print(">>>>> Error: Partition data for {} not found!! at {}".
+              format(args.dataset, part_config))
+        sys.exit(1)
+
+    prefix = part_config_g + "/"
+    part_files = part_metadata['part-{}'.format(args.rank)]
+    assert 'node_feats' in part_files, "the partition does not contain node features."
+    assert 'edge_feats' in part_files, "the partition does not contain edge feature."
+    assert 'part_graph' in part_files, "the partition does not contain graph structure."
+    node_feats = load_tensors(prefix + part_files['node_feats'])
+    graph = load_graphs(prefix + part_files['part_graph'])[0][0]
+
+    dle = time.time() - dls
+
+    #num_parts = part_metadata['num_parts']
+    #node_map_  = part_metadata['node_map']
+
+    standardize_metis_parts(graph, node_feats, args.rank, resize_ndata)
+    del graph
+    if args.use_bf16: node_feats['feat'] = node_feats['feat'].to(th.bfloat16)
+
+    #node_map = []   ## this block should go in standardize_metis_parts
+    #for nm in node_map_['_N']:
+    #    node_map.append(nm[1])
+
+    node_map = None
+    if args.rank == 0:
+        print("n_classes: ", n_classes, flush=True)
+
+    etypes = None
+    d = g_orig, node_feats, node_map, num_parts, n_classes, dle, etypes
+    pb = partition_book(d)
+    return pb
+
+def create_partition_book(args, part_config, resize=False):
+    if args.part_method == 'metis':
+        pb = partition_book_metis(args, part_config, resize)
+    elif args.part_method == 'random':
+        pb = partition_book_random(args, part_config, resize)
+
+    return pb
+
+class args_:
+    def __init__(self, dataset):
+        self.dataset = dataset
+        print("dataset set to: ", self.dataset)
diff --git a/python/dgl/distgnn/queue.py b/python/dgl/distgnn/queue.py
new file mode 100644
index 00000000..a2c59e05
--- /dev/null
+++ b/python/dgl/distgnn/queue.py
@@ -0,0 +1,54 @@
+class gqueue():
+    def __init__(self):
+        self.queue = []
+
+    def push(self, val):
+        self.queue.append(val)
+
+    def pop(self):
+        if self.empty():
+            return -1
+        return self.queue.pop(0)
+
+    def size(self):
+        return len(self.queue)
+
+    def printq(self):
+        print(self.queue)
+
+    def empty(self):
+        if (len(self.queue)) == 0:
+            return True
+        else:
+            return False
+
+    def reset(self):
+        self.queue.clear()
+        assert len(self.queue) == 0
+
+class gstack():
+    def __init__(self):
+        self.stack = []
+
+    def push(self, val):
+        self.stack.append(val)
+
+    def pop(self):
+        if self.empty():
+            return -1
+        return self.stack.pop()
+
+    def size(self):
+        return len(self.stack)
+
+    def printq(self):
+        print(self.stack)
+
+    def empty(self):
+        if (len(self.stack)) == 0:
+            return True
+        else:
+            return False
+
+    def purge(self):
+        self.stack = []
diff --git a/python/dgl/distributed/partition.py b/python/dgl/distributed/partition.py
index f74da1cf..1d112030 100644
--- a/python/dgl/distributed/partition.py
+++ b/python/dgl/distributed/partition.py
@@ -1,14 +1,10 @@
 """Functions for partitions. """
 
-import concurrent
-import concurrent.futures
 import copy
 import json
 import logging
-import multiprocessing as mp
 import os
 import time
-from functools import partial
 
 import numpy as np
 
@@ -22,10 +18,11 @@ from ..partition import (
     get_peak_mem,
     metis_partition_assignment,
     partition_graph_with_halo,
+    partition_graph_with_halo_hetero,
 )
 from ..random import choice as random_choice
 from ..transforms import sort_csc_by_tag, sort_csr_by_tag
-from .constants import DEFAULT_ETYPE, DEFAULT_NTYPE, DGL2GB_EID, GB_DST_ID
+from .constants import DEFAULT_ETYPE, DEFAULT_NTYPE
 from .graph_partition_book import (
     _etype_str_to_tuple,
     _etype_tuple_to_str,
@@ -88,68 +85,49 @@ def _dump_part_config(part_config, part_metadata):
         json.dump(part_metadata, outfile, sort_keys=False, indent=4)
 
 
-def process_partitions(g, formats=None, sort_etypes=False):
+def _save_graphs(filename, g_list, formats=None, sort_etypes=False):
     """Preprocess partitions before saving:
     1. format data types.
     2. sort csc/csr by tag.
     """
-    for k, dtype in RESERVED_FIELD_DTYPE.items():
-        if k in g.ndata:
-            g.ndata[k] = F.astype(g.ndata[k], dtype)
-        if k in g.edata:
-            g.edata[k] = F.astype(g.edata[k], dtype)
-
-    if (sort_etypes) and (formats is not None):
+    for g in g_list:
+        for k, dtype in RESERVED_FIELD_DTYPE.items():
+            if k in g.ndata:
+                g.ndata[k] = F.astype(g.ndata[k], dtype)
+            if k in g.edata:
+                g.edata[k] = F.astype(g.edata[k], dtype)
+    for g in g_list:
+        if (not sort_etypes) or (formats is None):
+            continue
         if "csr" in formats:
             g = sort_csr_by_tag(g, tag=g.edata[ETYPE], tag_type="edge")
         if "csc" in formats:
             g = sort_csc_by_tag(g, tag=g.edata[ETYPE], tag_type="edge")
-    return g
-
-
-def _save_dgl_graphs(filename, g_list, formats=None):
     save_graphs(filename, g_list, formats=formats)
 
 
-def _get_inner_node_mask(graph, ntype_id, gpb=None):
-    ndata = (
-        graph.node_attributes
-        if isinstance(graph, gb.FusedCSCSamplingGraph)
-        else graph.ndata
-    )
-    assert "inner_node" in ndata, "'inner_node' is not in nodes' data"
-    if NTYPE in ndata or gpb is not None:
-        ntype = (
-            gpb.map_to_per_ntype(ndata[NID])[0]
-            if gpb is not None
-            else ndata[NTYPE]
+def _get_inner_node_mask(graph, ntype_id):
+    if NTYPE in graph.ndata:
+        dtype = F.dtype(graph.ndata["inner_node"])
+        return (
+            graph.ndata["inner_node"]
+            * F.astype(graph.ndata[NTYPE] == ntype_id, dtype)
+            == 1
         )
-        dtype = F.dtype(ndata["inner_node"])
-        return ndata["inner_node"] * F.astype(ntype == ntype_id, dtype) == 1
     else:
-        return ndata["inner_node"] == 1
+        return graph.ndata["inner_node"] == 1
 
 
-def _get_inner_edge_mask(
-    graph,
-    etype_id,
-):
-    edata = (
-        graph.edge_attributes
-        if isinstance(graph, gb.FusedCSCSamplingGraph)
-        else graph.edata
-    )
-    assert "inner_edge" in edata, "'inner_edge' is not in edges' data"
-    etype = (
-        graph.type_per_edge
-        if isinstance(graph, gb.FusedCSCSamplingGraph)
-        else (graph.edata[ETYPE] if ETYPE in graph.edata else None)
-    )
-    if etype is not None:
-        dtype = F.dtype(edata["inner_edge"])
-        return edata["inner_edge"] * F.astype(etype == etype_id, dtype) == 1
+def _get_inner_edge_mask(graph, etype_id):
+    if ETYPE in graph.edata:
+        dtype = F.dtype(graph.edata["inner_edge"])
+        return (
+            graph.edata["inner_edge"]
+            * F.astype(graph.edata[ETYPE] == etype_id, dtype)
+            == 1
+        )
     else:
-        return edata["inner_edge"] == 1
+        return graph.edata["inner_edge"] == 1
 
 
 def _get_part_ranges(id_ranges):
@@ -334,31 +312,9 @@ def load_partition(part_config, part_id, load_feats=True, use_graphbolt=False):
         "part-{}".format(part_id) in part_metadata
     ), "part-{} does not exist".format(part_id)
     part_files = part_metadata["part-{}".format(part_id)]
-
-    exist_dgl_graph = exist_graphbolt_graph = False
-    if os.path.exists(os.path.join(config_path, f"part{part_id}", "graph.dgl")):
-        use_graphbolt = False
-        exist_dgl_graph = True
-    if os.path.exists(
-        os.path.join(
-            config_path, f"part{part_id}", "fused_csc_sampling_graph.pt"
-        )
-    ):
-        use_graphbolt = True
-        exist_graphbolt_graph = True
-
-    # Check if both DGL graph and GraphBolt graph exist or not exist. Make sure only one exists.
-    if not exist_dgl_graph and not exist_graphbolt_graph:
-        raise ValueError("The graph object doesn't exist.")
-    if exist_dgl_graph and exist_graphbolt_graph:
-        raise ValueError(
-            "Both DGL graph and GraphBolt graph exist. Please remove one."
-        )
-
+    part_graph_field = "part_graph"
     if use_graphbolt:
         part_graph_field = "part_graph_graphbolt"
-    else:
-        part_graph_field = "part_graph"
     assert (
         part_graph_field in part_files
     ), f"the partition does not contain graph structure: {part_graph_field}"
@@ -371,7 +327,7 @@ def load_partition(part_config, part_id, load_feats=True, use_graphbolt=False):
         os.path.getsize(partition_path),
     )
     graph = (
-        torch.load(partition_path)
+        torch.load(partition_path, weights_only=False)
         if use_graphbolt
         else load_graphs(partition_path)[0][0]
     )
@@ -485,7 +441,7 @@ def load_partition_feats(
     return node_feats, edge_feats
 
 
-def load_partition_book(part_config, part_id, part_metadata=None):
+def load_partition_book(part_config, part_id):
     """Load a graph partition book from the partition config file.
 
     Parameters
@@ -494,8 +450,6 @@ def load_partition_book(part_config, part_id, part_metadata=None):
         The path of the partition config file.
     part_id : int
         The partition ID.
-    part_metadata : dict
-        The meta data of partition.
 
     Returns
     -------
@@ -508,8 +462,7 @@ def load_partition_book(part_config, part_id, part_metadata=None):
     dict
         The edge types
     """
-    if part_metadata is None:
-        part_metadata = _load_part_config(part_config)
+    part_metadata = _load_part_config(part_config)
     assert "num_parts" in part_metadata, "num_parts does not exist."
     assert (
         part_metadata["num_parts"] > part_id
@@ -693,127 +646,6 @@ def _set_trainer_ids(g, sim_g, node_parts):
             g.edges[c_etype].data["trainer_id"] = trainer_id
 
 
-def _partition_to_graphbolt(
-    parts,
-    part_i,
-    part_config,
-    part_metadata,
-    *,
-    store_eids=True,
-    store_inner_node=False,
-    store_inner_edge=False,
-    graph_formats=None,
-):
-    gpb, _, ntypes, etypes = load_partition_book(
-        part_config=part_config, part_id=part_i, part_metadata=part_metadata
-    )
-    graph = parts[part_i]
-    csc_graph = _convert_dgl_partition_to_gb(
-        ntypes=ntypes,
-        etypes=etypes,
-        gpb=gpb,
-        part_meta=part_metadata,
-        graph=graph,
-        store_eids=store_eids,
-        store_inner_edge=store_inner_edge,
-        store_inner_node=store_inner_node,
-        graph_formats=graph_formats,
-    )
-    rel_path_result = _save_graph_gb(
-        part_config=part_config, part_id=part_i, csc_graph=csc_graph
-    )
-    part_metadata[f"part-{part_i}"]["part_graph_graphbolt"] = rel_path_result
-
-
-def _update_node_edge_map(node_map_val, edge_map_val, g, num_parts):
-    """
-    If the original graph contains few nodes or edges for specific node/edge
-    types, the partitioned graph may have empty partitions for these types. And
-    the node_map_val and edge_map_val will have -1 for the start and end ID of
-    these types. This function updates the node_map_val and edge_map_val to be
-    contiguous.
-
-    Example case:
-    Suppose we have a heterogeneous graph with 3 node/edge types and the number
-    of partitions is 3. A possible node_map_val or edge_map_val is as follows:
-
-    | part_id\\Node/Edge Type| Type A |  Type B | Type C |
-    |------------------------|--------|---------|--------|
-    | 0                      | 0, 1   |  -1, -1 |  2, 3  |
-    | 1                      | -1, -1 |  3, 4   |  4, 5  |
-    | 2                      | 5, 6   |  7, 8   |  -1, -1|
-
-    As node/edge IDs are contiguous in node/edge type for each partition, we can
-    update the node_map_val and edge_map_val via updating the start and end ID
-    in row-wise order.
-
-    Updated node_map_val or edge_map_val:
-
-    | part_id\\Node/Edge Type| Type A |  Type B | Type C |
-    |------------------------|--------|---------|--------|
-    | 0                      |  0, 1  |  1, 1   |  2, 3  |
-    | 1                      |  3, 3  |  3, 4   |  4, 5  |
-    | 2                      |  5, 6  |  7, 8   |  8, 8  |
-
-    """
-    # Update the node_map_val to be contiguous.
-    ntype_ids = {ntype: g.get_ntype_id(ntype) for ntype in g.ntypes}
-    ntype_ids_reverse = {v: k for k, v in ntype_ids.items()}
-    for part_id in range(num_parts):
-        for ntype_id in list(ntype_ids.values()):
-            ntype = ntype_ids_reverse[ntype_id]
-            start_id = node_map_val[ntype][part_id][0]
-            end_id = node_map_val[ntype][part_id][1]
-            if not (start_id == -1 and end_id == -1):
-                continue
-            prev_ntype_id = (
-                ntype_ids[ntype] - 1
-                if ntype_ids[ntype] > 0
-                else max(ntype_ids.values())
-            )
-            prev_ntype = ntype_ids_reverse[prev_ntype_id]
-            if ntype_ids[ntype] == 0:
-                if part_id == 0:
-                    node_map_val[ntype][part_id][0] = 0
-                else:
-                    node_map_val[ntype][part_id][0] = node_map_val[prev_ntype][
-                        part_id - 1
-                    ][1]
-            else:
-                node_map_val[ntype][part_id][0] = node_map_val[prev_ntype][
-                    part_id
-                ][1]
-            node_map_val[ntype][part_id][1] = node_map_val[ntype][part_id][0]
-    # Update the edge_map_val to be contiguous.
-    etype_ids = {etype: g.get_etype_id(etype) for etype in g.canonical_etypes}
-    etype_ids_reverse = {v: k for k, v in etype_ids.items()}
-    for part_id in range(num_parts):
-        for etype_id in list(etype_ids.values()):
-            etype = etype_ids_reverse[etype_id]
-            start_id = edge_map_val[etype][part_id][0]
-            end_id = edge_map_val[etype][part_id][1]
-            if not (start_id == -1 and end_id == -1):
-                continue
-            prev_etype_id = (
-                etype_ids[etype] - 1
-                if etype_ids[etype] > 0
-                else max(etype_ids.values())
-            )
-            prev_etype = etype_ids_reverse[prev_etype_id]
-            if etype_ids[etype] == 0:
-                if part_id == 0:
-                    edge_map_val[etype][part_id][0] = 0
-                else:
-                    edge_map_val[etype][part_id][0] = edge_map_val[prev_etype][
-                        part_id - 1
-                    ][1]
-            else:
-                edge_map_val[etype][part_id][0] = edge_map_val[prev_etype][
-                    part_id
-                ][1]
-            edge_map_val[etype][part_id][1] = edge_map_val[etype][part_id][0]
-
-
 def partition_graph(
     g,
     graph_name,
@@ -828,6 +660,7 @@ def partition_graph(
     objtype="cut",
     graph_formats=None,
     use_graphbolt=False,
+    feat_part_only=False,
     **kwargs,
 ):
     """Partition a graph for distributed training and store the partitions on files.
@@ -1030,526 +863,350 @@ def partition_graph(
     ...     g, node_feats, edge_feats, gpb, graph_name, ntypes_list, etypes_list,
     ... ) = dgl.distributed.load_partition('output/test.json', 0)
     """
-    # 'coo' is required for partition
-    assert "coo" in np.concatenate(
-        list(g.formats().values())
-    ), "'coo' format should be allowed for partitioning graph."
-
-    def get_homogeneous(g, balance_ntypes):
-        if g.is_homogeneous:
-            sim_g = to_homogeneous(g)
-            if isinstance(balance_ntypes, dict):
-                assert len(balance_ntypes) == 1
-                bal_ntypes = list(balance_ntypes.values())[0]
-            else:
-                bal_ntypes = balance_ntypes
-        elif isinstance(balance_ntypes, dict):
-            # Here we assign node types for load balancing.
-            # The new node types includes the ones provided by users.
-            num_ntypes = 0
-            for key in g.ntypes:
-                if key in balance_ntypes:
-                    g.nodes[key].data["bal_ntype"] = (
-                        F.astype(balance_ntypes[key], F.int32) + num_ntypes
-                    )
-                    uniq_ntypes = F.unique(balance_ntypes[key])
-                    assert np.all(
-                        F.asnumpy(uniq_ntypes) == np.arange(len(uniq_ntypes))
-                    )
-                    num_ntypes += len(uniq_ntypes)
+    start = time.time()
+    if not feat_part_only:
+        # 'coo' is required for partition
+        assert "coo" in np.concatenate(
+            list(g.formats().values())
+        ), "'coo' format should be allowed for partitioning graph."
+    
+        def get_homogeneous(g, balance_ntypes):
+            if g.is_homogeneous:
+                sim_g = to_homogeneous(g)
+                if isinstance(balance_ntypes, dict):
+                    assert len(balance_ntypes) == 1
+                    bal_ntypes = list(balance_ntypes.values())[0]
                 else:
-                    g.nodes[key].data["bal_ntype"] = (
-                        F.ones((g.num_nodes(key),), F.int32, F.cpu())
-                        * num_ntypes
+                    bal_ntypes = balance_ntypes
+            elif isinstance(balance_ntypes, dict):
+                # Here we assign node types for load balancing.
+                # The new node types includes the ones provided by users.
+                num_ntypes = 0
+                for key in g.ntypes:
+                    if key in balance_ntypes:
+                        g.nodes[key].data["bal_ntype"] = (
+                            F.astype(balance_ntypes[key], F.int32) + num_ntypes
+                        )
+                        uniq_ntypes = F.unique(balance_ntypes[key])
+                        assert np.all(
+                            F.asnumpy(uniq_ntypes) == np.arange(len(uniq_ntypes))
+                        )
+                        num_ntypes += len(uniq_ntypes)
+                    else:
+                        g.nodes[key].data["bal_ntype"] = (
+                            F.ones((g.num_nodes(key),), F.int32, F.cpu())
+                            * num_ntypes
+                        )
+                        num_ntypes += 1
+                sim_g = to_homogeneous(g, ndata=["bal_ntype"])
+                bal_ntypes = sim_g.ndata["bal_ntype"]
+                print(
+                    "The graph has {} node types and balance among {} types".format(
+                        len(g.ntypes), len(F.unique(bal_ntypes))
                     )
-                    num_ntypes += 1
-            sim_g = to_homogeneous(g, ndata=["bal_ntype"])
-            bal_ntypes = sim_g.ndata["bal_ntype"]
-            print(
-                "The graph has {} node types and balance among {} types".format(
-                    len(g.ntypes), len(F.unique(bal_ntypes))
                 )
-            )
-            # We now no longer need them.
-            for key in g.ntypes:
-                del g.nodes[key].data["bal_ntype"]
-            del sim_g.ndata["bal_ntype"]
-        else:
-            sim_g = to_homogeneous(g)
-            bal_ntypes = sim_g.ndata[NTYPE]
-        return sim_g, bal_ntypes
-
-    if objtype not in ["cut", "vol"]:
-        raise ValueError
-
-    if num_parts == 1:
-        start = time.time()
-        sim_g, balance_ntypes = get_homogeneous(g, balance_ntypes)
-        print(
-            "Converting to homogeneous graph takes {:.3f}s, peak mem: {:.3f} GB".format(
-                time.time() - start, get_peak_mem()
-            )
-        )
-        assert num_trainers_per_machine >= 1
-        if num_trainers_per_machine > 1:
-            # First partition the whole graph to each trainer and save the trainer ids in
-            # the node feature "trainer_id".
+                # We now no longer need them.
+                for key in g.ntypes:
+                    del g.nodes[key].data["bal_ntype"]
+                del sim_g.ndata["bal_ntype"]
+            else:
+                sim_g = to_homogeneous(g)
+                bal_ntypes = sim_g.ndata[NTYPE]
+            return sim_g, bal_ntypes
+    
+        if objtype not in ["cut", "vol"]:
+            raise ValueError
+    
+        if num_parts == 1:
             start = time.time()
-            node_parts = metis_partition_assignment(
-                sim_g,
-                num_parts * num_trainers_per_machine,
-                balance_ntypes=balance_ntypes,
-                balance_edges=balance_edges,
-                mode="k-way",
-            )
-            _set_trainer_ids(g, sim_g, node_parts)
+            sim_g, balance_ntypes = get_homogeneous(g, balance_ntypes)
             print(
-                "Assigning nodes to METIS partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
+                "Converting to homogeneous graph takes {:.3f}s, peak mem: {:.3f} GB".format(
                     time.time() - start, get_peak_mem()
                 )
             )
-
-        node_parts = F.zeros((sim_g.num_nodes(),), F.int64, F.cpu())
-        parts = {0: sim_g.clone()}
-        orig_nids = parts[0].ndata[NID] = F.arange(0, sim_g.num_nodes())
-        orig_eids = parts[0].edata[EID] = F.arange(0, sim_g.num_edges())
-        # For one partition, we don't really shuffle nodes and edges. We just need to simulate
-        # it and set node data and edge data of orig_id.
-        parts[0].ndata["orig_id"] = orig_nids
-        parts[0].edata["orig_id"] = orig_eids
-        if return_mapping:
-            if g.is_homogeneous:
-                orig_nids = F.arange(0, sim_g.num_nodes())
-                orig_eids = F.arange(0, sim_g.num_edges())
-            else:
-                orig_nids = {
-                    ntype: F.arange(0, g.num_nodes(ntype)) for ntype in g.ntypes
-                }
-                orig_eids = {
-                    etype: F.arange(0, g.num_edges(etype))
-                    for etype in g.canonical_etypes
-                }
-        parts[0].ndata["inner_node"] = F.ones(
-            (sim_g.num_nodes(),),
-            RESERVED_FIELD_DTYPE["inner_node"],
-            F.cpu(),
-        )
-        parts[0].edata["inner_edge"] = F.ones(
-            (sim_g.num_edges(),),
-            RESERVED_FIELD_DTYPE["inner_edge"],
-            F.cpu(),
-        )
-    elif part_method in ("metis", "random"):
-        start = time.time()
-        sim_g, balance_ntypes = get_homogeneous(g, balance_ntypes)
-        print(
-            "Converting to homogeneous graph takes {:.3f}s, peak mem: {:.3f} GB".format(
-                time.time() - start, get_peak_mem()
-            )
-        )
-        if part_method == "metis":
             assert num_trainers_per_machine >= 1
-            start = time.time()
             if num_trainers_per_machine > 1:
                 # First partition the whole graph to each trainer and save the trainer ids in
                 # the node feature "trainer_id".
+                start = time.time()
                 node_parts = metis_partition_assignment(
                     sim_g,
                     num_parts * num_trainers_per_machine,
                     balance_ntypes=balance_ntypes,
                     balance_edges=balance_edges,
                     mode="k-way",
-                    objtype=objtype,
                 )
                 _set_trainer_ids(g, sim_g, node_parts)
-
-                # And then coalesce the partitions of trainers on the same machine into one
-                # larger partition.
-                node_parts = F.floor_div(node_parts, num_trainers_per_machine)
-            else:
-                node_parts = metis_partition_assignment(
-                    sim_g,
-                    num_parts,
-                    balance_ntypes=balance_ntypes,
-                    balance_edges=balance_edges,
-                    objtype=objtype,
-                )
-            print(
-                "Assigning nodes to METIS partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
-                    time.time() - start, get_peak_mem()
+                print(
+                    "Assigning nodes to METIS partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
+                        time.time() - start, get_peak_mem()
+                    )
                 )
+    
+            node_parts = F.zeros((sim_g.num_nodes(),), F.int64, F.cpu())
+            parts = {0: sim_g.clone()}
+            orig_nids = parts[0].ndata[NID] = F.arange(0, sim_g.num_nodes())
+            orig_eids = parts[0].edata[EID] = F.arange(0, sim_g.num_edges())
+            # For one partition, we don't really shuffle nodes and edges. We just need to simulate
+            # it and set node data and edge data of orig_id.
+            parts[0].ndata["orig_id"] = orig_nids
+            parts[0].edata["orig_id"] = orig_eids
+            if return_mapping:
+                if g.is_homogeneous:
+                    orig_nids = F.arange(0, sim_g.num_nodes())
+                    orig_eids = F.arange(0, sim_g.num_edges())
+                else:
+                    orig_nids = {
+                        ntype: F.arange(0, g.num_nodes(ntype)) for ntype in g.ntypes
+                    }
+                    orig_eids = {
+                        etype: F.arange(0, g.num_edges(etype))
+                        for etype in g.canonical_etypes
+                    }
+            parts[0].ndata["inner_node"] = F.ones(
+                (sim_g.num_nodes(),),
+                RESERVED_FIELD_DTYPE["inner_node"],
+                F.cpu(),
             )
-        else:
-            node_parts = random_choice(num_parts, sim_g.num_nodes())
-        start = time.time()
-        parts, orig_nids, orig_eids = partition_graph_with_halo(
-            sim_g, node_parts, num_hops, reshuffle=True
-        )
-        print(
-            "Splitting the graph into partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
-                time.time() - start, get_peak_mem()
-            )
-        )
-        if return_mapping:
-            orig_nids, orig_eids = _get_orig_ids(g, sim_g, orig_nids, orig_eids)
-    else:
-        raise Exception("Unknown partitioning method: " + part_method)
-
-    # If the input is a heterogeneous graph, get the original node types and original node IDs.
-    # `part' has three types of node data at this point.
-    # NTYPE: the node type.
-    # orig_id: the global node IDs in the homogeneous version of input graph.
-    # NID: the global node IDs in the reshuffled homogeneous version of the input graph.
-    if not g.is_homogeneous:
-        for name in parts:
-            orig_ids = parts[name].ndata["orig_id"]
-            ntype = F.gather_row(sim_g.ndata[NTYPE], orig_ids)
-            parts[name].ndata[NTYPE] = F.astype(
-                ntype, RESERVED_FIELD_DTYPE[NTYPE]
-            )
-            assert np.all(
-                F.asnumpy(ntype) == F.asnumpy(parts[name].ndata[NTYPE])
-            )
-            # Get the original edge types and original edge IDs.
-            orig_ids = parts[name].edata["orig_id"]
-            etype = F.gather_row(sim_g.edata[ETYPE], orig_ids)
-            parts[name].edata[ETYPE] = F.astype(
-                etype, RESERVED_FIELD_DTYPE[ETYPE]
-            )
-            assert np.all(
-                F.asnumpy(etype) == F.asnumpy(parts[name].edata[ETYPE])
-            )
-
-            # Calculate the global node IDs to per-node IDs mapping.
-            inner_ntype = F.boolean_mask(
-                parts[name].ndata[NTYPE], parts[name].ndata["inner_node"] == 1
-            )
-            inner_nids = F.boolean_mask(
-                parts[name].ndata[NID], parts[name].ndata["inner_node"] == 1
+            parts[0].edata["inner_edge"] = F.ones(
+                (sim_g.num_edges(),),
+                RESERVED_FIELD_DTYPE["inner_edge"],
+                F.cpu(),
             )
-            for ntype in g.ntypes:
-                inner_ntype_mask = inner_ntype == g.get_ntype_id(ntype)
-                if F.sum(F.astype(inner_ntype_mask, F.int64), 0) == 0:
-                    # Skip if there is no node of this type in this partition.
-                    continue
-                typed_nids = F.boolean_mask(inner_nids, inner_ntype_mask)
-                # inner node IDs are in a contiguous ID range.
-                expected_range = np.arange(
-                    int(F.as_scalar(typed_nids[0])),
-                    int(F.as_scalar(typed_nids[-1])) + 1,
+        elif part_method in ("metis", "random"):
+            start = time.time()
+            sim_g, balance_ntypes = get_homogeneous(g, balance_ntypes)
+            print(
+                "Converting to homogeneous graph takes {:.3f}s, peak mem: {:.3f} GB".format(
+                    time.time() - start, get_peak_mem()
                 )
-                assert np.all(F.asnumpy(typed_nids) == expected_range)
-            # Calculate the global edge IDs to per-edge IDs mapping.
-            inner_etype = F.boolean_mask(
-                parts[name].edata[ETYPE], parts[name].edata["inner_edge"] == 1
-            )
-            inner_eids = F.boolean_mask(
-                parts[name].edata[EID], parts[name].edata["inner_edge"] == 1
             )
-            for etype in g.canonical_etypes:
-                inner_etype_mask = inner_etype == g.get_etype_id(etype)
-                if F.sum(F.astype(inner_etype_mask, F.int64), 0) == 0:
-                    # Skip if there is no edge of this type in this partition.
-                    continue
-                typed_eids = np.sort(
-                    F.asnumpy(F.boolean_mask(inner_eids, inner_etype_mask))
-                )
-                assert np.all(
-                    typed_eids
-                    == np.arange(int(typed_eids[0]), int(typed_eids[-1]) + 1)
-                )
-
-    os.makedirs(out_path, mode=0o775, exist_ok=True)
-    tot_num_inner_edges = 0
-    out_path = os.path.abspath(out_path)
-
-    # With reshuffling, we can ensure that all nodes and edges are reshuffled
-    # and are in contiguous ID space.
-    if num_parts > 1:
-        node_map_val = {}
-        edge_map_val = {}
-        for ntype in g.ntypes:
-            ntype_id = g.get_ntype_id(ntype)
-            val = []
-            node_map_val[ntype] = []
-            for i in parts:
-                inner_node_mask = _get_inner_node_mask(parts[i], ntype_id)
-                val.append(
-                    F.as_scalar(F.sum(F.astype(inner_node_mask, F.int64), 0))
-                )
-                if F.sum(F.astype(inner_node_mask, F.int64), 0) == 0:
-                    node_map_val[ntype].append([-1, -1])
-                    continue
-                inner_nids = F.boolean_mask(
-                    parts[i].ndata[NID], inner_node_mask
-                )
-                node_map_val[ntype].append(
-                    [
-                        int(F.as_scalar(inner_nids[0])),
-                        int(F.as_scalar(inner_nids[-1])) + 1,
-                    ]
-                )
-            val = np.cumsum(val).tolist()
-            assert val[-1] == g.num_nodes(ntype)
-        for etype in g.canonical_etypes:
-            etype_id = g.get_etype_id(etype)
-            val = []
-            edge_map_val[etype] = []
-            for i in parts:
-                inner_edge_mask = _get_inner_edge_mask(parts[i], etype_id)
-                val.append(
-                    F.as_scalar(F.sum(F.astype(inner_edge_mask, F.int64), 0))
-                )
-                if F.sum(F.astype(inner_edge_mask, F.int64), 0) == 0:
-                    edge_map_val[etype].append([-1, -1])
-                    continue
-                inner_eids = np.sort(
-                    F.asnumpy(
-                        F.boolean_mask(parts[i].edata[EID], inner_edge_mask)
-                    )
-                )
-                edge_map_val[etype].append(
-                    [int(inner_eids[0]), int(inner_eids[-1]) + 1]
-                )
-            val = np.cumsum(val).tolist()
-            assert val[-1] == g.num_edges(etype)
-        # Update the node_map_val and edge_map_val to be contiguous.
-        _update_node_edge_map(node_map_val, edge_map_val, g, num_parts)
-    else:
-        node_map_val = {}
-        edge_map_val = {}
-        for ntype in g.ntypes:
-            ntype_id = g.get_ntype_id(ntype)
-            inner_node_mask = _get_inner_node_mask(parts[0], ntype_id)
-            inner_nids = F.boolean_mask(parts[0].ndata[NID], inner_node_mask)
-            node_map_val[ntype] = [
-                [
-                    int(F.as_scalar(inner_nids[0])),
-                    int(F.as_scalar(inner_nids[-1])) + 1,
-                ]
-            ]
-        for etype in g.canonical_etypes:
-            etype_id = g.get_etype_id(etype)
-            inner_edge_mask = _get_inner_edge_mask(parts[0], etype_id)
-            inner_eids = F.boolean_mask(parts[0].edata[EID], inner_edge_mask)
-            edge_map_val[etype] = [
-                [
-                    int(F.as_scalar(inner_eids[0])),
-                    int(F.as_scalar(inner_eids[-1])) + 1,
-                ]
-            ]
-
-        # Double check that the node IDs in the global ID space are sorted.
-        for ntype in node_map_val:
-            val = np.concatenate([np.array(l) for l in node_map_val[ntype]])
-            assert np.all(val[:-1] <= val[1:])
-        for etype in edge_map_val:
-            val = np.concatenate([np.array(l) for l in edge_map_val[etype]])
-            assert np.all(val[:-1] <= val[1:])
-
-    start = time.time()
-    ntypes = {ntype: g.get_ntype_id(ntype) for ntype in g.ntypes}
-    etypes = {etype: g.get_etype_id(etype) for etype in g.canonical_etypes}
-    part_metadata = {
-        "graph_name": graph_name,
-        "num_nodes": g.num_nodes(),
-        "num_edges": g.num_edges(),
-        "part_method": part_method,
-        "num_parts": num_parts,
-        "halo_hops": num_hops,
-        "node_map": node_map_val,
-        "edge_map": edge_map_val,
-        "ntypes": ntypes,
-        "etypes": etypes,
-    }
-    part_config = os.path.join(out_path, graph_name + ".json")
-    for part_id in range(num_parts):
-        part = parts[part_id]
-
-        # Get the node/edge features of each partition.
-        node_feats = {}
-        edge_feats = {}
-        if num_parts > 1:
-            for ntype in g.ntypes:
-                ntype_id = g.get_ntype_id(ntype)
-                # To get the edges in the input graph, we should use original node IDs.
-                # Both orig_id and NID stores the per-node-type IDs.
-                ndata_name = "orig_id"
-                inner_node_mask = _get_inner_node_mask(part, ntype_id)
-                # This is global node IDs.
-                local_nodes = F.boolean_mask(
-                    part.ndata[ndata_name], inner_node_mask
-                )
-                if len(g.ntypes) > 1:
-                    # If the input is a heterogeneous graph.
-                    local_nodes = F.gather_row(sim_g.ndata[NID], local_nodes)
-                    print(
-                        "part {} has {} nodes of type {} and {} are inside the partition".format(
-                            part_id,
-                            F.as_scalar(
-                                F.sum(part.ndata[NTYPE] == ntype_id, 0)
-                            ),
-                            ntype,
-                            len(local_nodes),
-                        )
+            if part_method == "metis":
+                assert num_trainers_per_machine >= 1
+                start = time.time()
+                if num_trainers_per_machine > 1:
+                    # First partition the whole graph to each trainer and save the trainer ids in
+                    # the node feature "trainer_id".
+                    node_parts = metis_partition_assignment(
+                        sim_g,
+                        num_parts * num_trainers_per_machine,
+                        balance_ntypes=balance_ntypes,
+                        balance_edges=balance_edges,
+                        mode="k-way",
+                        objtype=objtype,
                     )
+                    _set_trainer_ids(g, sim_g, node_parts)
+    
+                    # And then coalesce the partitions of trainers on the same machine into one
+                    # larger partition.
+                    node_parts = F.floor_div(node_parts, num_trainers_per_machine)
                 else:
-                    print(
-                        "part {} has {} nodes and {} are inside the partition".format(
-                            part_id, part.num_nodes(), len(local_nodes)
-                        )
+                    node_parts = metis_partition_assignment(
+                        sim_g,
+                        num_parts,
+                        balance_ntypes=balance_ntypes,
+                        balance_edges=balance_edges,
+                        objtype=objtype,
                     )
-
-                for name in g.nodes[ntype].data:
-                    if name in [NID, "inner_node"]:
-                        continue
-                    node_feats[ntype + "/" + name] = F.gather_row(
-                        g.nodes[ntype].data[name], local_nodes
+                print(
+                    "Assigning nodes to METIS partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
+                        time.time() - start, get_peak_mem()
                     )
-
-            for etype in g.canonical_etypes:
-                etype_id = g.get_etype_id(etype)
-                edata_name = "orig_id"
-                inner_edge_mask = _get_inner_edge_mask(part, etype_id)
-                # This is global edge IDs.
-                local_edges = F.boolean_mask(
-                    part.edata[edata_name], inner_edge_mask
                 )
-                if not g.is_homogeneous:
-                    local_edges = F.gather_row(sim_g.edata[EID], local_edges)
-                    print(
-                        "part {} has {} edges of type {} and {} are inside the partition".format(
-                            part_id,
-                            F.as_scalar(
-                                F.sum(part.edata[ETYPE] == etype_id, 0)
-                            ),
-                            etype,
-                            len(local_edges),
-                        )
-                    )
-                else:
-                    print(
-                        "part {} has {} edges and {} are inside the partition".format(
-                            part_id, part.num_edges(), len(local_edges)
-                        )
-                    )
-                tot_num_inner_edges += len(local_edges)
-
-                for name in g.edges[etype].data:
-                    if name in [EID, "inner_edge"]:
-                        continue
-                    edge_feats[
-                        _etype_tuple_to_str(etype) + "/" + name
-                    ] = F.gather_row(g.edges[etype].data[name], local_edges)
+            else:
+                #node_parts = random_choice(num_parts, sim_g.num_nodes())
+                node_parts = torch.arange(sim_g.num_nodes(), dtype=torch.int64)
+                node_parts = node_parts % num_parts
+                rand_order = torch.randperm(node_parts.size(0))
+                node_parts = node_parts[rand_order]
+                del rand_order
+    
+            start = time.time()
+            #parts, orig_nids, orig_eids = partition_graph_with_halo(
+            #    sim_g, node_parts, num_hops, reshuffle=True
+            #)
+            ntype_id = {}
+            for nt in g.ntypes:
+                ntype_id[nt] = g.get_ntype_id(nt)
+
+            etype_id = {}
+            for et in g.canonical_etypes:
+                etype_id[et] = g.get_etype_id(et)
+
+            partition_graph_with_halo_hetero(
+                sim_g, node_parts, num_hops, graph_name, part_method, num_parts, out_path, 
+                sim_g.ndata, sim_g.edata, g.ntypes, g.canonical_etypes, g.num_nodes(), g.num_edges(), 
+                ntype_id, etype_id, reshuffle=True
+            )
+            
+            print(
+                "Splitting the graph into partitions takes {:.3f}s, peak mem: {:.3f} GB".format(
+                    time.time() - start, get_peak_mem()
+                )
+            )
+            if return_mapping:
+                orig_nids, orig_eids = _get_orig_ids(g, sim_g, orig_nids, orig_eids)
         else:
-            for ntype in g.ntypes:
-                if len(g.ntypes) > 1:
-                    ndata_name = "orig_id"
+            raise Exception("Unknown partitioning method: " + part_method)
+    else:
+        tot_num_inner_edges = 0
+        sim_g = to_homogeneous(g)
+        part_metadata = _load_part_config(f"{out_path}/{graph_name}.json")
+        for part_id in range(num_parts):
+            part_dir = os.path.join(out_path, "part" + str(part_id))
+            part_graph_file = os.path.join(part_dir, "graph.dgl")
+            part = load_graphs(part_graph_file)[0][0] #parts[part_id]
+
+            # Get the node/edge features of each partition.
+            node_feats = {}
+            edge_feats = {}
+            if num_parts > 1:
+                for ntype in g.ntypes:
                     ntype_id = g.get_ntype_id(ntype)
+                    # To get the edges in the input graph, we should use original node IDs.
+                    # Both orig_id and NID stores the per-node-type IDs.
+                    ndata_name = "orig_id"
                     inner_node_mask = _get_inner_node_mask(part, ntype_id)
                     # This is global node IDs.
                     local_nodes = F.boolean_mask(
                         part.ndata[ndata_name], inner_node_mask
                     )
-                    local_nodes = F.gather_row(sim_g.ndata[NID], local_nodes)
-                else:
-                    local_nodes = sim_g.ndata[NID]
-                for name in g.nodes[ntype].data:
-                    if name in [NID, "inner_node"]:
-                        continue
-                    node_feats[ntype + "/" + name] = F.gather_row(
-                        g.nodes[ntype].data[name], local_nodes
-                    )
-            for etype in g.canonical_etypes:
-                if not g.is_homogeneous:
-                    edata_name = "orig_id"
+                    if len(g.ntypes) > 1:
+                        # If the input is a heterogeneous graph.
+                        local_nodes = F.gather_row(sim_g.ndata[NID], local_nodes)
+                        print(
+                            "part {} has {} nodes of type {} and {} are inside the partition".format(
+                                part_id,
+                                F.as_scalar(
+                                    F.sum(part.ndata[NTYPE] == ntype_id, 0)
+                                ),
+                                ntype,
+                                len(local_nodes),
+                            )
+                        )
+                    else:
+                        print(
+                            "part {} has {} nodes and {} are inside the partition".format(
+                                part_id, part.num_nodes(), len(local_nodes)
+                            )
+                        )
+    
+                    for name in g.nodes[ntype].data:
+                        if name in [NID, "inner_node"]:
+                            continue
+                        node_feats[ntype + "/" + name] = F.gather_row(
+                            g.nodes[ntype].data[name], local_nodes
+                        )
+    
+                for etype in g.canonical_etypes:
                     etype_id = g.get_etype_id(etype)
+                    edata_name = "orig_id"
                     inner_edge_mask = _get_inner_edge_mask(part, etype_id)
                     # This is global edge IDs.
                     local_edges = F.boolean_mask(
                         part.edata[edata_name], inner_edge_mask
                     )
-                    local_edges = F.gather_row(sim_g.edata[EID], local_edges)
-                else:
-                    local_edges = sim_g.edata[EID]
-                for name in g.edges[etype].data:
-                    if name in [EID, "inner_edge"]:
-                        continue
-                    edge_feats[
-                        _etype_tuple_to_str(etype) + "/" + name
-                    ] = F.gather_row(g.edges[etype].data[name], local_edges)
-        # delete `orig_id` from ndata/edata
-        del part.ndata["orig_id"]
-        del part.edata["orig_id"]
-
-        part_dir = os.path.join(out_path, "part" + str(part_id))
-        node_feat_file = os.path.join(part_dir, "node_feat.dgl")
-        edge_feat_file = os.path.join(part_dir, "edge_feat.dgl")
-
-        os.makedirs(part_dir, mode=0o775, exist_ok=True)
-        save_tensors(node_feat_file, node_feats)
-        save_tensors(edge_feat_file, edge_feats)
-
-        part_metadata["part-{}".format(part_id)] = {
-            "node_feats": os.path.relpath(node_feat_file, out_path),
-            "edge_feats": os.path.relpath(edge_feat_file, out_path),
-        }
-        sort_etypes = len(g.etypes) > 1
-        part = process_partitions(part, graph_formats, sort_etypes)
-
-    # transmit to graphbolt and save graph
-    if use_graphbolt:
-        # save FusedCSCSamplingGraph
-        kwargs["graph_formats"] = graph_formats
-        n_jobs = kwargs.pop("n_jobs", 1)
-        mp_ctx = mp.get_context("spawn")
-        with concurrent.futures.ProcessPoolExecutor(  # pylint: disable=unexpected-keyword-arg
-            max_workers=min(num_parts, n_jobs),
-            mp_context=mp_ctx,
-        ) as executor:
-            for part_id in range(num_parts):
-                executor.submit(
-                    _partition_to_graphbolt(
-                        part_i=part_id,
-                        part_config=part_config,
-                        part_metadata=part_metadata,
-                        parts=parts,
-                        **kwargs,
-                    )
-                )
-        part_metadata["node_map_dtype"] = "int64"
-        part_metadata["edge_map_dtype"] = "int64"
-    else:
-        for part_id, part in parts.items():
+                    if not g.is_homogeneous:
+                        local_edges = F.gather_row(sim_g.edata[EID], local_edges)
+                        print(
+                            "part {} has {} edges of type {} and {} are inside the partition".format(
+                                part_id,
+                                F.as_scalar(
+                                    F.sum(part.edata[ETYPE] == etype_id, 0)
+                                ),
+                                etype,
+                                len(local_edges),
+                            )
+                        )
+                    else:
+                        print(
+                            "part {} has {} edges and {} are inside the partition".format(
+                                part_id, part.num_edges(), len(local_edges)
+                            )
+                        )
+                    tot_num_inner_edges += len(local_edges)
+    
+                    for name in g.edges[etype].data:
+                        if name in [EID, "inner_edge"]:
+                            continue
+                        edge_feats[
+                            _etype_tuple_to_str(etype) + "/" + name
+                        ] = F.gather_row(g.edges[etype].data[name], local_edges)
+            else:
+                for ntype in g.ntypes:
+                    if len(g.ntypes) > 1:
+                        ndata_name = "orig_id"
+                        ntype_id = g.get_ntype_id(ntype)
+                        inner_node_mask = _get_inner_node_mask(part, ntype_id)
+                        # This is global node IDs.
+                        local_nodes = F.boolean_mask(
+                            part.ndata[ndata_name], inner_node_mask
+                        )
+                        local_nodes = F.gather_row(sim_g.ndata[NID], local_nodes)
+                    else:
+                        local_nodes = sim_g.ndata[NID]
+                    for name in g.nodes[ntype].data:
+                        if name in [NID, "inner_node"]:
+                            continue
+                        node_feats[ntype + "/" + name] = F.gather_row(
+                            g.nodes[ntype].data[name], local_nodes
+                        )
+                for etype in g.canonical_etypes:
+                    if not g.is_homogeneous:
+                        edata_name = "orig_id"
+                        etype_id = g.get_etype_id(etype)
+                        inner_edge_mask = _get_inner_edge_mask(part, etype_id)
+                        # This is global edge IDs.
+                        local_edges = F.boolean_mask(
+                            part.edata[edata_name], inner_edge_mask
+                        )
+                        local_edges = F.gather_row(sim_g.edata[EID], local_edges)
+                    else:
+                        local_edges = sim_g.edata[EID]
+                    for name in g.edges[etype].data:
+                        if name in [EID, "inner_edge"]:
+                            continue
+                        edge_feats[
+                            _etype_tuple_to_str(etype) + "/" + name
+                        ] = F.gather_row(g.edges[etype].data[name], local_edges)
+            # delete `orig_id` from ndata/edata
+            #del part.ndata["orig_id"]
+            #del part.edata["orig_id"]
+    
             part_dir = os.path.join(out_path, "part" + str(part_id))
+            node_feat_file = os.path.join(part_dir, "node_feat.dgl")
+            edge_feat_file = os.path.join(part_dir, "edge_feat.dgl")
             part_graph_file = os.path.join(part_dir, "graph.dgl")
-            part_metadata["part-{}".format(part_id)][
-                "part_graph"
-            ] = os.path.relpath(part_graph_file, out_path)
-            # save DGLGraph
-            _save_dgl_graphs(
-                part_graph_file,
-                [part],
-                formats=graph_formats,
-            )
-
-    _dump_part_config(part_config, part_metadata)
-
-    num_cuts = sim_g.num_edges() - tot_num_inner_edges
-    if num_parts == 1:
-        num_cuts = 0
-    print(
-        "There are {} edges in the graph and {} edge cuts for {} partitions.".format(
-            g.num_edges(), num_cuts, num_parts
-        )
-    )
-
+            part_metadata["part-{}".format(part_id)] = {
+                "node_feats": os.path.relpath(node_feat_file, out_path),
+                "edge_feats": os.path.relpath(edge_feat_file, out_path),
+                "part_graph": os.path.relpath(part_graph_file, out_path),
+            }
+            #os.makedirs(part_dir, mode=0o775, exist_ok=True)
+            save_tensors(node_feat_file, node_feats)
+            save_tensors(edge_feat_file, edge_feats)
+    
+            #sort_etypes = len(g.etypes) > 1
+            #_save_graphs(
+            #   part_graph_file,
+            #    [part],
+            #    formats=graph_formats,
+            #    sort_etypes=sort_etypes,
+            #)
+        _dump_part_config(f"{out_path}/{graph_name}.json", part_metadata)
     print(
         "Save partitions: {:.3f} seconds, peak memory: {:.3f} GB".format(
             time.time() - start, get_peak_mem()
         )
     )
 
+    if use_graphbolt:
+        dgl_partition_to_graphbolt(
+            part_config,
+            **kwargs,
+        )
+
     if return_mapping:
         return orig_nids, orig_eids
 
@@ -1584,179 +1241,28 @@ def _cast_to_minimum_dtype(predicate, data, field=None):
     return data
 
 
-# Utility functions.
-def is_homogeneous(ntypes, etypes):
-    """Checks if the provided ntypes and etypes form a homogeneous graph."""
-    return len(ntypes) == 1 and len(etypes) == 1
-
-
-def init_type_per_edge(graph, gpb):
-    """Initialize edge ids for every edge type."""
-    etype_ids = gpb.map_to_per_etype(graph.edata[EID])[0]
-    return etype_ids
-
-
-def _load_part(part_config, part_id, parts=None):
-    """load parts from variable or dist."""
-    if parts is None:
-        graph, _, _, _, _, _, _ = load_partition(
-            part_config, part_id, load_feats=False
-        )
-    else:
-        graph = parts[part_id]
-    return graph
-
-
-def _save_graph_gb(part_config, part_id, csc_graph):
-    csc_graph_save_dir = os.path.join(
-        os.path.dirname(part_config),
-        f"part{part_id}",
-    )
-    csc_graph_path = os.path.join(
-        csc_graph_save_dir, "fused_csc_sampling_graph.pt"
-    )
-    torch.save(csc_graph, csc_graph_path)
-
-    return os.path.relpath(csc_graph_path, os.path.dirname(part_config))
-
-
-def cast_various_to_minimum_dtype_gb(
-    num_parts,
-    indptr,
-    indices,
-    type_per_edge,
-    etypes,
-    ntypes,
-    node_attributes,
-    edge_attributes,
-    part_meta=None,
-    graph=None,
-    edge_count=None,
-    node_count=None,
-    tot_edge_count=None,
-    tot_node_count=None,
-):
-    """Cast various data to minimum dtype."""
-    if graph is not None:
-        assert part_meta is not None
-        tot_edge_count = graph.num_edges()
-        tot_node_count = graph.num_nodes()
-        node_count = part_meta["num_nodes"]
-        edge_count = part_meta["num_edges"]
-    else:
-        assert tot_edge_count is not None
-        assert tot_node_count is not None
-        assert edge_count is not None
-        assert node_count is not None
-
-    # Cast 1: indptr.
-    indptr = _cast_to_minimum_dtype(tot_edge_count, indptr)
-    # Cast 2: indices.
-    indices = _cast_to_minimum_dtype(tot_node_count, indices)
-    # Cast 3: type_per_edge.
-    type_per_edge = _cast_to_minimum_dtype(
-        len(etypes), type_per_edge, field=ETYPE
-    )
-    # Cast 4: node/edge_attributes.
-    predicates = {
-        NID: node_count,
-        "part_id": num_parts,
-        NTYPE: len(ntypes),
-        EID: edge_count,
-        ETYPE: len(etypes),
-        DGL2GB_EID: edge_count,
-        GB_DST_ID: node_count,
-    }
-    for attributes in [node_attributes, edge_attributes]:
-        for key in attributes:
-            if key not in predicates:
-                continue
-            attributes[key] = _cast_to_minimum_dtype(
-                predicates[key], attributes[key], field=key
-            )
-    return indptr, indices, type_per_edge
-
-
-def _create_attributes_gb(
-    graph,
-    gpb,
-    edge_ids,
-    is_homo,
-    store_inner_node,
-    store_inner_edge,
-    store_eids,
-    debug_mode,
-):
-    # Save node attributes. Detailed attributes are shown below.
-    #  DGL_GB\Attributes  dgl.NID("_ID")  dgl.NTYPE("_TYPE")  "inner_node"  "part_id"
-    #  DGL_Homograph           ✅                🚫                  ✅          ✅
-    #  GB_Homograph            ✅                🚫               optional       🚫
-    #  DGL_Heterograph         ✅                ✅                  ✅          ✅
-    #  GB_Heterograph          ✅                🚫               optional       🚫
-    required_node_attrs = [NID]
-    if store_inner_node:
-        required_node_attrs.append("inner_node")
-    if debug_mode:
-        required_node_attrs = list(graph.ndata.keys())
-    node_attributes = {attr: graph.ndata[attr] for attr in required_node_attrs}
-
-    # Save edge attributes. Detailed attributes are shown below.
-    #  DGL_GB\Attributes  dgl.EID("_ID")  dgl.ETYPE("_TYPE")  "inner_edge"
-    #  DGL_Homograph           ✅               🚫                  ✅
-    #  GB_Homograph         optional            🚫               optional
-    #  DGL_Heterograph         ✅               ✅                  ✅
-    #  GB_Heterograph       optional            ✅               optional
-    type_per_edge = None
-    if not is_homo:
-        type_per_edge = init_type_per_edge(graph, gpb)[edge_ids]
-        type_per_edge = type_per_edge.to(RESERVED_FIELD_DTYPE[ETYPE])
-    required_edge_attrs = []
-    if store_eids:
-        required_edge_attrs.append(EID)
-    if store_inner_edge:
-        required_edge_attrs.append("inner_edge")
-    if debug_mode:
-        required_edge_attrs = list(graph.edata.keys())
-    edge_attributes = {
-        attr: graph.edata[attr][edge_ids] for attr in required_edge_attrs
-    }
-    return node_attributes, edge_attributes, type_per_edge
-
-
-def _convert_dgl_partition_to_gb(
-    ntypes,
-    etypes,
-    gpb,
-    part_meta,
-    graph,
-    graph_formats=None,
+def dgl_partition_to_graphbolt(
+    part_config,
+    *,
     store_eids=False,
     store_inner_node=False,
     store_inner_edge=False,
 ):
-    """Converts a single DGL partition to GraphBolt.
+    """Convert partitions of dgl to FusedCSCSamplingGraph of GraphBolt.
+
+    This API converts `DGLGraph` partitions to `FusedCSCSamplingGraph` which is
+    dedicated for sampling in `GraphBolt`. New graphs will be stored alongside
+    original graph as `fused_csc_sampling_graph.pt`.
+
+    In the near future, partitions are supposed to be saved as
+    `FusedCSCSamplingGraph` directly. At that time, this API should be deprecated.
 
     Parameters
     ----------
-    node types : dict
-        The node types
-    edge types : dict
-        The edge types
-    gpb : GraphPartitionBook
-        The global partition information.
-    part_meta : dict
-        Contain the meta data of the partition.
-    graph : DGLGraph
-        The graph to be converted to graphbolt graph.
-    graph_formats : str or list[str], optional
-        Save partitions in specified formats. It could be any combination of
-        `coo`, `csc`. As `csc` format is mandatory for `FusedCSCSamplingGraph`,
-        it is not necessary to specify this argument. It's mainly for
-        specifying `coo` format to save edge ID mapping and destination node
-        IDs. If not specified, whether to save `coo` format is determined by
-        the availability of the format in DGL partitions. Default: None.
+    part_config : str
+        The partition configuration JSON file.
     store_eids : bool, optional
-        Whether to store edge IDs in the new graph. Default: True.
+        Whether to store edge IDs in the new graph. Default: False.
     store_inner_node : bool, optional
         Whether to store inner node mask in the new graph. Default: False.
     store_inner_edge : bool, optional
@@ -1768,144 +1274,18 @@ def _convert_dgl_partition_to_gb(
             "Running in debug mode which means all attributes of DGL partitions"
             " will be saved to the new format."
         )
+    part_meta = _load_part_config(part_config)
+    new_part_meta = copy.deepcopy(part_meta)
     num_parts = part_meta["num_parts"]
 
-    is_homo = is_homogeneous(ntypes, etypes)
-    node_type_to_id = (
-        None if is_homo else {ntype: ntid for ntid, ntype in enumerate(ntypes)}
-    )
-    edge_type_to_id = (
-        None
-        if is_homo
-        else {
-            gb.etype_tuple_to_str(etype): etid for etype, etid in etypes.items()
-        }
-    )
-    # Obtain CSC indtpr and indices.
-    indptr, indices, edge_ids = graph.adj_tensors("csc")
-
-    node_attributes, edge_attributes, type_per_edge = _create_attributes_gb(
-        graph,
-        gpb,
-        edge_ids,
-        is_homo,
-        store_inner_node,
-        store_inner_edge,
-        store_eids,
-        debug_mode,
-    )
-    # When converting DGLGraph to FusedCSCSamplingGraph, edge IDs are
-    # re-ordered(actually FusedCSCSamplingGraph does not have edge IDs
-    # in nature). So we need to save such re-order info for any
-    # operations that uses original local edge IDs. For now, this is
-    # required by `DistGraph.find_edges()` for link prediction tasks.
-    #
-    # What's more, in order to find the dst nodes efficiently, we save
-    # dst nodes directly in the edge attributes.
-    #
-    # So we require additional `(2 * E) * dtype` space in total.
-    if graph_formats is not None and isinstance(graph_formats, str):
-        graph_formats = [graph_formats]
-    save_coo = (
-        graph_formats is None and "coo" in graph.formats()["created"]
-    ) or (graph_formats is not None and "coo" in graph_formats)
-    if save_coo:
-        edge_attributes[DGL2GB_EID] = torch.argsort(edge_ids)
-        edge_attributes[GB_DST_ID] = gb.expand_indptr(
-            indptr, dtype=indices.dtype
-        )
-
-    indptr, indices, type_per_edge = cast_various_to_minimum_dtype_gb(
-        graph=graph,
-        part_meta=part_meta,
-        num_parts=num_parts,
-        indptr=indptr,
-        indices=indices,
-        type_per_edge=type_per_edge,
-        etypes=etypes,
-        ntypes=ntypes,
-        node_attributes=node_attributes,
-        edge_attributes=edge_attributes,
-    )
-
-    csc_graph = gb.fused_csc_sampling_graph(
-        indptr,
-        indices,
-        node_type_offset=None,
-        type_per_edge=type_per_edge,
-        node_attributes=node_attributes,
-        edge_attributes=edge_attributes,
-        node_type_to_id=node_type_to_id,
-        edge_type_to_id=edge_type_to_id,
-    )
-    return csc_graph
-
-
-def gb_convert_single_dgl_partition(
-    part_id,
-    graph_formats,
-    part_config,
-    store_eids=True,
-    store_inner_node=True,
-    store_inner_edge=True,
-):
-    """
-    The pipeline converting signle partition to graphbolt.
-
-    Parameters
-    ----------
-    part_id : int
-        The partition ID.
-    graph_formats : str or list[str]
-        Save partitions in specified formats. It could be any combination of
-        `coo`, `csc`. As `csc` format is mandatory for `FusedCSCSamplingGraph`,
-        it is not necessary to specify this argument. It's mainly for
-        specifying `coo` format to save edge ID mapping and destination node
-        IDs. If not specified, whether to save `coo` format is determined by
-        the availability of the format in DGL partitions. Default: None.
-    part_config : str
-        The path of the partition config file.
-    store_eids : bool, optional
-        Whether to store edge IDs in the new graph. Default: True.
-    store_inner_node : bool, optional
-        Whether to store inner node mask in the new graph. Default: False.
-    store_inner_edge : bool, optional
-        Whether to store inner edge mask in the new graph. Default: False.
-
-    Returns
-    -------
-    str
-        The path csc_graph to save.
-    """
-    gpb, _, ntypes, etypes = load_partition_book(
-        part_config=part_config, part_id=part_id
-    )
-    part = _load_part(part_config, part_id)
-    part_meta = copy.deepcopy(_load_part_config(part_config))
-    csc_graph = _convert_dgl_partition_to_gb(
-        graph=part,
-        ntypes=ntypes,
-        etypes=etypes,
-        gpb=gpb,
-        part_meta=part_meta,
-        graph_formats=graph_formats,
-        store_eids=store_eids,
-        store_inner_node=store_inner_node,
-        store_inner_edge=store_inner_edge,
-    )
-    rel_path = _save_graph_gb(part_config, part_id, csc_graph)
-    return rel_path
+    # Utility functions.
+    def is_homogeneous(ntypes, etypes):
+        return len(ntypes) == 1 and len(etypes) == 1
 
+    def init_type_per_edge(graph, gpb):
+        etype_ids = gpb.map_to_per_etype(graph.edata[EID])[0]
+        return etype_ids
 
-def _convert_partition_to_graphbolt_wrapper(
-    graph_formats,
-    part_config,
-    store_eids,
-    store_inner_node,
-    store_inner_edge,
-    n_jobs,
-    num_parts,
-):
     # [Rui] DGL partitions are always saved as homogeneous graphs even though
     # the original graph is heterogeneous. But heterogeneous information like
     # node/edge types are saved as node/edge data alongside with partitions.
@@ -1916,106 +1296,118 @@ def _convert_partition_to_graphbolt_wrapper(
     # We can simply pass None to it.
 
     # Iterate over partitions.
-    convert_with_format = partial(
-        gb_convert_single_dgl_partition,
-        part_config=part_config,
-        graph_formats=graph_formats,
-        store_eids=store_eids,
-        store_inner_node=store_inner_node,
-        store_inner_edge=store_inner_edge,
-    )
-    # Need to create entirely new interpreters, because we call C++ downstream
-    # See https://docs.python.org/3.12/library/multiprocessing.html#contexts-and-start-methods
-    # and https://pybind11.readthedocs.io/en/stable/advanced/misc.html#global-interpreter-lock-gil
-    rel_path_results = []
-    if n_jobs > 1 and num_parts > 1:
-        mp_ctx = mp.get_context("spawn")
-        with concurrent.futures.ProcessPoolExecutor(  # pylint: disable=unexpected-keyword-arg
-            max_workers=min(num_parts, n_jobs),
-            mp_context=mp_ctx,
-        ) as executor:
-            for part_id in range(num_parts):
-                rel_path_results.append(
-                    executor.submit(
-                        convert_with_format, part_id=part_id
-                    ).result()
+    for part_id in range(num_parts):
+        graph, _, _, gpb, _, _, _ = load_partition(
+            part_config, part_id, load_feats=False
+        )
+        _, _, ntypes, etypes = load_partition_book(part_config, part_id)
+        is_homo = is_homogeneous(ntypes, etypes)
+        node_type_to_id = (
+            None
+            if is_homo
+            else {ntype: ntid for ntid, ntype in enumerate(ntypes)}
+        )
+        edge_type_to_id = (
+            None
+            if is_homo
+            else {
+                gb.etype_tuple_to_str(etype): etid
+                for etype, etid in etypes.items()
+            }
+        )
+        # Obtain CSC indtpr and indices.
+        indptr, indices, edge_ids = graph.adj_tensors("csc")
+
+        # Save node attributes. Detailed attributes are shown below.
+        #  DGL_GB\Attributes  dgl.NID("_ID")  dgl.NTYPE("_TYPE")  "inner_node"  "part_id"
+        #  DGL_Homograph           ✅                🚫                  ✅          ✅
+        #  GB_Homograph            ✅                🚫               optional       🚫
+        #  DGL_Heterograph         ✅                ✅                  ✅          ✅
+        #  GB_Heterograph          ✅                🚫               optional       🚫
+        required_node_attrs = [NID]
+        if store_inner_node:
+            required_node_attrs.append("inner_node")
+        if debug_mode:
+            required_node_attrs = list(graph.ndata.keys())
+        node_attributes = {
+            attr: graph.ndata[attr] for attr in required_node_attrs
+        }
+
+        # Save edge attributes. Detailed attributes are shown below.
+        #  DGL_GB\Attributes  dgl.EID("_ID")  dgl.ETYPE("_TYPE")  "inner_edge"
+        #  DGL_Homograph           ✅               🚫                  ✅
+        #  GB_Homograph         optional            🚫               optional
+        #  DGL_Heterograph         ✅               ✅                  ✅
+        #  GB_Heterograph       optional            ✅               optional
+        type_per_edge = None
+        if not is_homo:
+            type_per_edge = init_type_per_edge(graph, gpb)[edge_ids]
+            type_per_edge = type_per_edge.to(RESERVED_FIELD_DTYPE[ETYPE])
+        required_edge_attrs = []
+        if store_eids:
+            required_edge_attrs.append(EID)
+        if store_inner_edge:
+            required_edge_attrs.append("inner_edge")
+        if debug_mode:
+            required_edge_attrs = list(graph.edata.keys())
+        edge_attributes = {
+            attr: graph.edata[attr][edge_ids] for attr in required_edge_attrs
+        }
+
+        # Cast various data to minimum dtype.
+        # Cast 1: indptr.
+        indptr = _cast_to_minimum_dtype(graph.num_edges(), indptr)
+        # Cast 2: indices.
+        indices = _cast_to_minimum_dtype(graph.num_nodes(), indices)
+        # Cast 3: type_per_edge.
+        type_per_edge = _cast_to_minimum_dtype(
+            len(etypes), type_per_edge, field=ETYPE
+        )
+        # Cast 4: node/edge_attributes.
+        predicates = {
+            NID: part_meta["num_nodes"],
+            "part_id": num_parts,
+            NTYPE: len(ntypes),
+            EID: part_meta["num_edges"],
+            ETYPE: len(etypes),
+        }
+        for attributes in [node_attributes, edge_attributes]:
+            for key in attributes:
+                if key not in predicates:
+                    continue
+                attributes[key] = _cast_to_minimum_dtype(
+                    predicates[key], attributes[key], field=key
                 )
 
-    else:
-        # If running single-threaded, avoid spawning new interpreter, which is slow
-        for part_id in range(num_parts):
-            rel_path = convert_with_format(part_id=part_id)
-            rel_path_results.append(rel_path)
-    part_meta = _load_part_config(part_config)
-    for part_id in range(num_parts):
+        csc_graph = gb.fused_csc_sampling_graph(
+            indptr,
+            indices,
+            node_type_offset=None,
+            type_per_edge=type_per_edge,
+            node_attributes=node_attributes,
+            edge_attributes=edge_attributes,
+            node_type_to_id=node_type_to_id,
+            edge_type_to_id=edge_type_to_id,
+        )
+        orig_graph_path = os.path.join(
+            os.path.dirname(part_config),
+            part_meta[f"part-{part_id}"]["part_graph"],
+        )
+        csc_graph_path = os.path.join(
+            os.path.dirname(orig_graph_path), "fused_csc_sampling_graph.pt"
+        )
+        torch.save(csc_graph, csc_graph_path)
+
         # Update graph path.
-        part_meta[f"part-{part_id}"]["part_graph_graphbolt"] = rel_path_results[
-            part_id
-        ]
+        new_part_meta[f"part-{part_id}"][
+            "part_graph_graphbolt"
+        ] = os.path.relpath(csc_graph_path, os.path.dirname(part_config))
 
     # Save dtype info into partition config.
     # [TODO][Rui] Always use int64_t for node/edge IDs in GraphBolt. See more
     # details in #7175.
-    part_meta["node_map_dtype"] = "int64"
-    part_meta["edge_map_dtype"] = "int64"
+    new_part_meta["node_map_dtype"] = "int64"
+    new_part_meta["edge_map_dtype"] = "int64"
 
-    return part_meta
-
-
-def dgl_partition_to_graphbolt(
-    part_config,
-    *,
-    store_eids=True,
-    store_inner_node=False,
-    store_inner_edge=False,
-    graph_formats=None,
-    n_jobs=1,
-):
-    """Convert partitions of dgl to FusedCSCSamplingGraph of GraphBolt.
-
-    This API converts `DGLGraph` partitions to `FusedCSCSamplingGraph` which is
-    dedicated for sampling in `GraphBolt`. New graphs will be stored alongside
-    original graph as `fused_csc_sampling_graph.pt`.
-
-    In the near future, partitions are supposed to be saved as
-    `FusedCSCSamplingGraph` directly. At that time, this API should be deprecated.
-
-    Parameters
-    ----------
-    part_config : str
-        The partition configuration JSON file.
-    store_eids : bool, optional
-        Whether to store edge IDs in the new graph. Default: True.
-    store_inner_node : bool, optional
-        Whether to store inner node mask in the new graph. Default: False.
-    store_inner_edge : bool, optional
-        Whether to store inner edge mask in the new graph. Default: False.
-    graph_formats : str or list[str], optional
-        Save partitions in specified formats. It could be any combination of
-        `coo`, `csc`. As `csc` format is mandatory for `FusedCSCSamplingGraph`,
-        it is not necessary to specify this argument. It's mainly for
-        specifying `coo` format to save edge ID mapping and destination node
-        IDs. If not specified, whether to save `coo` format is determined by
-        the availability of the format in DGL partitions. Default: None.
-    n_jobs: int
-        Number of parallel jobs to run during partition conversion. Max parallelism
-        is determined by the partition count.
-    """
-    debug_mode = "DGL_DIST_DEBUG" in os.environ
-    if debug_mode:
-        dgl_warning(
-            "Running in debug mode which means all attributes of DGL partitions"
-            " will be saved to the new format."
-        )
-    part_meta = _load_part_config(part_config)
-    num_parts = part_meta["num_parts"]
-    part_meta = _convert_partition_to_graphbolt_wrapper(
-        graph_formats=graph_formats,
-        part_config=part_config,
-        store_eids=store_eids,
-        store_inner_node=store_inner_node,
-        store_inner_edge=store_inner_edge,
-        n_jobs=n_jobs,
-        num_parts=num_parts,
-    )
-    _dump_part_config(part_config, part_meta)
+    _dump_part_config(part_config, new_part_meta)
+    print(f"Converted partitions to GraphBolt format into {part_config}")
diff --git a/python/dgl/nn/pytorch/hetero.py b/python/dgl/nn/pytorch/hetero.py
index a4292717..6c40d12c 100644
--- a/python/dgl/nn/pytorch/hetero.py
+++ b/python/dgl/nn/pytorch/hetero.py
@@ -187,9 +187,14 @@ class HeteroGraphConv(nn.Module):
                 src_inputs, dst_inputs = inputs
             else:
                 src_inputs = inputs
-                dst_inputs = {
-                    k: v[: g.number_of_dst_nodes(k)] for k, v in inputs.items()
-                }
+                for etype in g.etypes:
+                    if len(mod_kwargs[etype]) > 1:
+                        dst_inputs = inputs
+                    else:
+                        dst_inputs = {
+                            k: v[: g.number_of_dst_nodes(k)] for k, v in inputs.items()
+                        }
+                    break
 
             for stype, etype, dtype in g.canonical_etypes:
                 rel_graph = g[stype, etype, dtype]
diff --git a/python/dgl/partition.py b/python/dgl/partition.py
index 9727dca5..b63cbf26 100644
--- a/python/dgl/partition.py
+++ b/python/dgl/partition.py
@@ -4,6 +4,9 @@ import re
 import time
 
 import numpy as np
+import copy
+import torch
+import json
 
 from . import backend as F, utils
 from ._ffi.function import _init_api
@@ -18,6 +21,17 @@ __all__ = [
     "partition_graph_with_halo",
 ]
 
+RESERVED_FIELD_DTYPE = {
+    "inner_node": F.uint8,  # A flag indicates whether the node is inside a partition.
+    "inner_edge": F.uint8,  # A flag indicates whether the edge is inside a partition.
+    NID: F.int64,
+    EID: F.int64,
+    NTYPE: F.int16,
+    # `sort_csr_by_tag` and `sort_csc_by_tag` works on int32/64 only.
+    ETYPE: F.int32,
+}
+
+CANONICAL_ETYPE_DELIMITER = ":"
 
 def reorder_nodes(g, new_node_ids):
     """Generate a new graph with new node IDs.
@@ -257,7 +271,281 @@ def partition_graph_with_halo(g, node_part, extra_cached_hops, reshuffle=False):
     else:
         return subg_dict, None, None
 
+def _format_part_metadata(part_metadata, formatter):
+    """Format etypes with specified formatter."""
+    for key in ["edge_map", "etypes"]:
+        if key not in part_metadata:
+            continue
+        orig_data = part_metadata[key]
+        if not isinstance(orig_data, dict):
+            continue
+        new_data = {}
+        for etype, data in orig_data.items():
+            etype = formatter(etype)
+            new_data[etype] = data
+        part_metadata[key] = new_data
+    return part_metadata
+
+def _etype_tuple_to_str(c_etype):
+    return CANONICAL_ETYPE_DELIMITER.join(c_etype)
+
+def _dump_part_config(part_config, part_metadata):
+    """Format and dump part config."""
+    part_metadata = _format_part_metadata(part_metadata, _etype_tuple_to_str)
+    with open(part_config, "w") as outfile:
+        json.dump(part_metadata, outfile, sort_keys=False, indent=4)
+
+
+def _save_graphs(filename, g_list, formats=None, sort_etypes=False):
+    """Preprocess partitions before saving:
+    1. format data types.
+    2. sort csc/csr by tag.
+    """
+    for g in g_list:
+        for k, dtype in RESERVED_FIELD_DTYPE.items():
+            if k in g.ndata:
+                g.ndata[k] = F.astype(g.ndata[k], dtype)
+            if k in g.edata:
+                g.edata[k] = F.astype(g.edata[k], dtype)
+    for g in g_list:
+        if (not sort_etypes) or (formats is None):
+            continue
+        if "csr" in formats:
+            g = sort_csr_by_tag(g, tag=g.edata[ETYPE], tag_type="edge")
+        if "csc" in formats:
+            g = sort_csc_by_tag(g, tag=g.edata[ETYPE], tag_type="edge")
+    save_graphs(filename, g_list, formats=formats)
+
+
+def _get_inner_node_mask(graph, ntype_id):
+    if NTYPE in graph.ndata:
+        dtype = F.dtype(graph.ndata["inner_node"])
+        return (
+            graph.ndata["inner_node"]
+            * F.astype(graph.ndata[NTYPE] == ntype_id, dtype)
+            == 1
+        )
+    else:
+        return graph.ndata["inner_node"] == 1
+
+
+def _get_inner_edge_mask(graph, etype_id):
+    if ETYPE in graph.edata:
+        dtype = F.dtype(graph.edata["inner_edge"])
+        return (
+            graph.edata["inner_edge"]
+            * F.astype(graph.edata[ETYPE] == etype_id, dtype)
+            == 1
+        )
+    else:
+        return graph.edata["inner_edge"] == 1
+
+def partition_graph_with_halo_hetero(g, node_part, extra_cached_hops, graph_name, part_method, num_parts, out_path, 
+        ndata, edata, ntypes, etypes, num_nodes, num_edges, ntype_id, etype_id, reshuffle=False):
+    """Partition a graph.
+
+    Based on the given node assignments for each partition, the function splits
+    the input graph into subgraphs. A subgraph may contain HALO nodes which does
+    not belong to the partition of a subgraph but are connected to the nodes
+    in the partition within a fixed number of hops.
+
+    If `reshuffle` is turned on, the function reshuffles node IDs and edge IDs
+    of the input graph before partitioning. After reshuffling, all nodes and edges
+    in a partition fall in a contiguous ID range in the input graph.
+    The partitioend subgraphs have node data 'orig_id', which stores the node IDs
+    in the original input graph.
+
+    Parameters
+    ------------
+    g: DGLGraph
+        The graph to be partitioned
+    node_part: 1D tensor
+        Specify which partition a node is assigned to. The length of this tensor
+        needs to be the same as the number of nodes of the graph. Each element
+        indicates the partition ID of a node.
+    extra_cached_hops: int
+        The number of hops a HALO node can be accessed.
+    reshuffle : bool
+        Resuffle nodes so that nodes in the same partition are in the same ID range.
+
+    Returns
+    --------
+    a dict of DGLGraphs
+        The key is the partition ID and the value is the DGLGraph of the partition.
+    Tensor
+        1D tensor that stores the mapping between the reshuffled node IDs and
+        the original node IDs if 'reshuffle=True'. Otherwise, return None.
+    Tensor
+        1D tensor that stores the mapping between the reshuffled edge IDs and
+        the original edge IDs if 'reshuffle=True'. Otherwise, return None.
+    """
+    assert len(node_part) == g.num_nodes()
+    if reshuffle:
+        g, node_part = reshuffle_graph(g, node_part)
+        orig_nids = g.ndata["orig_id"]
+        orig_eids = g.edata["orig_id"]
+
+    node_part = utils.toindex(node_part)
+    start = time.time()
+    subgs = _CAPI_DGLPartitionWithHalo_Hetero(
+        g._graph, node_part.todgltensor(), extra_cached_hops
+    )
+
+    # g is no longer needed. Free memory.
+    g = None
+    print("Split the graph: {:.3f} seconds".format(time.time() - start))
+    node_part = node_part.tousertensor()
+    start = time.time()
+
+    os.makedirs(out_path, mode=0o775, exist_ok=True)
+    tot_num_inner_edges = 0
+    out_path = os.path.abspath(out_path)
+
+    # This function determines whether an edge belongs to a partition.
+    # An edge is assigned to a partition based on its destination node. If its destination node
+    # is assigned to a partition, we assign the edge to the partition as well.
+    def get_inner_edge(subg, inner_node):
+        inner_edge = F.zeros((subg.num_edges(),), F.int8, F.cpu())
+        inner_nids = F.nonzero_1d(inner_node)
+        # TODO(zhengda) we need to fix utils.toindex() to avoid the dtype cast below.
+        inner_nids = F.astype(inner_nids, F.int64)
+        inner_eids = subg.in_edges(inner_nids, form="eid")
+        inner_edge = F.scatter_row(
+            inner_edge,
+            inner_eids,
+            F.ones((len(inner_eids),), F.dtype(inner_edge), F.cpu()),
+        )
+        return inner_edge
 
+    # This creaets a subgraph from subgraphs returned from the CAPI above.
+    def create_subgraph(subg, induced_nodes, induced_edges, inner_node):
+        subg1 = DGLGraph(gidx=subg.graph, ntypes=["_N"], etypes=["_E"])
+        # If IDs are shuffled, we should shuffled edges. This will help us collect edge data
+        # from the distributed graph after training.
+        if reshuffle:
+            # When we shuffle edges, we need to make sure that the inner edges are assigned with
+            # contiguous edge IDs and their ID range starts with 0. In other words, we want to
+            # place these edge IDs in the front of the edge list. To ensure that, we add the IDs
+            # of outer edges with a large value, so we will get the sorted list as we want.
+            max_eid = F.max(induced_edges[0], 0) + 1
+            inner_edge = get_inner_edge(subg1, inner_node)
+            eid = F.astype(induced_edges[0], F.int64) + max_eid * F.astype(
+                inner_edge == 0, F.int64
+            )
+
+            _, index = F.sort_1d(eid)
+            subg1 = edge_subgraph(subg1, index, relabel_nodes=False)
+            subg1.ndata[NID] = induced_nodes[0]
+            subg1.edata[EID] = F.gather_row(induced_edges[0], index)
+        else:
+            subg1.ndata[NID] = induced_nodes[0]
+            subg1.edata[EID] = induced_edges[0]
+        return subg1
+
+    for i, subg in enumerate(subgs):
+        inner_node = _get_halo_heterosubgraph_inner_node(subg)
+        inner_node = F.zerocopy_from_dlpack(inner_node.to_dlpack())
+        subg = create_subgraph(
+            subg, subg.induced_nodes, subg.induced_edges, inner_node
+        )
+        subg.ndata["inner_node"] = inner_node
+        subg.ndata["part_id"] = F.gather_row(node_part, subg.ndata[NID])
+        if reshuffle:
+            subg.ndata["orig_id"] = F.gather_row(orig_nids, subg.ndata[NID])
+            subg.edata["orig_id"] = F.gather_row(orig_eids, subg.edata[EID])
+
+        if extra_cached_hops >= 1:
+            inner_edge = get_inner_edge(subg, inner_node)
+        else:
+            inner_edge = F.ones((subg.num_edges(),), F.int8, F.cpu())
+        subg.edata["inner_edge"] = inner_edge
+
+        #ntype = F.gather_row(sim_g.ndata[NTYPE], subg.ndata["orig_id"])
+        ntype = F.gather_row(ndata[NTYPE], subg.ndata["orig_id"])
+        subg.ndata[NTYPE] = F.astype(ntype, RESERVED_FIELD_DTYPE[NTYPE])
+        assert np.all(F.asnumpy(ntype) == F.asnumpy(subg.ndata[NTYPE]))
+
+        #etype = F.gather_row(sim_g.edata[ETYPE], subg.edata["orig_id"])
+        etype = F.gather_row(edata[ETYPE], subg.edata["orig_id"])
+        subg.edata[ETYPE] = F.astype(etype, RESERVED_FIELD_DTYPE[ETYPE])
+        assert np.all(F.asnumpy(etype) == F.asnumpy(subg.edata[ETYPE]))
+        
+        inner_ntype = F.boolean_mask(subg.ndata[NTYPE], subg.ndata["inner_node"] == 1)
+        inner_nids = F.boolean_mask(subg.ndata[NID], subg.ndata["inner_node"] == 1)
+
+        for nt in ntypes:
+            inner_ntype_mask = inner_ntype == ntype_id[nt]
+            typed_nids = F.boolean_mask(inner_nids, inner_ntype_mask)
+            expected_range = np.arange(
+                int(F.as_scalar(typed_nids[0])),
+                int(F.as_scalar(typed_nids[-1])) + 1,
+            )
+            assert np.all(F.asnumpy(typed_nids) == expected_range)
+
+        inner_etype = F.boolean_mask(
+            subg.edata[ETYPE], subg.edata["inner_edge"] == 1
+        )
+        inner_eids = F.boolean_mask(
+            subg.edata[EID], subg.edata["inner_edge"] == 1
+        )
+        for et in etypes:
+            inner_etype_mask = inner_etype == etype_id[et]
+            typed_eids = np.sort(
+                F.asnumpy(F.boolean_mask(inner_eids, inner_etype_mask))
+            )
+            assert np.all(
+                typed_eids
+                == np.arange(int(typed_eids[0]), int(typed_eids[-1]) + 1)
+            )
+        part_dir = os.path.join(out_path, "part" + str(i))
+        os.makedirs(part_dir, mode=0o775, exist_ok=True)
+        part_dir = os.path.abspath(part_dir)
+        #part_graph_file = os.path.join(part_dir, "graph.dgl")
+        part_graph_file = os.path.join(part_dir, "graph.pt")
+        torch.save(subg, part_graph_file)
+
+        #_save_graphs(
+        #       part_graph_file,
+        #       [subg],
+        #       formats=None,
+        #       sort_etypes=sort_etypes,
+        #)
+        node_map_val={}
+        edge_map_val={}
+        for nt in ntypes:
+            ntid = ntype_id[nt]
+            inner_node_mask = _get_inner_node_mask(subg, ntid)
+            inner_nids = F.boolean_mask(subg.ndata[NID], inner_node_mask)
+            node_map_val[nt] = [int(F.as_scalar(inner_nids[0])),
+                                int(F.as_scalar(inner_nids[-1])) + 1,
+                               ]
+
+        for et in etypes:
+            etid = etype_id[et]
+            inner_edge_mask = _get_inner_edge_mask(subg, etid)
+            inner_eids = np.sort(
+                F.asnumpy(
+                    F.boolean_mask(subg.edata[EID], inner_edge_mask)
+                )
+            )
+            edge_map_val[et] = [int(inner_eids[0]), int(inner_eids[-1]) + 1]
+
+        ntype_d = {ntype: ntype_id[ntype] for ntype in ntypes}
+        etype_d = {etype: etype_id[etype] for etype in etypes}
+        part_metadata = {
+            "graph_name": graph_name,
+            "num_nodes": num_nodes,
+            "num_edges": num_edges,
+            "part_method": part_method,
+            "num_parts": num_parts,
+            "halo_hops": extra_cached_hops,
+            "node_map": node_map_val,
+            "edge_map": edge_map_val,
+            "ntypes": ntype_d,
+            "etypes": etype_d,
+        }
+        _dump_part_config(f"{part_dir}/{graph_name}.json", part_metadata)
+        
 def get_peak_mem():
     """Get the peak memory size.
 
diff --git a/src/array/cpu/array_index_select.cc b/src/array/cpu/array_index_select.cc
index c19cecd9..34f490b6 100644
--- a/src/array/cpu/array_index_select.cc
+++ b/src/array/cpu/array_index_select.cc
@@ -33,6 +33,8 @@ template NDArray IndexSelect<kDGLCPU, int32_t, int32_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, int32_t, int64_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, int64_t, int32_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, int64_t, int64_t>(NDArray, IdArray);
+template NDArray IndexSelect<kDGLCPU, BFloat16, int32_t>(NDArray, IdArray);
+template NDArray IndexSelect<kDGLCPU, BFloat16, int64_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, float, int32_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, float, int64_t>(NDArray, IdArray);
 template NDArray IndexSelect<kDGLCPU, double, int32_t>(NDArray, IdArray);
@@ -46,6 +48,7 @@ DType IndexSelect(NDArray array, int64_t index) {
 
 template int32_t IndexSelect<kDGLCPU, int32_t>(NDArray array, int64_t index);
 template int64_t IndexSelect<kDGLCPU, int64_t>(NDArray array, int64_t index);
+template BFloat16 IndexSelect<kDGLCPU, BFloat16>(NDArray array, int64_t index);
 template float IndexSelect<kDGLCPU, float>(NDArray array, int64_t index);
 template double IndexSelect<kDGLCPU, double>(NDArray array, int64_t index);
 
diff --git a/src/array/cpu/array_pack.cc b/src/array/cpu/array_pack.cc
index aa571da2..bf2b2dca 100644
--- a/src/array/cpu/array_pack.cc
+++ b/src/array/cpu/array_pack.cc
@@ -46,6 +46,8 @@ template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, int32_t, int32_t>(
     NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, int64_t, int32_t>(
     NDArray, IdArray);
+template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, bfloat16, int32_t>(
+    NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, float, int32_t>(
     NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, double, int32_t>(
@@ -54,6 +56,8 @@ template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, int32_t, int64_t>(
     NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, int64_t, int64_t>(
     NDArray, IdArray);
+template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, bfloat16, int64_t>(
+    NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, float, int64_t>(
     NDArray, IdArray);
 template std::pair<NDArray, IdArray> ConcatSlices<kDGLCPU, double, int64_t>(
@@ -87,6 +91,8 @@ template std::tuple<NDArray, IdArray, IdArray> Pack<kDGLCPU, int32_t>(
     NDArray, int32_t);
 template std::tuple<NDArray, IdArray, IdArray> Pack<kDGLCPU, int64_t>(
     NDArray, int64_t);
+template std::tuple<NDArray, IdArray, IdArray> Pack<kDGLCPU, bfloat16>(
+    NDArray, bfloat16);
 template std::tuple<NDArray, IdArray, IdArray> Pack<kDGLCPU, float>(
     NDArray, float);
 template std::tuple<NDArray, IdArray, IdArray> Pack<kDGLCPU, double>(
diff --git a/src/array/cpu/array_repeat.cc b/src/array/cpu/array_repeat.cc
index 0b2098a6..89bd9e74 100644
--- a/src/array/cpu/array_repeat.cc
+++ b/src/array/cpu/array_repeat.cc
@@ -39,10 +39,12 @@ NDArray Repeat(NDArray array, IdArray repeats) {
 
 template NDArray Repeat<kDGLCPU, int32_t, int32_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, int64_t, int32_t>(NDArray, IdArray);
+template NDArray Repeat<kDGLCPU, bfloat16, int32_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, float, int32_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, double, int32_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, int32_t, int64_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, int64_t, int64_t>(NDArray, IdArray);
+template NDArray Repeat<kDGLCPU, bfloat16, int64_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, float, int64_t>(NDArray, IdArray);
 template NDArray Repeat<kDGLCPU, double, int64_t>(NDArray, IdArray);
 
diff --git a/src/array/cpu/array_scatter.cc b/src/array/cpu/array_scatter.cc
index 8ab54156..893ee355 100644
--- a/src/array/cpu/array_scatter.cc
+++ b/src/array/cpu/array_scatter.cc
@@ -28,10 +28,12 @@ NDArray Scatter(NDArray array, IdArray indices) {
 
 template NDArray Scatter<kDGLCPU, int32_t, int32_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, int64_t, int32_t>(NDArray, IdArray);
+template NDArray Scatter<kDGLCPU, bfloat16, int32_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, float, int32_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, double, int32_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, int32_t, int64_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, int64_t, int64_t>(NDArray, IdArray);
+template NDArray Scatter<kDGLCPU, bfloat16, int64_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, float, int64_t>(NDArray, IdArray);
 template NDArray Scatter<kDGLCPU, double, int64_t>(NDArray, IdArray);
 
@@ -50,10 +52,12 @@ void Scatter_(IdArray index, NDArray value, NDArray out) {
 
 template void Scatter_<kDGLCPU, int32_t, int32_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, int64_t, int32_t>(IdArray, NDArray, NDArray);
+template void Scatter_<kDGLCPU, bfloat16, int32_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, float, int32_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, double, int32_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, int32_t, int64_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, int64_t, int64_t>(IdArray, NDArray, NDArray);
+template void Scatter_<kDGLCPU, bfloat16, int64_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, float, int64_t>(IdArray, NDArray, NDArray);
 template void Scatter_<kDGLCPU, double, int64_t>(IdArray, NDArray, NDArray);
 
diff --git a/src/array/cpu/rowwise_topk.cc b/src/array/cpu/rowwise_topk.cc
index 2ce522ad..64922f7a 100644
--- a/src/array/cpu/rowwise_topk.cc
+++ b/src/array/cpu/rowwise_topk.cc
@@ -83,6 +83,10 @@ template COOMatrix CSRRowWiseTopk<kDGLCPU, int32_t, int64_t>(
     CSRMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix CSRRowWiseTopk<kDGLCPU, int64_t, int64_t>(
     CSRMatrix, IdArray, int64_t, NDArray, bool);
+template COOMatrix CSRRowWiseTopk<kDGLCPU, int32_t, bfloat16>(
+    CSRMatrix, IdArray, int64_t, NDArray, bool);
+template COOMatrix CSRRowWiseTopk<kDGLCPU, int64_t, bfloat16>(
+    CSRMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix CSRRowWiseTopk<kDGLCPU, int32_t, float>(
     CSRMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix CSRRowWiseTopk<kDGLCPU, int64_t, float>(
@@ -108,6 +112,10 @@ template COOMatrix COORowWiseTopk<kDGLCPU, int32_t, int64_t>(
     COOMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix COORowWiseTopk<kDGLCPU, int64_t, int64_t>(
     COOMatrix, IdArray, int64_t, NDArray, bool);
+template COOMatrix COORowWiseTopk<kDGLCPU, int32_t, bfloat16>(
+    COOMatrix, IdArray, int64_t, NDArray, bool);
+template COOMatrix COORowWiseTopk<kDGLCPU, int64_t, bfloat16>(
+    COOMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix COORowWiseTopk<kDGLCPU, int32_t, float>(
     COOMatrix, IdArray, int64_t, NDArray, bool);
 template COOMatrix COORowWiseTopk<kDGLCPU, int64_t, float>(
diff --git a/src/array/cpu/sddmm.h b/src/array/cpu/sddmm.h
index 9e372bfc..cf7587ac 100644
--- a/src/array/cpu/sddmm.h
+++ b/src/array/cpu/sddmm.h
@@ -29,7 +29,7 @@ namespace cpu {
 template <
     typename IdType, typename DType, typename Op, int LhsTarget = 0,
     int RhsTarget = 2>
-void SDDMMCsr(
+void SDDMMCsrNaive(
     const BcastOff& bcast, const CSRMatrix& csr, NDArray lhs, NDArray rhs,
     NDArray out) {
   const bool has_idx = !IsNullArray(csr.data);
@@ -81,7 +81,7 @@ void SDDMMCsr(
 template <
     typename IdType, typename DType, typename Op, int LhsTarget = 0,
     int RhsTarget = 2>
-void SDDMMCoo(
+void SDDMMCooNaive(
     const BcastOff& bcast, const COOMatrix& coo, NDArray lhs, NDArray rhs,
     NDArray out) {
   const bool has_idx = !IsNullArray(coo.data);
@@ -222,6 +222,34 @@ struct Dot {
 
 }  // namespace op
 
+#ifdef USE_LIBXSMM
+#include "sddmm_blocking_libxsmm.h"
+#endif
+
+template <typename IdType, typename DType, typename Op,
+          int LhsTarget = 0, int RhsTarget = 2>
+void SDDMMCsr(const BcastOff& bcast,
+              const CSRMatrix& csr,
+              NDArray lhs, NDArray rhs, NDArray out) {
+#ifdef USE_LIBXSMM
+  SDDMMCsrLibxsmm<IdType, DType, Op, LhsTarget, RhsTarget>(bcast, csr, lhs, rhs, out);
+#else
+  SDDMMCsrNaive<IdType, DType, Op, LhsTarget, RhsTarget>(bcast, csr, lhs, rhs, out);
+#endif
+}
+
+template <typename IdType, typename DType, typename Op,
+          int LhsTarget = 0, int RhsTarget = 2>
+void SDDMMCoo(const BcastOff& bcast,
+              const COOMatrix& coo,
+              NDArray lhs, NDArray rhs, NDArray out) {
+#ifdef USE_LIBXSMM
+  SDDMMCooLibxsmm<IdType, DType, Op, LhsTarget, RhsTarget>(bcast, coo, lhs, rhs, out);
+#else
+  SDDMMCooNaive<IdType, DType, Op, LhsTarget, RhsTarget>(bcast, coo, lhs,  rhs, out);
+#endif
+}
+
 }  // namespace cpu
 }  // namespace aten
 }  // namespace dgl
diff --git a/src/array/cpu/sddmm_blocking_libxsmm.h b/src/array/cpu/sddmm_blocking_libxsmm.h
new file mode 100644
index 00000000..ee825d09
--- /dev/null
+++ b/src/array/cpu/sddmm_blocking_libxsmm.h
@@ -0,0 +1,249 @@
+/*!
+ *  Copyright (c) 2020 by Contributors
+ * \file array/cpu/sddmm.h
+ * \brief SDDMM CPU kernel function header.
+ */
+#ifndef DGL_ARRAY_CPU_SDDMM_BLOCKING_LIBXSMM_H_
+#define DGL_ARRAY_CPU_SDDMM_BLOCKING_LIBXSMM_H_
+
+#include <dgl/array.h>
+#include <dgl/bcast.h>
+#include "../selector.h"
+#ifdef USE_LIBXSMM
+#include <libxsmm_source.h>
+#include <unistd.h>
+#include "xsmm_functors.h"
+
+/*!
+ * \brief CPU kernel of g-SDDMM on Csr format using LIBXSMM.
+ * \param bcast Broadcast information.
+ * \param csr The Csr matrix.
+ * \param lhs The left hand side operand feature.
+ * \param rhs The right hand size operand feature.
+ * \param out The result feature on edges.
+ * \note it uses node parallel strategy, different threads are responsible
+ *       for the computation of different nodes.
+ */
+template <typename IdType, typename DType, typename Op,
+          int LhsTarget = 0, int RhsTarget = 2>
+void SDDMMCsrLibxsmm(const BcastOff& bcast,
+              const CSRMatrix& csr,
+              NDArray lhs, NDArray rhs, NDArray out) {
+  const bool has_idx = !IsNullArray(csr.data);
+  const IdType* indptr = csr.indptr.Ptr<IdType>();
+  IdType* indices = csr.indices.Ptr<IdType>();
+  IdType* edges = csr.data.Ptr<IdType>();
+  DType* X = lhs.Ptr<DType>();
+  DType* Y = rhs.Ptr<DType>();
+  const int64_t dim = bcast.out_len,
+                lhs_dim = bcast.lhs_len,
+                rhs_dim = bcast.rhs_len,
+                reduce_size = bcast.reduce_size;
+  DType* O = out.Ptr<DType>();
+
+  if(std::is_same<Op, dgl::aten::cpu::op::Add<DType>>::value) {
+    auto add_tpp = dgl_tpp::AddTPP<DType>(1, dim);
+#pragma omp parallel for
+    for(IdType i = 0; i < csr.num_rows; i++) {
+      IdType row_start = indptr[i];
+      IdType row_end = indptr[i+1];
+
+      auto n = row_end - row_start;
+
+      IdType *eid = has_idx ? &edges[row_start] : &row_start;
+      IdType *cid = &indices[row_start];
+
+      for(auto r=0; r<n; r++) {
+	add_tpp(&X[cid[r]*dim], &Y[cid[r]*dim], &O[eid[r]*dim]);
+      }
+    }
+  }
+  else if(std::is_same<Op, dgl::aten::cpu::op::Sub<DType>>::value) {
+    auto sub_tpp = dgl_tpp::SubTPP<DType>(1, dim);
+#pragma omp parallel for
+    for(IdType i = 0; i < csr.num_rows; i++) {
+      IdType row_start = indptr[i];
+      IdType row_end = indptr[i+1];
+
+      auto n = row_end - row_start;
+
+      IdType *eid = has_idx ? &edges[row_start] : &row_start;
+      IdType *cid = &indices[row_start];
+
+      for(auto r=0; r<n; r++) {
+	sub_tpp(&X[cid[r]*dim], &Y[cid[r]*dim], &O[eid[r]*dim]);
+      }
+    }
+  }
+  else if(std::is_same<Op, dgl::aten::cpu::op::Mul<DType>>::value) {
+    auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+#pragma omp parallel for
+    for(IdType i = 0; i < csr.num_rows; i++) {
+      IdType row_start = indptr[i];
+      IdType row_end = indptr[i+1];
+
+      auto n = row_end - row_start;
+
+      IdType *eid = has_idx ? &edges[row_start] : &row_start;
+      IdType *cid = &indices[row_start];
+
+      for(auto r=0; r<n; r++) {
+	mul_tpp(&X[cid[r]*dim], &Y[cid[r]*dim], &O[eid[r]*dim]);
+      }
+    }
+  }
+  else if(std::is_same<Op, dgl::aten::cpu::op::Div<DType>>::value) {
+    auto div_tpp = dgl_tpp::DivTPP<DType>(1, dim);
+#pragma omp parallel for
+    for(IdType i = 0; i < csr.num_rows; i++) {
+      IdType row_start = indptr[i];
+      IdType row_end = indptr[i+1];
+
+      auto n = row_end - row_start;
+
+      IdType *eid = has_idx ? &edges[row_start] : &row_start;
+      IdType *cid = &indices[row_start];
+
+      for(auto r=0; r<n; r++) {
+	div_tpp(&X[cid[r]*dim], &Y[cid[r]*dim], &O[eid[r]*dim]);
+      }
+    }
+  }
+  else if(std::is_same<Op, dgl::aten::cpu::op::Dot<DType>>::value) {
+    auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+    auto reduce_add_tpp = dgl_tpp::ReduceAddColsTPP<DType>(dim, reduce_size);
+#pragma omp parallel for
+    for(IdType i = 0; i < csr.num_rows; i++) {
+      IdType row_start = indptr[i];
+      IdType row_end = indptr[i+1];
+
+      auto n = row_end - row_start;
+
+      IdType *eid = has_idx ? &edges[row_start] : &row_start;
+      IdType *cid = &indices[row_start];
+      DType evec[lhs_dim];
+
+      for(auto r=0; r<n; r++) {
+	mul_tpp(&X[cid[r]*lhs_dim], &Y[cid[r]*rhs_dim], evec);
+	reduce_add_tpp(evec, &O[eid[r]*dim]);
+      }
+    }
+  }
+}
+
+/*!
+ * \brief CPU kernel of g-SDDMM on Coo format using LIBXSMM.
+ * \param bcast Broadcast information.
+ * \param coo The COO matrix.
+ * \param lhs The left hand side operand feature.
+ * \param rhs The right hand size operand feature.
+ * \param out The result feature on edges.
+ * \note it uses edge parallel strategy, different threads are responsible
+ *       for the computation of different edges.
+ */
+template <typename IdType, typename DType, typename Op,
+          int LhsTarget = 0, int RhsTarget = 2>
+void SDDMMCooLibxsmm(const BcastOff& bcast,
+              const COOMatrix& coo,
+              NDArray lhs, NDArray rhs, NDArray out) {
+  const bool has_idx = !IsNullArray(coo.data);
+  IdType* row = coo.row.Ptr<IdType>();
+  IdType* col = coo.col.Ptr<IdType>();
+  IdType* edges = has_idx ? coo.data.Ptr<IdType>() : NULL;
+  DType* X = lhs.Ptr<DType>();
+  DType* Y = rhs.Ptr<DType>();
+  const int64_t dim = bcast.out_len,
+                lhs_dim = bcast.lhs_len,
+                rhs_dim = bcast.rhs_len,
+                reduce_size = bcast.reduce_size;
+  DType* O = out.Ptr<DType>();
+
+  auto N = coo.row->shape[0];
+
+  if(edges == NULL) {
+    if(std::is_same<Op, dgl::aten::cpu::op::Add<DType>>::value) {
+      auto add_tpp = dgl_tpp::AddTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	add_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[n*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Sub<DType>>::value) {
+      auto sub_tpp = dgl_tpp::SubTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	sub_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[n*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Mul<DType>>::value) {
+      auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	mul_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[n*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Div<DType>>::value) {
+      auto div_tpp = dgl_tpp::DivTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	div_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[n*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Dot<DType>>::value){
+      auto mul_tpp = dgl_tpp::MulTPP<DType>(1, lhs_dim);
+      auto reduce_add_tpp = dgl_tpp::ReduceAddColsTPP<DType>(dim, reduce_size);
+
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	DType fvec[lhs_dim];
+	mul_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], fvec);
+	reduce_add_tpp(fvec, &O[n*dim]);
+      }
+    }
+  }
+  else {
+
+    if(std::is_same<Op, dgl::aten::cpu::op::Add<DType>>::value) {
+      auto add_tpp = dgl_tpp::AddTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	add_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[edges[n]*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Sub<DType>>::value) {
+      auto sub_tpp = dgl_tpp::SubTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	sub_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[edges[n]*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Mul<DType>>::value) {
+      auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	mul_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[edges[n]*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Div<DType>>::value) {
+      auto div_tpp = dgl_tpp::DivTPP<DType>(1, dim);
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	div_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], &O[edges[n]*dim]);
+      }
+    }
+    else if(std::is_same<Op, dgl::aten::cpu::op::Dot<DType>>::value) {
+      auto mul_tpp = dgl_tpp::MulTPP<DType>(1, lhs_dim);
+      auto reduce_add_tpp = dgl_tpp::ReduceAddColsTPP<DType>(dim, reduce_size);
+
+#pragma omp parallel for
+      for(auto n = 0; n < N; n++) {
+	DType fvec[lhs_dim];
+	mul_tpp(&X[row[n]*lhs_dim], &Y[col[n]*rhs_dim], fvec);
+	reduce_add_tpp(fvec, &O[edges[n]*dim]);
+      }
+    }
+  }
+}
+
+#endif
+#endif  // DGL_ARRAY_CPU_SDDMM_BLOCKING_LIBXSMM_H_
diff --git a/src/array/cpu/spmm.h b/src/array/cpu/spmm.h
index aea4a0a5..6299063e 100644
--- a/src/array/cpu/spmm.h
+++ b/src/array/cpu/spmm.h
@@ -141,11 +141,8 @@ void SpMMSumCsr(
   }
 #if !defined(_WIN32)
 #ifdef USE_LIBXSMM
-  int cpu_id = libxsmm_cpuid_x86();
-  const bool no_libxsmm =
-      bcast.use_bcast || std::is_same<DType, double>::value ||
-      (std::is_same<DType, BFloat16>::value && cpu_id < LIBXSMM_X86_AVX512) ||
-      !dgl::runtime::Config::Global()->IsLibxsmmAvailable();
+  const bool no_libxsmm = std::is_same<DType, double>::value ||
+                          !dgl::runtime::Config::Global()->IsLibxsmmAvailable();
   if (!no_libxsmm) {
     SpMMSumCsrLibxsmm<IdType, DType, Op>(bcast, csr, ufeat, efeat, out);
   } else {
@@ -266,10 +263,8 @@ void SpMMCmpCsr(
   }
 #if !defined(_WIN32)
 #ifdef USE_LIBXSMM
-  int cpu_id = libxsmm_cpuid_x86();
-  const bool no_libxsmm = bcast.use_bcast ||
-                          std::is_same<DType, double>::value ||
-                          cpu_id < LIBXSMM_X86_AVX512 ||
+
+  const bool no_libxsmm = std::is_same<DType, double>::value ||
                           !dgl::runtime::Config::Global()->IsLibxsmmAvailable();
   if (!no_libxsmm) {
     SpMMCmpCsrLibxsmm<IdType, DType, Op, Cmp>(
@@ -482,7 +477,7 @@ void SpMMCmpCoo(
  * @param out The result of edge_softmax_forward.
  */
 template <typename IdType, typename DType, typename Op>
-void Edge_softmax_csr_forward(
+void Edge_softmax_csr_forward_base(
     const BcastOff& bcast, const CSRMatrix& csr, NDArray ufeat, NDArray efeat,
     NDArray out) {
   const bool has_idx = !IsNullArray(csr.data);
@@ -530,7 +525,7 @@ void Edge_softmax_csr_forward(
  * @param back_out The result of edge_softmax_backward.
  */
 template <typename IdType, typename DType, typename Op>
-void Edge_softmax_csr_backward(
+void Edge_softmax_csr_backward_base(
     const BcastOff& bcast, const CSRMatrix& csr, NDArray out, NDArray sds,
     NDArray back_out) {
   typedef typename std::conditional<
@@ -569,6 +564,25 @@ void Edge_softmax_csr_backward(
   });
 }
 
+template <typename IdType, typename DType, typename Op>
+void Edge_softmax_csr_forward(const BcastOff& bcast, const CSRMatrix& csr, NDArray ufeat,
+                NDArray efeat, NDArray out) {
+#ifdef USE_LIBXSMM
+  Edge_softmax_csr_forward_libxsmm<IdType, DType, Op>(bcast, csr, ufeat, efeat, out);
+#else
+  Edge_softmax_csr_forward_base<IdType, DType, Op>(bcast, csr, ufeat, efeat, out);
+#endif
+}
+
+template <typename IdType, typename DType, typename Op>
+void Edge_softmax_csr_backward(const BcastOff& bcast, const CSRMatrix& csr, NDArray out,
+                NDArray sds, NDArray back_out) {
+#ifdef USE_LIBXSMM
+  Edge_softmax_csr_backward_libxsmm<IdType, DType, Op>(bcast, csr, out, sds, back_out);
+#else
+  Edge_softmax_csr_backward_base<IdType, DType, Op>(bcast, csr, out, sds, back_out);
+#endif
+}
 }  // namespace cpu
 }  // namespace aten
 }  // namespace dgl
diff --git a/src/array/cpu/spmm_blocking_libxsmm.h b/src/array/cpu/spmm_blocking_libxsmm.h
index de3579fb..e51a0543 100644
--- a/src/array/cpu/spmm_blocking_libxsmm.h
+++ b/src/array/cpu/spmm_blocking_libxsmm.h
@@ -1,8 +1,8 @@
-/**
+/*!
  *  Copyright (c) 2021 Intel Corporation
- * @file array/cpu/spmm.h
- * @brief SPMM CPU kernel function header.
- * @author Sanchit Misra <sanchit.misra@intel.com>,
+ * \file array/cpu/spmm.h
+ * \brief SPMM CPU kernel function header.
+ * \author Sanchit Misra <sanchit.misra@intel.com>,
  *         Ramanarayan Mohanty <ramanarayan.mohanty@intel.com>,
  *         Vasimuddin Md <vasimuddin.md@intel.com>,
  *         Sasikanth Avancha <sasikanth.avancha@intel.com>
@@ -13,13 +13,14 @@
 #include <dgl/array.h>
 #include <dgl/bcast.h>
 #include <dmlc/logging.h>
-
 #include <algorithm>
 
 #if !defined(_WIN32)
 #ifdef USE_LIBXSMM
 #include <libxsmm_source.h>
 #include <unistd.h>
+#include <libxsmm.h>
+#include "xsmm_functors.h"
 #ifdef DEBUG
 #include <x86intrin.h>
 #endif  // DEBUG
@@ -27,6 +28,9 @@
 
 #define NUM_BLOCKS_PER_THREAD 20
 #define BLOCKING_HEURISTIC_PARAM 500
+#define MIN_WORK_HEURISTIC_PARAM 1000000
+#define MAX_COL_BLOCK_HI 64 
+#define MAX_COL_BLOCK_LO 32 
 
 namespace dgl {
 namespace aten {
@@ -42,62 +46,62 @@ struct CSRMatrixInternal {
 };
 
 int32_t GetLLCSize() {
-#ifdef _SC_LEVEL3_CACHE_SIZE
   int32_t cache_size = sysconf(_SC_LEVEL3_CACHE_SIZE);
   if (cache_size < 0) cache_size = DGL_CPU_LLC_SIZE;
-#else
-  int32_t cache_size = DGL_CPU_LLC_SIZE;
-#endif
   return cache_size;
 }
 
-/**
- * @brief Tile the CSR matrix to roughly make sure that the column tiles and
+/*!
+ * \brief Tile the CSR matrix to roughly make sure that the column tiles and
  *        corresponding neighbor features fit into LLC and the row tiles
  *        are assigned to OMP threads.
- * @param csr The Csr matrix.
- * @param block_csr_array The array containing csr matrices of all blocks.
- * @param num_M_blocks Number of blocks to create along the rows of adjacency
- *        matrix.
- * @param num_K_blocks Number of blocks to create along the columns of adjacency
- *        matrix.
- * @param M_block_size block size along the rows of adjacency matrix.
- * @param K_block_size block size along the columns of adjacency matrix.
- * @param use_lhs Whether to use lhs.
- * @param use_rhs Whether to use rhs.
+ * \param csr The Csr matrix.
+ * \param block_csr_array The array containing csr matrices of all blocks.
+ * \param num_M_blocks Number of blocks to create along the rows of adjacency matrix.
+ * \param num_K_blocks Number of blocks to create along the columns of adjacency matrix.
+ * \param M_block_size block size along the rows of adjacency matrix.
+ * \param K_block_size block size along the columns of adjacency matrix.
+ * \param use_lhs Whether to use lhs.
+ * \param use_rhs Whether to use rhs.
  */
 template <typename IdType>
 inline void SpMMCreateBlocks(
-    const CSRMatrix &csr, CSRMatrixInternal<IdType, IdType> *block_csr_array,
-    IdType num_M_blocks, IdType num_K_blocks, IdType M_block_size,
-    IdType K_block_size, bool use_lhs, bool use_rhs) {
+    const CSRMatrix& csr,
+    CSRMatrixInternal<IdType, IdType> *block_csr_array,
+    IdType num_M_blocks,
+    IdType num_K_blocks,
+    IdType M_block_size,
+    IdType K_block_size,
+    bool use_lhs, bool use_rhs) {
+
   const IdType M = csr.num_rows;
   const IdType K = csr.num_cols;
-  IdType *indptr = csr.indptr.Ptr<IdType>();
-  IdType *indices = csr.indices.Ptr<IdType>();
-  IdType *edges = csr.data.Ptr<IdType>();
+  IdType* indptr = csr.indptr.Ptr<IdType>();
+  IdType* indices = csr.indices.Ptr<IdType>();
+  IdType* edges = csr.data.Ptr<IdType>();
   CHECK_NOTNULL(indptr);
-  if (use_lhs) CHECK_NOTNULL(indices);
-  if (use_rhs) CHECK_NOTNULL(edges);
+  if (use_lhs)
+    CHECK_NOTNULL(indices);
+  if (use_rhs)
+    CHECK_NOTNULL(edges);
 
   if (num_K_blocks > 1) {
-    IdType *indptr_block_buf = reinterpret_cast<IdType *>(aligned_alloc(
-        64, (M_block_size + 1) * num_M_blocks * num_K_blocks * sizeof(IdType)));
-    IdType *indices_block_buf = nullptr;
-    if (use_lhs) {
-      indices_block_buf = reinterpret_cast<IdType *>(
-          aligned_alloc(64, indptr[M] * sizeof(IdType)));
-    }
-    IdType *edges_block_buf = nullptr;
-    if (use_rhs) {
-      edges_block_buf = reinterpret_cast<IdType *>(
-          aligned_alloc(64, indptr[M] * sizeof(IdType)));
-    }
+    IdType *indptr_block_buf = reinterpret_cast<IdType *>(aligned_alloc(64,
+                                                             (M_block_size + 1) * num_M_blocks *
+                                                             num_K_blocks * sizeof(IdType)));
+    IdType *indices_block_buf = NULL;
+	if(use_lhs)
+	  indices_block_buf = reinterpret_cast<IdType *>(aligned_alloc(64,
+                                                              indptr[M] * sizeof(IdType)));
+    IdType *edges_block_buf = NULL;
+	if(use_rhs)
+	  edges_block_buf = reinterpret_cast<IdType *>(aligned_alloc(64,
+                                                            indptr[M] * sizeof(IdType)));
 
 #pragma omp parallel
     {
-      IdType *my_cur_col_id = reinterpret_cast<IdType *>(
-          aligned_alloc(64, 2 * M_block_size * sizeof(IdType)));
+      IdType *my_cur_col_id = reinterpret_cast<IdType *>(aligned_alloc(64, 2 * M_block_size *
+                                                                          sizeof(IdType)));
 
 #pragma omp for
       for (IdType m = 0; m < num_M_blocks; m++) {
@@ -107,8 +111,10 @@ inline void SpMMCreateBlocks(
 
         IdType cur_indices_id = 0;
         IdType *my_indices_block_buf, *my_edges_block_buf;
-        if (use_lhs) my_indices_block_buf = indices_block_buf + indptr[M_start];
-        if (use_rhs) my_edges_block_buf = edges_block_buf + indptr[M_start];
+        if (use_lhs)
+          my_indices_block_buf = indices_block_buf + indptr[M_start];
+        if (use_rhs)
+          my_edges_block_buf = edges_block_buf + indptr[M_start];
 
         for (IdType i = M_start; i < M_end; i++) {
           my_cur_col_id[(i - M_start) * 2] = indptr[i];
@@ -121,15 +127,16 @@ inline void SpMMCreateBlocks(
           cur_csr.num_rows = M_end - M_start;
           cur_csr.num_cols = K_end - K_start;
           // Create csr_ij
-          IdType *cur_csr_indptr =
-              indptr_block_buf + (m * num_K_blocks + k) * (M_block_size + 1);
+          IdType *cur_csr_indptr = indptr_block_buf + (m * num_K_blocks + k) * (M_block_size + 1);
           IdType *cur_csr_indices = nullptr, *cur_csr_edges = nullptr;
-          if (use_lhs) cur_csr_indices = my_indices_block_buf + cur_indices_id;
-          if (use_rhs) cur_csr_edges = my_edges_block_buf + cur_indices_id;
+          if (use_lhs)
+            cur_csr_indices = my_indices_block_buf + cur_indices_id;
+          if (use_rhs)
+            cur_csr_edges = my_edges_block_buf + cur_indices_id;
           IdType cur_nnz = 0;
           for (IdType i = M_start; i < M_end; i++) {
             const IdType row_start = my_cur_col_id[(i - M_start) * 2];
-            const IdType row_end = my_cur_col_id[(i - M_start) * 2 + 1];
+            const IdType row_end   = my_cur_col_id[(i - M_start) * 2 + 1];
             cur_csr_indptr[i - M_start] = cur_nnz;
             IdType eid;
             for (eid = row_start; eid < row_end; eid++) {
@@ -139,8 +146,10 @@ inline void SpMMCreateBlocks(
                 break;
               }
               CHECK_LT(cur_indices_id + cur_nnz, nnz);
-              if (use_lhs) cur_csr_indices[cur_nnz] = src;
-              if (use_rhs) cur_csr_edges[cur_nnz] = edge;
+              if (use_lhs)
+                cur_csr_indices[cur_nnz] = src;
+              if (use_rhs)
+                cur_csr_edges[cur_nnz] = edge;
               cur_nnz++;
             }
             my_cur_col_id[(i - M_start) * 2] = eid;
@@ -148,8 +157,10 @@ inline void SpMMCreateBlocks(
           cur_csr_indptr[cur_csr.num_rows] = cur_nnz;
           cur_indices_id += cur_nnz;
           cur_csr.indptr = cur_csr_indptr;
-          if (use_lhs) cur_csr.indices = cur_csr_indices;
-          if (use_rhs) cur_csr.data = cur_csr_edges;
+          if (use_lhs)
+            cur_csr.indices = cur_csr_indices;
+          if (use_rhs)
+            cur_csr.data = cur_csr_edges;
           block_csr_array[m * num_K_blocks + k] = cur_csr;
         }
         CHECK_EQ(nnz, cur_indices_id);
@@ -157,6 +168,7 @@ inline void SpMMCreateBlocks(
       free(my_cur_col_id);
     }
   } else {
+#pragma omp for
     for (IdType m = 0; m < num_M_blocks; m++) {
       const IdType M_start = m * M_block_size;
       const IdType M_end = std::min((m + 1) * M_block_size, M);
@@ -173,266 +185,57 @@ inline void SpMMCreateBlocks(
   }
 }
 
-/**
- * @brief Create libxsmm kernel.
- * @param has_idx For the edge features, are there indices available.
- * @param N Feature size.
- * @param redop_flag Flag specifying the reduction operation.
- * @param is_cmp Is the reduction operation a compare operation.
- * @note libxsmm_dispatch_meltw_opreduce_vecs_idx creates a JIT'ed kernel.
- *       Given a node u, the kernel performs an elementwise "Op" on the
- *       features of the neighbors and/or the edges incident on u.
- *       Subsequently, it performs an elementwise "Redop" on all such
- *       features created and stores into the feature of node u.
- *       It uses a SIMD and a cache efficient design and also provides
- *       support to enable software prefetching if needed. For IdType,
- *       it supports INT32 and INT64. For DType, it supports BF16 and FP32.
- *       It supports all the "Ops" and "Redops" supported by DGL. Once a
- *       kernel is generated by libxsmm_dispatch_meltw_opreduce_vecs_idx,
- *       it is cached for the entire duration of the execution of a program
- *       so that subsequently if the kernel is needed again, it just returns
- *       the cached copy.
- */
-template <typename IdType, typename DType, typename Op>
-inline libxsmm_meltwfunction_opreduce_vecs_idx SpMMCreateLibxsmmKernel(
-    bool has_idx, IdType N, libxsmm_meltw_opreduce_vecs_flags redop_flag,
-    bool is_cmp) {
-  int _ld = N;
-  libxsmm_meltw_opreduce_vecs_flags opredop_flags;
-  // First, set the Op in the opredop_flags
-  if (std::is_same<Op, op::Add<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_ADD;
-  } else if (std::is_same<Op, op::Sub<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_SUB;
-  } else if (std::is_same<Op, op::Mul<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_MUL;
-  } else if (std::is_same<Op, op::Div<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_DIV;
-  } else if (std::is_same<Op, op::CopyLhs<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_COPY;
-  } else if (std::is_same<Op, op::CopyRhs<DType>>::value) {
-    opredop_flags = LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OP_COPY;
-  }
-  // Second, set which of lhs or rhs is considered first and second operand.
-  // This is needed since libxsmm assumes that the copy operation always copies
-  // the first operand. So, if we need to copy rhs, we need to set that as the
-  // first operand. For rhs, we also set whether to use implicit indices or
-  // provided indices.
-  // TODO(Steve): fix this long line in a separate PR.
-  if (std::is_same<Op, op::CopyLhs<DType>>::value) {
-    opredop_flags =
-        (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OPORDER_VECIDX_VECIN);  // NOLINT
-  } else if (std::is_same<Op, op::CopyRhs<DType>>::value) {
-    opredop_flags =
-        (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OPORDER_VECIN_VECIDX);  // NOLINT
-    if (!has_idx) {
-      opredop_flags =
-          (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_IMPLICIT_INDEXED_VECIDX);  // NOLINT
-    }
-  } else {
-    opredop_flags =
-        (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_OPORDER_VECIDX_VECIN);  // NOLINT
-    if (has_idx) {
-      opredop_flags =
-          (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_INDEXED_VEC);  // NOLINT
-    } else {
-      opredop_flags =
-          (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_IMPLICIT_INDEXED_VEC);  // NOLINT
-    }
-  }
-  // Third, we set the Redop in the opredop_flags
-  opredop_flags =
-      (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | redop_flag);
-  // Fourth, in case of Cmp Redop, set whether to record argmax/argmin for
-  // lhs/rhs
-  if (is_cmp) {
-    if (Op::use_lhs) {
-      opredop_flags =
-          (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_RECORD_ARGOP_OFF_VEC_0);  // NOLINT
-    }
-    if (Op::use_rhs) {
-      opredop_flags =
-          (libxsmm_meltw_opreduce_vecs_flags)(opredop_flags | LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_RECORD_ARGOP_OFF_VEC_1);  // NOLINT
-    }
-  }
-  libxsmm_meltwfunction_opreduce_vecs_idx kernel = nullptr;
-  if (std::is_same<DType, float>::value) {
-    kernel = libxsmm_dispatch_meltw_opreduce_vecs_idx(
-        N, &_ld, &_ld, LIBXSMM_DATATYPE_F32, LIBXSMM_DATATYPE_F32,
-        (sizeof(IdType) == 8) ? LIBXSMM_DATATYPE_I64 : LIBXSMM_DATATYPE_I32,
-        opredop_flags, 0);
-  } else {  // assume bf16
-    kernel = libxsmm_dispatch_meltw_opreduce_vecs_idx(
-        N, &_ld, &_ld, LIBXSMM_DATATYPE_BF16, LIBXSMM_DATATYPE_BF16,
-        (sizeof(IdType) == 8) ? LIBXSMM_DATATYPE_I64 : LIBXSMM_DATATYPE_I32,
-        opredop_flags, 0);
-  }
-
-  if (kernel == nullptr) {
-    LOG(FATAL) << "Failed to generate libxsmm kernel for the SpMM operation."
-                  "To disable libxsmm, use dgl.use_libxsmm(false).";
-  }
-  return kernel;
-}
-
-/**
- * @brief Use libxsmm to perform SpMM-Sum on all blocks.
- * @param block_csr_array The array containing csr matrices of all blocks.
- * @param B The feature on source nodes.
- * @param E The feature on edges.
- * @param C The result feature on destination nodes.
- * @param has_idx For the edge features, are there indices available.
- * @param N Feature size.
- * @param num_M_blocks Number of blocks to create along the rows of adjacency
- *        matrix.
- * @param num_K_blocks Number of blocks to create along the columns of adjacency
- *        matrix.
- * @param M_block_size block size along the rows of adjacency matrix.
- * @param kernel The libxsmm kernel.
- */
-template <typename IdType, typename DType>
-inline void SpMMBlockwiseOpSum(
-    CSRMatrixInternal<IdType, IdType> *block_csr_array, const DType *B,
-    const DType *E, DType *C, bool has_idx, IdType N, IdType num_M_blocks,
-    IdType num_K_blocks, IdType M_block_size,
-    libxsmm_meltwfunction_opreduce_vecs_idx kernel) {
-  const DType *in_matrix1 = B;
-  const DType *in_matrix2 = E;
-  DType *output = C;
-#pragma omp parallel
-  {
-    for (IdType k = 0; k < num_K_blocks; k++) {
-#pragma omp for schedule(dynamic)
-      for (IdType m = 0; m < num_M_blocks; m++) {
-        CSRMatrixInternal<IdType, IdType> cur_csr =
-            block_csr_array[m * num_K_blocks + k];
-
-        const IdType M_start = m * M_block_size;
-        for (IdType i = 0; i < cur_csr.num_rows; i++) {
-          const IdType row_start = cur_csr.indptr[i];
-          const IdType row_end = cur_csr.indptr[i + 1];
-          const IdType dst = i + M_start;
-
-          libxsmm_meltw_opreduce_vecs_idx_param params;
-          params.n = row_end - row_start;
-          params.indices = &cur_csr.indices[row_start];
-          params.in_matrix = in_matrix1;
-          params.out_vec = &output[dst * N];
-          params.scale_vals = nullptr;
-          if (has_idx) {
-            params.in_matrix2 = in_matrix2;
-            params.indices2 = &cur_csr.data[row_start];
-          } else {
-            params.in_matrix2 = &in_matrix2[row_start * N];
-          }
-          kernel(&params);
-        }
-      }
-    }
-  }
-}
-
-/**
- * @brief Use libxsmm to perform SpMM-Max/Min on all blocks.
- * @param block_csr_array The array containing csr matrices of all blocks.
- * @param B The feature on source nodes.
- * @param E The feature on edges.
- * @param C The result feature on destination nodes.
- * @param argB Arg-Min/Max on source nodes.
- * @param argE Arg-Min/Max on edges.
- * @param has_idx For the edge features, are there indices available.
- * @param N Feature size.
- * @param num_M_blocks Number of blocks to create along the rows of adjacency
- *        matrix.
- * @param num_K_blocks Number of blocks to create along the columns of adjacency
- *        matrix.
- * @param M_block_size block size along the rows of adjacency matrix.
- * @param kernel The libxsmm kernel.
- */
-template <typename IdType, typename DType, typename Op, typename Cmp>
-inline void SpMMBlockwiseOpCmp(
-    CSRMatrixInternal<IdType, IdType> *block_csr_array, const DType *B,
-    const DType *E, DType *C, IdType *argB, IdType *argE, bool has_idx,
-    IdType N, IdType num_M_blocks, IdType num_K_blocks, IdType M_block_size,
-    libxsmm_meltwfunction_opreduce_vecs_idx kernel) {
-  const DType *in_matrix1 = B;
-  const DType *in_matrix2 = E;
-  DType *output = C;
-  IdType *out_matrix1 = argB;
-  IdType *out_matrix2 = argE;
-
-#pragma omp parallel
-  {
-    for (IdType k = 0; k < num_K_blocks; k++) {
-#pragma omp for schedule(dynamic)
-      for (IdType m = 0; m < num_M_blocks; m++) {
-        CSRMatrixInternal<IdType, IdType> cur_csr =
-            block_csr_array[m * num_K_blocks + k];
-
-        const IdType M_start = m * M_block_size;
-        for (IdType i = 0; i < cur_csr.num_rows; i++) {
-          const IdType row_start = cur_csr.indptr[i];
-          const IdType row_end = cur_csr.indptr[i + 1];
-          const IdType dst = i + M_start;
-
-          libxsmm_meltw_opreduce_vecs_idx_param params;
-          params.n = row_end - row_start;
-          params.indices = &cur_csr.indices[row_start];
-          params.in_matrix = in_matrix1;
-          params.out_vec = &output[dst * N];
-          params.argop_off_vec_0 = &out_matrix1[dst * N];
-          params.argop_off_vec_1 = &out_matrix2[dst * N];
-          params.scale_vals = nullptr;
-          if (has_idx) {
-            params.in_matrix2 = in_matrix2;
-            params.indices2 = &cur_csr.data[row_start];
-          } else {
-            params.in_matrix2 = &in_matrix2[row_start * N];
-          }
-          kernel(&params);
-        }
-      }
-    }
-  }
-}
-
-/**
- * @brief Free the tiled CSR matrix data.
- * @param block_csr_array The array containing csr matrices of all blocks.
- * @param num_M_blocks Number of blocks to create along the rows of adjacency
- *        matrix.
- * @param num_K_blocks Number of blocks to create along the columns of adjacency
- *        matrix.
- * @param use_lhs Whether to use lhs.
- * @param use_rhs Whether to use rhs.
+/*!
+ * \brief Free the tiled CSR matrix data.
+ * \param block_csr_array The array containing csr matrices of all blocks.
+ * \param num_M_blocks Number of blocks to create along the rows of adjacency matrix.
+ * \param num_K_blocks Number of blocks to create along the columns of adjacency matrix.
+ * \param use_lhs Whether to use lhs.
+ * \param use_rhs Whether to use rhs.
  */
 template <typename IdType>
 inline void SpMMFreeBlocks(
-    CSRMatrixInternal<IdType, IdType> *block_csr_array, IdType num_M_blocks,
-    IdType num_K_blocks, bool use_lhs, bool use_rhs) {
+    CSRMatrixInternal<IdType, IdType> *block_csr_array,
+    IdType num_M_blocks, IdType num_K_blocks,
+    bool use_lhs, bool use_rhs) {
+
   if (num_K_blocks > 1) {
     free(block_csr_array[0].indptr);
-    if (use_lhs) free(block_csr_array[0].indices);
-    if (use_rhs) free(block_csr_array[0].data);
+    if (use_lhs)
+      free(block_csr_array[0].indices);
+    if (use_rhs)
+      free(block_csr_array[0].data);
   }
   free(block_csr_array);
 }
 
-/**
- * @brief Optimized CPU kernel of SpMM-Sum/Max/Min on Csr format.
- * @param bcast Broadcast information.
- * @param csr The Csr matrix.
- * @param ufeat The feature on source nodes.
- * @param efeat The feature on edges.
- * @param out The result feature on destination nodes.
- * @param argu Arg-Min/Max on source nodes.
- * @param arge Arg-Min/Max on edges.
- * @note it uses libxsmm, blocking and dynamic thread scheduling.
+/*!
+ * \brief Optimized CPU kernel of SpMM-Sum/Max/Min on Csr format.
+ * \param bcast Broadcast information.
+ * \param csr The Csr matrix.
+ * \param ufeat The feature on source nodes.
+ * \param efeat The feature on edges.
+ * \param out The result feature on destination nodes.
+ * \param argu Arg-Min/Max on source nodes.
+ * \param arge Arg-Min/Max on edges.
+ * \note it uses libxsmm, blocking and dynamic thread scheduling.
  */
 template <typename IdType, typename DType, typename Op, typename Redop>
-void SpMMRedopCsrOpt(
-    const BcastOff &bcast, const CSRMatrix &csr, NDArray ufeat, NDArray efeat,
-    NDArray out, NDArray argu, NDArray arge) {
+inline int SpMMRedopCsrOpt(
+    const BcastOff& bcast,
+    const CSRMatrix& csr,
+    NDArray ufeat, NDArray efeat,
+    NDArray out,
+    NDArray argu, NDArray arge) {
+
+  const IdType M = csr.num_rows;
+  const IdType N = bcast.out_len;
+  const IdType K = csr.num_cols;
+  const IdType* indptr = csr.indptr.Ptr<IdType>();
+  CHECK_NOTNULL(indptr);
+  const IdType total_nnz = indptr[M];
+  if (M <= 0 || K <= 0 || N <= 0 || total_nnz <= 0) return 0;
+
   int32_t llc_size = GetLLCSize();
 
 #ifdef DEBUG
@@ -442,31 +245,26 @@ void SpMMRedopCsrOpt(
 
   const bool has_idx = !IsNullArray(csr.data);
 
-  DType *C = out.Ptr<DType>();
-  const DType *B = ufeat.Ptr<DType>();
-  const DType *E = efeat.Ptr<DType>();
-  IdType *argB, *argE;
-  if (std::is_same<Redop, op::Max<DType>>::value ||
-      std::is_same<Redop, op::Min<DType>>::value) {
+  DType* C = out.Ptr<DType>();
+  DType* B = ufeat.Ptr<DType>();
+  DType* E = efeat.Ptr<DType>();
+
+  IdType *argB=NULL, *argE=NULL;
+  if (std::is_same<Redop, op::Max<DType>>::value || std::is_same<Redop, op::Min<DType>>::value) {
     argB = argu.Ptr<IdType>();
     argE = arge.Ptr<IdType>();
   }
 
   const int nthreads = omp_get_max_threads();
-  const IdType M = csr.num_rows;
-  const IdType N = bcast.out_len;
-  const IdType K = csr.num_cols;
-  const IdType *indptr = csr.indptr.Ptr<IdType>();
-  CHECK_NOTNULL(indptr);
-  const IdType total_nnz = indptr[M];
-  if (M <= 0 || K <= 0 || N <= 0 || total_nnz <= 0) return;
+  int64_t dim = N;
+  int64_t lhs_dim = Op::use_rhs ? bcast.lhs_len / bcast.rhs_len : bcast.lhs_len;
+  int64_t rhs_dim = bcast.rhs_len;
 
   const double avg_degree = total_nnz * 1.0 / M;
   const double nnz_prob = avg_degree / K;
 
-  IdType K_block_size = std::min(
-      (int64_t)K,
-      (int64_t)(llc_size / (N * sizeof(DType) * nnz_prob * BLOCKING_HEURISTIC_PARAM)));  // NOLINT
+  IdType K_block_size = std::min((int64_t)K, (int64_t)(llc_size / (N * sizeof(DType) *
+                                                       nnz_prob * BLOCKING_HEURISTIC_PARAM)));
   IdType M_block_size = M / (nthreads * NUM_BLOCKS_PER_THREAD);
   if (M_block_size == 0) M_block_size = 1;
   if (K_block_size == 0) K_block_size = 1;
@@ -474,13 +272,7 @@ void SpMMRedopCsrOpt(
   IdType num_M_blocks = (M + M_block_size - 1) / M_block_size;
   IdType num_K_blocks = (K + K_block_size - 1) / K_block_size;
 
-  CSRMatrixInternal<IdType, IdType> *block_csr_array =
-      (CSRMatrixInternal<IdType, IdType> *)aligned_alloc(
-          64, sizeof(CSRMatrixInternal<IdType, IdType>) * num_M_blocks *
-                  num_K_blocks);
-
 #ifdef DEBUG
-  endTick = __rdtsc();
   if (std::is_same<Redop, op::Max<DType>>::value) {
     LOG(INFO) << "Redop = Max";
   } else if (std::is_same<Redop, op::Min<DType>>::value) {
@@ -494,105 +286,362 @@ void SpMMRedopCsrOpt(
   LOG(INFO) << "total_nnz = " << total_nnz << ", avg_degree = " << avg_degree;
   LOG(INFO) << "has_idx = " << has_idx;
   LOG(INFO) << "nnz_prob = " << nnz_prob;
-  LOG(INFO) << "K_block_size = " << K_block_size
-            << ", M_block_size = " << M_block_size;
-  LOG(INFO) << "num_K_blocks = " << num_K_blocks
-            << ", num_M_blocks = " << num_M_blocks;
-  LOG(INFO) << "stage0 ticks = " << (endTick - startTick);
-  startTick = __rdtsc();
+  LOG(INFO) << "K_block_size = " << K_block_size << ", M_block_size = " << M_block_size;
+  LOG(INFO) << "num_K_blocks = " << num_K_blocks << ", num_M_blocks = " << num_M_blocks;
+  LOG(INFO) << "lhs_dim = " << lhs_dim << ", rhs_dim = " << rhs_dim << ", dim = " << dim;
 #endif  // DEBUG
 
-  SpMMCreateBlocks(
-      csr, block_csr_array, num_M_blocks, num_K_blocks, M_block_size,
-      K_block_size, Op::use_lhs, Op::use_rhs);
+  auto ef_bcast_row_tpp = dgl_tpp::CpyBcastRowTPP<DType>(rhs_dim, lhs_dim);
+  auto set_zero_tpp = dgl_tpp::SetZeroTPP<DType>(1, dim);
 
-#ifdef DEBUG
-  endTick = __rdtsc();
-  LOG(INFO) << "stage1 ticks = " << (endTick - startTick);
-  startTick = __rdtsc();
-#endif  // DEBUG
+  auto copy_tpp = dgl_tpp::CpyTPP<DType>(1, dim);
 
-  libxsmm_meltwfunction_opreduce_vecs_idx kernel = nullptr;
-  if (std::is_same<Redop, op::Max<DType>>::value) {
-    kernel = SpMMCreateLibxsmmKernel<IdType, DType, Op>(
-        has_idx, N, LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_REDOP_MAX, true);
-  } else if (std::is_same<Redop, op::Min<DType>>::value) {
-    kernel = SpMMCreateLibxsmmKernel<IdType, DType, Op>(
-        has_idx, N, LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_REDOP_MIN, true);
-  } else if (std::is_same<Redop, op::Add<DType>>::value) {
-    kernel = SpMMCreateLibxsmmKernel<IdType, DType, Op>(
-        has_idx, N, LIBXSMM_MELTW_FLAG_OPREDUCE_VECS_REDOP_SUM, false);
-  }
+  CSRMatrixInternal<IdType, IdType> *block_csr_array =
+    (CSRMatrixInternal<IdType, IdType> *)aligned_alloc(64,
+	sizeof(CSRMatrixInternal<IdType, IdType>) * num_M_blocks * num_K_blocks);
 
-#ifdef DEBUG
-  endTick = __rdtsc();
-  LOG(INFO) << "stage2 ticks = " << (endTick - startTick);
-  startTick = __rdtsc();
-#endif  // DEBUG
+  SpMMCreateBlocks(csr, block_csr_array, num_M_blocks, num_K_blocks, M_block_size, K_block_size,
+      Op::use_lhs, Op::use_rhs);
 
-  if (std::is_same<Redop, op::Max<DType>>::value ||
-      std::is_same<Redop, op::Min<DType>>::value) {
-    SpMMBlockwiseOpCmp<IdType, DType, Op, Redop>(
-        block_csr_array, B, E, C, argB, argE, has_idx, N, num_M_blocks,
-        num_K_blocks, M_block_size, kernel);
-  } else {
-    SpMMBlockwiseOpSum(
-        block_csr_array, B, E, C, has_idx, N, num_M_blocks, num_K_blocks,
-        M_block_size, kernel);
+  // Op is CopyLhs 
+  if(std::is_same<Op, op::CopyLhs<DType>>::value) {
+    DType (*ufeat_mat)[dim] = (DType (*)[dim])B;
+
+    // Reduction op is either Max or Min
+    if(std::is_same<Redop, op::Max<DType>>::value || std::is_same<Redop, op::Min<DType>>::value) {
+      auto reduce_max_idx_tpp = dgl_tpp::ReduceMaxRowsIdxTPP<DType, IdType>(dim);
+      auto reduce_min_idx_tpp = dgl_tpp::ReduceMinRowsIdxTPP<DType, IdType>(dim);
+#pragma omp parallel
+      {
+	DType (*ofeat_mat)[dim] = (DType (*)[dim])C;
+	IdType (*argB_mat)[dim] = (IdType (*)[dim])argB;
+
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType dst = i + M_start;
+	      IdType *cid = &cur_csr.indices[row_start];
+	      auto n = row_end - row_start;
+
+	      if(std::is_same<Redop, op::Max<DType>>::value)
+		reduce_max_idx_tpp(ufeat_mat[0], cid, n, ofeat_mat[dst], argB_mat[dst]);
+	      else {
+		reduce_min_idx_tpp(ufeat_mat[0], cid, n, ofeat_mat[dst], argB_mat[dst]);
+	      }
+	    }
+	  }
+	}
+      }
+    }
+    else { // Reduction op is other than Max or Min
+      auto reduce_add_idx_tpp = dgl_tpp::ReduceAddRowsIdxTPP<DType, IdType>(dim);
+#pragma omp parallel
+      {
+	DType (*ofeat_mat)[dim] = (DType (*)[dim])C;
+
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType *cid = &cur_csr.indices[row_start];
+	      IdType dst = i + M_start;
+	      auto n = row_end - row_start;
+
+              reduce_add_idx_tpp(ufeat_mat[0], cid, n, ofeat_mat[dst]);
+	    }
+	  }
+	}
+      }
+    }
   }
+  // Op is CopyRhs
+  else if(std::is_same<Op, op::CopyRhs<DType>>::value) {
+    DType (*efeat_mat)[dim] = (DType (*)[dim])E;
 
-#ifdef DEBUG
-  endTick = __rdtsc();
-  LOG(INFO) << "stage3 ticks = " << (endTick - startTick);
-  startTick = __rdtsc();
-#endif  // DEBUG
+    if(std::is_same<Redop, op::Max<DType>>::value || std::is_same<Redop, op::Min<DType>>::value) {
+      auto reduce_max_idx_tpp = dgl_tpp::ReduceMaxRowsIdxTPP<DType, IdType>(dim);
+      auto reduce_min_idx_tpp = dgl_tpp::ReduceMinRowsIdxTPP<DType, IdType>(dim);
 
-  SpMMFreeBlocks(
-      block_csr_array, num_M_blocks, num_K_blocks, Op::use_lhs, Op::use_rhs);
+#pragma omp parallel
+      {
+	IdType (*argE_mat)[rhs_dim] = (IdType (*)[rhs_dim])argE;
+	DType (*ofeat_mat)[dim] = (DType (*)[dim])C;
 
-#ifdef DEBUG
-  endTick = __rdtsc();
-  LOG(INFO) << "stage4 ticks = " << (endTick - startTick);
-#endif  // DEBUG
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+
+
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType dst = i + M_start;
+	      IdType *eid = has_idx ? &cur_csr.data[row_start] : &row_start;
+	      auto n = row_end - row_start;
+
+
+              if(std::is_same<Redop, op::Max<DType>>::value)
+		reduce_max_idx_tpp(efeat_mat[0], eid, n, ofeat_mat[dst], argE_mat[dst]);
+              else {
+		reduce_min_idx_tpp(efeat_mat[0], eid, n, ofeat_mat[dst], argE_mat[dst]);
+              }
+	    }
+	  }
+	}
+      }
+    }
+    else {
+      auto reduce_add_idx_tpp = dgl_tpp::ReduceAddRowsIdxTPP<DType, IdType>(dim);
+#pragma omp parallel
+      {
+	DType (*ofeat_mat)[dim] = (DType (*)[dim])C;
+
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+
+
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType *eid = has_idx ? &cur_csr.data[row_start] : &row_start;
+	      IdType dst = i + M_start;
+	      auto n = row_end - row_start;
+
+              reduce_add_idx_tpp(efeat_mat[0], eid, n, ofeat_mat[dst]);
+	    }
+	  }
+	}
+      }
+    }
+  }
+  else { //Binary + Unary
+    if(std::is_same<Redop, op::Max<DType>>::value || std::is_same<Redop, op::Min<DType>>::value) {
+      auto max_tpp = dgl_tpp::MaxTPP<DType>(1, dim);
+      auto min_tpp = dgl_tpp::MinTPP<DType>(1, dim);
+#pragma omp parallel
+      {
+	IdType (*argB_mat)[dim]     = (IdType (*)[dim])argB;
+	IdType (*argE_mat)[dim] = (IdType (*)[dim])argE;
+
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+	    DType etmp[rhs_dim][lhs_dim];
+	    DType oprev[dim]; 
+
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType *eid = has_idx ? &cur_csr.data[row_start] : &row_start;
+	      IdType *cid = &cur_csr.indices[row_start];
+	      IdType dst = i + M_start;
+	      auto n = row_end - row_start;
+
+	      ef_bcast_row_tpp(&E[eid[0]*rhs_dim], etmp[0]);
+              if(std::is_same<Redop, op::Max<DType>>::value)
+                max_tpp(&B[cid[0]*dim], etmp[0], &C[dst*dim]);
+              else
+                min_tpp(&B[cid[0]*dim], etmp[0], &C[dst*dim]);
+
+#pragma omp simd
+              for(auto x=0; x<dim; x++) {
+                argB_mat[dst][x] = cid[0];
+                argE_mat[dst][x] = eid[0];
+              }
+              for(auto r=1; r<n; r++) {
+                copy_tpp(&C[dst*dim], oprev);
+                ef_bcast_row_tpp(&E[eid[r]*rhs_dim], etmp[0]);
+                if(std::is_same<Redop, op::Max<DType>>::value)
+                  max_tpp(&B[cid[0]*dim], etmp[0], &C[dst*dim]);
+                else
+                  min_tpp(&B[cid[0]*dim], etmp[0], &C[dst*dim]);
+#pragma omp simd
+                for(auto x=0; x<dim; x++) {
+                  if(C[dst*dim+x] != oprev[x]) {
+                    argB_mat[dst][x] = cid[r];
+                    argE_mat[dst][x] = eid[r];
+                  }
+                }
+              }
+	    }
+	  }
+	}
+      }
+    }
+    else {
+      auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+      auto add_tpp = dgl_tpp::AddTPP<DType>(1, dim);
+#pragma omp parallel
+      {
+	for (IdType k = 0; k < num_K_blocks; k++) {
+#pragma omp for schedule(dynamic)
+	  for (IdType m = 0; m < num_M_blocks; m++) {
+	    CSRMatrixInternal<IdType, IdType> cur_csr = block_csr_array[m * num_K_blocks + k];
+	    IdType M_start = m * M_block_size;
+
+	    DType evec[dim], ovec[dim];
+	    for (IdType i = 0; i < cur_csr.num_rows; i++) {
+	      IdType row_start = cur_csr.indptr[i];
+	      IdType row_end   = cur_csr.indptr[i + 1];
+	      IdType *eid = has_idx ? &cur_csr.data[row_start] : &row_start;
+	      IdType *cid = &cur_csr.indices[row_start];
+	      IdType dst = i + M_start;
+	      auto n = row_end - row_start;
+
+              if(k==0)
+                set_zero_tpp(&C[dst*dim]);
+              for(auto r=0; r<n; r++) {
+                ef_bcast_row_tpp(&E[eid[r]*rhs_dim], evec);
+                mul_tpp(&B[cid[r]*dim], evec, ovec);
+                add_tpp(&C[dst*dim], ovec, &C[dst*dim]);
+              }
+	    }
+	  }
+	}
+      }
+    }
+  }
+  SpMMFreeBlocks(block_csr_array, num_M_blocks, num_K_blocks, Op::use_lhs, Op::use_rhs);
+
+  return 0;
 }
 
-/**
- * @brief Optimized CPU kernel of SpMM-Sum on Csr format.
- * @param bcast Broadcast information.
- * @param csr The Csr matrix.
- * @param ufeat The feature on source nodes.
- * @param efeat The feature on edges.
- * @param out The result feature on destination nodes.
- * @note it uses libxsmm, blocking and dynamic thread scheduling.
+/*!
+ * \brief Optimized CPU kernel of SpMM-Sum on Csr format.
+ * \param bcast Broadcast information.
+ * \param csr The Csr matrix.
+ * \param ufeat The feature on source nodes.
+ * \param efeat The feature on edges.
+ * \param out The result feature on destination nodes.
+ * \note it uses libxsmm, blocking and dynamic thread scheduling.
  */
 template <typename IdType, typename DType, typename Op>
-void SpMMSumCsrLibxsmm(
-    const BcastOff &bcast, const CSRMatrix &csr, NDArray ufeat, NDArray efeat,
-    NDArray out) {
+// void SpMMSumCsrLibxsmm(const BcastOff& bcast, const CSRMatrix& csr,
+int SpMMSumCsrLibxsmm(const BcastOff& bcast, const CSRMatrix& csr,
+                   NDArray ufeat, NDArray efeat, NDArray out) {
   NDArray dummy;
-  SpMMRedopCsrOpt<IdType, DType, Op, op::Add<DType>>(
-      bcast, csr, ufeat, efeat, out, dummy, dummy);
+  return SpMMRedopCsrOpt<IdType, DType, Op, op::Add<DType>>(bcast, csr, ufeat, efeat, out, dummy, dummy);
 }
 
-/**
- * @brief Optimized CPU kernel of SpMM-Min/Max on Csr format.
- * @param bcast Broadcast information.
- * @param csr The Csr matrix.
- * @param ufeat The feature on source nodes.
- * @param efeat The feature on edges.
- * @param out The result feature on destination nodes.
- * @param argu Arg-Min/Max on source nodes.
- * @param arge Arg-Min/Max on edges.
- * @note it uses libxsmm, blocking and dynamic thread scheduling.
+/*!
+ * \brief Optimized CPU kernel of SpMM-Min/Max on Csr format.
+ * \param bcast Broadcast information.
+ * \param csr The Csr matrix.
+ * \param ufeat The feature on source nodes.
+ * \param efeat The feature on edges.
+ * \param out The result feature on destination nodes.
+ * \param argu Arg-Min/Max on source nodes.
+ * \param arge Arg-Min/Max on edges.
+ * \note it uses libxsmm, blocking and dynamic thread scheduling.
  */
 template <typename IdType, typename DType, typename Op, typename Cmp>
-void SpMMCmpCsrLibxsmm(
-    const BcastOff &bcast, const CSRMatrix &csr, NDArray ufeat, NDArray efeat,
-    NDArray out, NDArray argu, NDArray arge) {
-  SpMMRedopCsrOpt<IdType, DType, Op, Cmp>(
-      bcast, csr, ufeat, efeat, out, argu, arge);
+// void SpMMCmpCsrLibxsmm(const BcastOff& bcast, const CSRMatrix& csr, NDArray ufeat,
+int SpMMCmpCsrLibxsmm(const BcastOff& bcast, const CSRMatrix& csr, NDArray ufeat,
+                   NDArray efeat, NDArray out, NDArray argu, NDArray arge) {
+  return SpMMRedopCsrOpt<IdType, DType, Op, Cmp>(bcast, csr, ufeat, efeat, out, argu, arge);
+}
+
+#if 1
+template <typename IdType, typename DType, typename Op>
+void Edge_softmax_csr_forward_libxsmm(const BcastOff& bcast, const CSRMatrix& csr, NDArray ufeat,
+    NDArray efeat, NDArray out) {
+
+  const bool has_idx = !IsNullArray(csr.data);
+  const IdType* indptr = static_cast<IdType*>(csr.indptr->data);
+  IdType* edges =
+    has_idx ? static_cast<IdType*>(csr.data->data) : nullptr;
+  const int64_t dim = bcast.out_len;
+  DType* Eo = out.Ptr<DType>();
+  DType* Ei = static_cast<DType*>(efeat->data);
+
+  auto red_max_idx_tpp = dgl_tpp::ReduceMaxRowsIdxTPP<DType, IdType>(dim);
+  auto max_tpp = dgl_tpp::MaxTPP<DType>(1, dim);
+  auto recp_tpp = dgl_tpp::RecpTPP<DType>(1, dim);
+  auto sub_tpp = dgl_tpp::SubTPP<DType, DType>(1, dim);
+  auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+  auto exp_tpp = dgl_tpp::ExpTPP<DType, DType>(1, dim);
+  auto add_tpp = dgl_tpp::AddTPP<DType>(1, dim);
+  auto set_zero_tpp = dgl_tpp::SetZeroTPP<DType>(1, dim);
+
+#pragma omp parallel for
+  for(auto rid = 0; rid < csr.num_rows; rid++) {
+    IdType row_start = indptr[rid];
+    IdType row_end = indptr[rid+1];
+    auto n = row_end - row_start;
+    IdType* eid = has_idx ? &edges[row_start] : &row_start;
+    DType emax[dim], esum[dim], eexp[n][dim];
+    IdType arge[dim];
+
+    red_max_idx_tpp(Ei, eid, n, emax, arge);
+
+    set_zero_tpp(esum);
+    for(auto r=0; r<n; r++) {
+      sub_tpp(&Ei[eid[r]*dim], emax, eexp[r]);
+      exp_tpp(eexp[r], eexp[r]);
+      add_tpp(eexp[r], esum, esum);
+    }
+    recp_tpp(esum, esum);
+    for(auto r=0; r<n; r++) {
+      mul_tpp(eexp[r], esum, &Eo[eid[r]*dim]);
+    }
+  }
 }
 
+template <typename IdType, typename DType, typename Op>
+void Edge_softmax_csr_backward_libxsmm(const BcastOff& bcast, const CSRMatrix& csr, NDArray out,
+                NDArray sds, NDArray back_out) {
+
+  const bool has_idx = !IsNullArray(csr.data);
+  const IdType* indptr = static_cast<IdType*>(csr.indptr->data);
+  IdType* edges =
+    has_idx ? static_cast<IdType*>(csr.data->data) : nullptr;
+  const int64_t dim = bcast.out_len;
+  DType* W_out = static_cast<DType*>(out->data);
+  DType* W_sds =  static_cast<DType*>(sds->data);
+  DType* O = back_out.Ptr<DType>();
+
+  auto red_add_idx_tpp = dgl_tpp::ReduceAddRowsIdxTPP<DType, IdType>(dim);
+  auto mul_tpp = dgl_tpp::MulTPP<DType>(1, dim);
+  auto sub_tpp = dgl_tpp::SubTPP<DType>(1, dim);
+  auto set_zero_tpp = dgl_tpp::SetZeroTPP<DType>(1, dim);
+
+#pragma omp parallel for
+  for(auto rid = 0; rid < csr.num_rows; rid++) {
+    IdType row_start = indptr[rid];
+    IdType row_end = indptr[rid+1];
+    auto n = row_end - row_start;
+    IdType* eid = has_idx ? &edges[row_start] : &row_start;
+    DType esum[dim], emul[dim];
+
+    set_zero_tpp(esum);
+    red_add_idx_tpp(W_sds, eid, n, esum);
+
+    for(auto r=0; r<n; r++) {
+      mul_tpp(&W_out[eid[r]*dim], esum, emul);
+      sub_tpp(&W_sds[eid[r]*dim], emul, &O[eid[r]*dim]);
+    }
+  }
+}
+#endif
+
 }  // namespace cpu
 }  // namespace aten
 }  // namespace dgl
diff --git a/src/array/cpu/xsmm_functors.h b/src/array/cpu/xsmm_functors.h
new file mode 100644
index 00000000..e9b33f85
--- /dev/null
+++ b/src/array/cpu/xsmm_functors.h
@@ -0,0 +1,1476 @@
+/******************************************************************************
+ * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
+ *                                                                            *
+ * For information on the license, see the LICENSE file.                      *
+ * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
+ * SPDX-License-Identifier: BSD-3-Clause                                      *
+ ******************************************************************************/
+/* Author: Dhiraj Kalamkar (Intel Corp.)
+ ******************************************************************************/
+
+#ifndef _XSMM_FUNCTORS_H_
+#define _XSMM_FUNCTORS_H_
+
+#ifdef __x86_64__
+#include <immintrin.h>
+#endif
+
+#include <libxsmm.h>
+#include <dgl/runtime/bfloat16.h>
+#include <string>
+#include <unordered_map>
+
+#define TPP_ASSERT(cond, x...) \
+  do {                         \
+    if (!(cond)) {             \
+      printf(x);               \
+      fflush(stdout);          \
+      exit(1);                 \
+    }                          \
+  } while (0)
+#define ALIGNDOWN(N, A) ((N) & ~((A)-1))
+
+namespace dgl_tpp {
+typedef BFloat16 bfloat16;
+template <typename T>
+inline libxsmm_datatype XsmmDtype();
+template <>
+inline libxsmm_datatype XsmmDtype<long>() {
+  return LIBXSMM_DATATYPE_I64;
+}
+template <>
+inline libxsmm_datatype XsmmDtype<int>() {
+  return LIBXSMM_DATATYPE_I32;
+}
+template <>
+inline libxsmm_datatype XsmmDtype<float>() {
+  return LIBXSMM_DATATYPE_F32;
+}
+template <>
+inline libxsmm_datatype XsmmDtype<double>() {
+  return LIBXSMM_DATATYPE_F64;
+}
+template <>
+inline libxsmm_datatype XsmmDtype<uint16_t>() {
+  return LIBXSMM_DATATYPE_BF16;
+}
+template <>
+inline libxsmm_datatype XsmmDtype<bfloat16>() {
+  return LIBXSMM_DATATYPE_BF16;
+}
+
+inline void debug_print_eqn_tree(libxsmm_blasint eqn_no, bool print=false) {
+  if (print) {
+    libxsmm_meqn_tree_print(eqn_no);
+    libxsmm_meqn_rpn_print(eqn_no);
+  }
+}
+
+inline int meqn_push_arg(
+    const libxsmm_blasint idx,
+    const libxsmm_blasint m,
+    const libxsmm_blasint n,
+    const libxsmm_blasint ld,
+    const libxsmm_blasint in_pos,
+    const libxsmm_blasint offs_in_pos,
+    const libxsmm_datatype dtype) {
+  // This "singular" type dictates that the arg is a regular tensor (and not a
+  // set of tensors)
+  libxsmm_matrix_arg_attributes arg_singular_attr =
+      libxsmm_create_matrix_arg_attributes(
+          LIBXSMM_MATRIX_ARG_TYPE_SINGULAR,
+          LIBXSMM_MATRIX_ARG_SET_TYPE_NONE,
+          0,
+          0);
+  // Arg metadata include equation id and pos in arg array at runtime
+  libxsmm_meqn_arg_metadata arg_metadata =
+      libxsmm_create_meqn_arg_metadata(idx, in_pos);
+  libxsmm_meqn_arg_shape arg_shape =
+      libxsmm_create_meqn_arg_shape(m, n, ld, dtype);
+  return libxsmm_meqn_push_back_arg(
+      arg_metadata, arg_shape, arg_singular_attr);
+}
+
+inline libxsmm_meqn_function meqn_dispatch(
+    const libxsmm_blasint m,
+    const libxsmm_blasint n,
+    const libxsmm_blasint* ldo,
+    const libxsmm_datatype out_type,
+    const unsigned int idx) {
+  libxsmm_meqn_arg_shape arg_shape =
+      libxsmm_create_meqn_arg_shape(m, n, *ldo, out_type);
+  return libxsmm_dispatch_meqn(idx, arg_shape);
+}
+
+inline int meqn_push_unary_op(
+    const libxsmm_blasint idx,
+    const libxsmm_meltw_unary_type type,
+    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_UNARY_NONE,
+    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
+  // OP metadata include equation id and an integer dictating where the op
+  // metadata at runtime (if any) are located in the op arg array. -1 dictates
+  // there are no op metadata needed
+  libxsmm_meqn_op_metadata op_metadata =
+      libxsmm_create_meqn_op_metadata(idx, -1);
+  return libxsmm_meqn_push_back_unary_op(
+      op_metadata, type, dtype, flags);
+}
+inline int meqn_push_binary_op(
+    const libxsmm_blasint idx,
+    const libxsmm_meltw_binary_type type,
+    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_BINARY_NONE,
+    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
+  libxsmm_meqn_op_metadata op_metadata =
+      libxsmm_create_meqn_op_metadata(idx, -1);
+  return libxsmm_meqn_push_back_binary_op(
+      op_metadata, type, dtype, flags);
+}
+
+inline int meqn_push_ternary_op(
+    const libxsmm_blasint idx,
+    const libxsmm_meltw_ternary_type type,
+    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_TERNARY_NONE,
+    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
+  libxsmm_meqn_op_metadata op_metadata =
+      libxsmm_create_meqn_op_metadata(idx, -1);
+  return libxsmm_meqn_push_back_ternary_op(
+      op_metadata, type, dtype, flags);
+}
+
+class BaseTPP {
+ public:
+  void* get_kernel() {
+    auto& kernel_cache = get_kernel_cache();
+    void* kernel = NULL;
+    if (hash == "")
+      hash = hash_str();
+    auto search = kernel_cache.find(hash);
+    if (search != kernel_cache.end())
+      kernel = search->second;
+    if (kernel == NULL) {
+      kernel = build_kernel();
+      if (kernel == NULL) {
+        fprintf(stderr, "Unable to get JIT kernel for %s\n", hash.c_str());
+        exit(1);
+      }
+      //printf("TPP: %s @ %p\n", hash.c_str(), kernel);
+      kernel_cache[hash] = kernel;
+      //printf("Hash size = %ld\n", (long)kernel_cache.size());
+    }
+    return kernel;
+  }
+  // We should make hash_str() public
+  std::string get_hash_str() {
+    return hash_str();
+  }
+
+ protected:
+  std::unordered_map<std::string, void*>& get_kernel_cache() {
+    static std::unordered_map<std::string, void*> kernel_cache;
+    return kernel_cache;
+  }
+  virtual std::string hash_str() = 0;
+  virtual void* build_kernel() = 0;
+  std::string hash = "";
+  bool initialized = false;
+};
+
+class UnaryTPP : public BaseTPP {
+ public:
+  UnaryTPP() {}
+  UnaryTPP(
+      libxsmm_blasint rows,
+      libxsmm_blasint cols,
+      libxsmm_blasint ldi,
+      libxsmm_blasint ldo,
+      libxsmm_datatype dt_in,
+      libxsmm_datatype dt_out,
+      libxsmm_datatype dt_compute,
+      libxsmm_bitfield flags,
+      libxsmm_meltw_unary_type type)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        dt_in(dt_in),
+        dt_out(dt_out),
+        dt_compute(dt_compute),
+        flags(flags),
+        type(type) {
+    kernel = (libxsmm_meltwfunction_unary)get_kernel();
+    if (kernel)
+      initialized = true;
+  }
+
+  void operator()(void* in, void* out) {
+    if (!initialized)
+      return;
+    libxsmm_meltw_unary_param unary_param;
+    unary_param.in.primary = in;
+    unary_param.in.secondary = NULL;
+    unary_param.in.tertiary = NULL;
+    unary_param.in.quaternary = NULL;
+    unary_param.out.primary = out;
+    unary_param.out.secondary = NULL;
+    unary_param.out.tertiary = NULL;
+    unary_param.out.quaternary = NULL;
+    kernel(&unary_param);
+  }
+  void operator()(void* in, void* out, void* out2) {
+    if (!initialized)
+      return;
+    libxsmm_meltw_unary_param unary_param;
+    unary_param.in.primary = in;
+    unary_param.in.secondary = NULL;
+    unary_param.in.tertiary = NULL;
+    unary_param.in.quaternary = NULL;
+    unary_param.out.primary = out;
+    unary_param.out.secondary = out2;
+    unary_param.out.tertiary = NULL;
+    unary_param.out.quaternary = NULL;
+    kernel(&unary_param);
+  }
+  void operator()(void* in, void* in2, void* in3, void* out, void* out2) {
+    if (!initialized)
+      return;
+    libxsmm_meltw_unary_param unary_param;
+    unary_param.in.primary = in;
+    unary_param.in.secondary = in2;
+    unary_param.in.tertiary = in3;
+    unary_param.in.quaternary = NULL;
+    unary_param.out.primary = out;
+    unary_param.out.secondary = out2;
+    unary_param.out.tertiary = NULL;
+    unary_param.out.quaternary = NULL;
+    kernel(&unary_param);
+  }
+
+  void operator()(
+      void* in,
+      void* in2,
+      void* in3,
+      void* op,
+      void* op2,
+      void* op3,
+      void* out,
+      void* out2) {
+    if (!initialized)
+      return;
+    libxsmm_meltw_unary_param unary_param;
+    unary_param.in.primary = in;
+    unary_param.in.secondary = in2;
+    unary_param.in.tertiary = in3;
+    unary_param.in.quaternary = NULL;
+    unary_param.op.primary = op;
+    unary_param.op.secondary = op2;
+    unary_param.op.tertiary = op3;
+    unary_param.out.primary = out;
+    unary_param.out.secondary = out2;
+    unary_param.out.tertiary = NULL;
+    unary_param.out.quaternary = NULL;
+    kernel(&unary_param);
+  }
+
+ protected:
+  std::string hash_str() override {
+    char hash[200]={""};
+    snprintf(
+        hash,
+        200,
+        "unary_r%d_c%d_i%d_o%d_di%d_do%d_dc%d_f%d_t%d",
+        rows,
+        cols,
+        ldi,
+        ldo,
+        dt_in,
+        dt_out,
+        dt_compute,
+        flags,
+        type);
+    return std::string(hash);
+  }
+  void* build_kernel() override {
+    libxsmm_meltw_unary_shape shape = libxsmm_create_meltw_unary_shape(
+        cols, rows, ldi, ldo, dt_in, dt_out, dt_compute);
+    return (void*)libxsmm_dispatch_meltw_unary(type, shape, flags);
+  }
+
+  libxsmm_blasint rows = 0;
+  libxsmm_blasint cols = 0;
+  libxsmm_blasint ldi = 0;
+  libxsmm_blasint ldo = 0;
+  libxsmm_datatype dt_in = LIBXSMM_DATATYPE_F32;
+  libxsmm_datatype dt_out = LIBXSMM_DATATYPE_F32;
+  libxsmm_datatype dt_compute = LIBXSMM_DATATYPE_F32;
+  libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_UNARY_NONE;
+  libxsmm_meltw_unary_type type = LIBXSMM_MELTW_TYPE_UNARY_IDENTITY;
+  libxsmm_meltwfunction_unary kernel = NULL;
+};
+
+class BinaryTPP : public BaseTPP {
+ public:
+  BinaryTPP() {}
+  BinaryTPP(
+      libxsmm_blasint rows,
+      libxsmm_blasint cols,
+      libxsmm_blasint ldi,
+      libxsmm_blasint ldo,
+      libxsmm_datatype dt_in,
+      libxsmm_datatype dt_out,
+      libxsmm_datatype dt_compute,
+      libxsmm_bitfield flags,
+      libxsmm_meltw_binary_type type)
+      : BinaryTPP(
+            rows,
+            cols,
+            ldi,
+            ldi,
+            ldo,
+            dt_in,
+            dt_in,
+            dt_out,
+            dt_compute,
+            flags,
+            type) {}
+  BinaryTPP(
+      libxsmm_blasint rows,
+      libxsmm_blasint cols,
+      libxsmm_blasint ldi0,
+      libxsmm_blasint ldi1,
+      libxsmm_blasint ldo,
+      libxsmm_datatype dt_in0,
+      libxsmm_datatype dt_in1,
+      libxsmm_datatype dt_out,
+      libxsmm_datatype dt_compute,
+      libxsmm_bitfield flags,
+      libxsmm_meltw_binary_type type)
+      : rows(rows),
+        cols(cols),
+        ldi0(ldi0),
+        ldi1(ldi1),
+        ldo(ldo),
+        dt_in0(dt_in0),
+        dt_in1(dt_in1),
+        dt_out(dt_out),
+        dt_compute(dt_compute),
+        flags(flags),
+        type(type) {
+    kernel = (libxsmm_meltwfunction_binary)get_kernel();
+    if (kernel)
+      initialized = true;
+  }
+
+  void operator()(void* in0, void* in1, void* out) {
+    if (!initialized)
+      return;
+    libxsmm_meltw_binary_param binary_param;
+    binary_param.in0.primary = in0;
+    binary_param.in1.primary = in1;
+    binary_param.out.primary = out;
+    kernel(&binary_param);
+  }
+
+ protected:
+  std::string hash_str() override {
+    char hash[200];
+    snprintf(
+        hash,
+        200,
+        "binary_r%d_c%d_i0%d_i1%d_o%d_di0%d_di1%d_do%d_dc%d_f%d_t%d",
+        rows,
+        cols,
+        ldi0,
+        ldi1,
+        ldo,
+        dt_in0,
+        dt_in1,
+        dt_out,
+        dt_compute,
+        flags,
+        type);
+    return std::string(hash);
+  }
+  void* build_kernel() override {
+    libxsmm_meltw_binary_shape shape = libxsmm_create_meltw_binary_shape(
+        cols, rows, ldi0, ldi1, ldo, dt_in0, dt_in1, dt_out, dt_compute);
+    return (void*)libxsmm_dispatch_meltw_binary(type, shape, flags);
+  }
+
+  libxsmm_blasint rows = 0;
+  libxsmm_blasint cols = 0;
+  libxsmm_blasint ldi0;
+  libxsmm_blasint ldi1;
+  libxsmm_blasint ldo;
+  libxsmm_datatype dt_in0;
+  libxsmm_datatype dt_in1;
+  libxsmm_datatype dt_out;
+  libxsmm_datatype dt_compute;
+  libxsmm_bitfield flags;
+  libxsmm_meltw_binary_type type;
+  libxsmm_meltwfunction_binary kernel = NULL;
+};
+
+template <typename T>
+class SetZeroTPP {
+ public:
+  SetZeroTPP() {}
+  SetZeroTPP(int N) : SetZeroTPP(1, N) {}
+  SetZeroTPP(int rows, int cols) : SetZeroTPP(rows, cols, cols) {}
+  SetZeroTPP(int rows, int cols, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldo,
+            ldo,
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+            LIBXSMM_MELTW_TYPE_UNARY_XOR) {}
+  void operator()(T* buf) {
+    kernel((void*)buf, (void*)buf);
+  }
+  void ref(T* buf) {
+    for (int i = 0; i < rows; i++) {
+      for (int j = 0; j < cols; j++) {
+        buf[i * ldo + j] = 0;
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout>
+class ConvertTPP {
+ public:
+  ConvertTPP() {}
+  ConvertTPP(int N) : ConvertTPP(1, N) {}
+  ConvertTPP(int rows, int cols) : ConvertTPP(rows, cols, cols, cols) {}
+  ConvertTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
+                                                  : LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY),
+        init_done(true) {}
+  void operator()(Tin* in, Tout* out) {
+    if (!(XsmmDtype<Tin>() == LIBXSMM_DATATYPE_F32 &&
+          XsmmDtype<Tout>() == LIBXSMM_DATATYPE_F32) ||
+        ((void*)in != (void*)out))
+      kernel((void*)in, (void*)out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int i = 0; i < rows; i++) {
+      for (int j = 0; j < cols; j++) {
+        out[i * ldo + j] = (Tout)in[i * ldi + j];
+      }
+    }
+  }
+  bool initialized() {
+    return init_done;
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi = 0;
+  int ldo = 0;
+  UnaryTPP kernel;
+  bool init_done = false;
+};
+
+template <typename T>
+class CpyTPP {
+ public:
+  CpyTPP() {}
+  CpyTPP(int N) : CpyTPP(1, N) {}
+  CpyTPP(int rows, int cols) : CpyTPP(rows, cols, cols, cols) {}
+  CpyTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
+  void operator()(T* in, T* out) {
+    kernel((void*)in, (void*)out);
+  }
+  void ref(T* in, T* out) {
+    for (int i = 0; i < rows; i++) {
+      for (int j = 0; j < cols; j++) {
+        out[i * ldo + j] = in[i * ldi + j];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class CpyBcastColTPP {
+ public:
+  CpyBcastColTPP() {}
+  CpyBcastColTPP(int rows, int cols) : CpyBcastColTPP(rows, cols, cols) {}
+  CpyBcastColTPP(int rows, int cols, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            cols,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
+                                                  : LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_BCAST_COL,
+            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
+  void operator()(Tin* in, Tout* out) {
+    kernel((void*)in, (void*)out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int i = 0; i < rows; i++) {
+      for (int j = 0; j < cols; j++) {
+        out[i * ldo + j] = (Tout)in[j];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class CpyBcastRowTPP {
+ public:
+  CpyBcastRowTPP() {}
+  CpyBcastRowTPP(int rows, int cols) : CpyBcastRowTPP(rows, cols, cols) {}
+  CpyBcastRowTPP(int rows, int cols, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            1,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
+                                                  : LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_BCAST_ROW,
+            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
+  void operator()(Tin* in, Tout* out) {
+    kernel((void*)in, (void*)out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int i = 0; i < rows; i++) {
+      for (int j = 0; j < cols; j++) {
+        out[i * ldo + j] = (Tout)in[i];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class AddTPP {
+ public:
+  AddTPP() {}
+  AddTPP(int N) : AddTPP(1, N) {}
+  AddTPP(int rows, int cols) : AddTPP(rows, cols, cols, cols) {}
+  AddTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_ADD) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] + (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class SubTPP {
+ public:
+  SubTPP() {}
+  SubTPP(int N) : SubTPP(1, N) {}
+  SubTPP(int rows, int cols) : SubTPP(rows, cols, cols, cols) {}
+  SubTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_SUB) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] - (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class SubBcastTPP {
+ public:
+  SubBcastTPP() {}
+  SubBcastTPP(int N) : SubBcastTPP(1, N) {}
+  SubBcastTPP(int rows, int cols) : SubBcastTPP(rows, cols, cols, cols) {}
+  SubBcastTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_BCAST_COL_IN_1,
+            LIBXSMM_MELTW_TYPE_BINARY_SUB) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = in0[r * ldi + c] - in1[c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class MulTPP {
+ public:
+  MulTPP() {}
+  MulTPP(int N) : MulTPP(1, N) {}
+  MulTPP(int rows, int cols) : MulTPP(rows, cols, cols, cols) {}
+  MulTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] * (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename T>
+class MulBcastVecTPP {
+ public:
+  MulBcastVecTPP() {}
+  MulBcastVecTPP(int rows, int cols) : MulBcastVecTPP(rows, cols, cols) {}
+  MulBcastVecTPP(int rows, int cols, int ld)
+      : rows(rows),
+        cols(cols),
+        ld(ld),
+        kernel(
+            rows,
+            cols,
+            ld,
+            ld,
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_BCAST_COL_IN_0,
+            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
+
+  void operator()(T* in1, T* in2, T* out) {
+      kernel((void*)in1, (void*)in2, (void*)out);
+  }
+  void ref(T* in1, T* in2, T* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ld + c] = in1[c] * in2[r*ld+c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ld;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class DivTPP {
+ public:
+  DivTPP() {}
+  DivTPP(int N) : DivTPP(1, N) {}
+  DivTPP(int rows, int cols) : DivTPP(rows, cols, cols, cols) {}
+  DivTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_DIV) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] / (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class MaxTPP {
+ public:
+  MaxTPP() {}
+  MaxTPP(int N) : MaxTPP(1, N) {}
+  MaxTPP(int rows, int cols) : MaxTPP(rows, cols, cols, cols) {}
+  MaxTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_MAX) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] > (float)in1[r * ldi + c] ?
+          (float)in0[r * ldi + c] : (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class MinTPP {
+ public:
+  MinTPP() {}
+  MinTPP(int N) : MinTPP(1, N) {}
+  MinTPP(int rows, int cols) : MinTPP(rows, cols, cols, cols) {}
+  MinTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_MIN) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] < (float)in1[r * ldi + c] ?
+          (float)in0[r * ldi + c] : (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
+template <typename Tin, typename Tind, typename Tout = Tin>
+class ReduceMaxRowsIdxTPP {
+ public:
+  ReduceMaxRowsIdxTPP() {}
+  ReduceMaxRowsIdxTPP(int E)
+      : E(E),
+        reduce(
+            0,
+            E,
+            E,
+            E,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+	    (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES :
+	                         LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES) | 
+	    LIBXSMM_MELTW_FLAG_UNARY_REDUCE_RECORD_ARGOP|LIBXSMM_MELTW_FLAG_UNARY_REDUCE_INF_ACC,
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_MAX) {}
+  void operator()(Tin* in, Tind* idx, Tind n, Tout* out, Tind* argM) {
+    reduce((void*)in, (void*)idx, (void*)&n, (void*)out, (void*)argM);
+  }
+  void ref(Tin* in, Tind* idx, Tind n, Tout* out, Tind* argM) {
+    if(sizeof(Tin) == 4) {
+      for (int c = 0; c < E; c++) {
+	Tin max = 0xff800000;
+	Tind am;
+	for(int r=0; r<n; r++) {
+	  if((float)in[idx[r] * E + c] > max) {
+	    max = (float)in[idx[r] * E + c];
+	    am = r;
+	  }
+	}
+	out[c] = max;
+	argM[c] = am;
+      }
+    }
+    else if(sizeof(Tin) == 2) {
+      for (int c = 0; c < E; c++) {
+	Tin max = 0xFF80;
+	Tind am;
+	for(int r=0; r<n; r++) {
+	  if((bfloat16)in[idx[r] * E + c] > max) {
+	    max = (bfloat16)in[idx[r] * E + c];
+	    am = r;
+	  }
+	}
+	out[c] = max;
+	argM[c] = am;
+      }
+    }
+  }
+
+ private:
+  int E;
+  UnaryTPP reduce;
+};
+
+template <typename Tin, typename Tind, typename Tout = Tin>
+class ReduceMinRowsIdxTPP {
+ public:
+  ReduceMinRowsIdxTPP() {}
+  ReduceMinRowsIdxTPP(int E)
+      : E(E),
+        reduce(
+            0,
+            E,
+            E,
+            E,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+	    (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES : 
+	                         LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES) | 
+	    LIBXSMM_MELTW_FLAG_UNARY_REDUCE_RECORD_ARGOP|LIBXSMM_MELTW_FLAG_UNARY_REDUCE_INF_ACC,
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_MIN) {}
+  void operator()(Tin* in, Tind* idx, Tind n, Tout* out, Tind* argM) {
+    reduce((void*)in, (void*)idx, (void*)&n, (void*)out, (void*)argM);
+  }
+  void ref(Tin* in, Tind* idx, Tind n, Tout* out, Tind* argM) {
+    if(sizeof(Tin) == 4) {
+      for (int c = 0; c < E; c++) {
+	Tin min = 0x7f800000;
+	Tind am;
+	for(int r=0; r<n; r++) {
+	  if((float)in[idx[r] * E + c] < min) {
+	    min = (float)in[idx[r] * E + c];
+	    am = r;
+	  }
+	}
+	out[c] = min;
+	argM[c] = am;
+      }
+    }
+    else if(sizeof(Tin) == 2) {
+      for (int c = 0; c < E; c++) {
+	Tin min = 0x7F80;
+	Tind am;
+	for(int r=0; r<n; r++) {
+	  if((bfloat16)in[idx[r] * E + c] < min) {
+	    min = (bfloat16)in[idx[r] * E + c];
+	    am = r;
+	  }
+	}
+	out[c] = min;
+	argM[c] = am;
+      }
+    }
+  }
+
+ private:
+  int E;
+  UnaryTPP reduce;
+};
+
+template <typename T>
+class RecpTPP {
+ public:
+  RecpTPP() {}
+  RecpTPP(int cols) : RecpTPP(1, cols, cols, cols) {}
+  RecpTPP(int rows, int cols) : RecpTPP(rows, cols, cols, cols) {}
+  RecpTPP(int rows, int cols, int ldi) : RecpTPP(rows, cols, ldi, cols) {}
+  RecpTPP(int rows, int cols, int ldi, int ldo) 
+      : rows(rows),
+	cols(cols),
+	ldi(ldi),
+	ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<T>(),
+            XsmmDtype<T>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+            LIBXSMM_MELTW_TYPE_UNARY_RECIPROCAL) {}
+  void operator()(T* in, T* out) {
+    kernel((void*)in, (void*)out);
+  }
+  void ref(T* in, T* out) {
+    for (int r = 0; r < rows; r++)
+      for (int c = 0; c < cols; c++)
+	out[r*ldo+c] = 1.0 / in[r*ldi+c];
+  }
+
+ private:
+  int rows = 1;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tind, typename Tout=Tin>
+class GatherTPP {
+ public:
+  GatherTPP() {}
+  GatherTPP(int rows, int cols, int ldi)
+      : GatherTPP(rows, cols, ldi, ldi) {}
+  GatherTPP(int rows, int cols)
+      : GatherTPP(rows, cols, cols, cols) {}
+  GatherTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tout>(),
+            (LIBXSMM_MELTW_FLAG_UNARY_GS_COLS |
+             (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
+                                : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES)),
+            LIBXSMM_MELTW_TYPE_UNARY_GATHER) {}
+  void operator()(Tin* in0, Tind* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, NULL, (void*)out, NULL);
+  }
+  void ref(Tin* in0, Tind* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      auto ind = in1[r];
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = in0[ind * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi = 0;
+  int ldo = 0;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tind, typename Tout>
+class ScatterTPP {
+ public:
+  ScatterTPP() {}
+  ScatterTPP(int rows, int cols, int ldi) : ScatterTPP(rows, cols, ldi, ldi) {}
+  ScatterTPP(int rows, int cols) : ScatterTPP(rows, cols, cols, cols) {}
+  ScatterTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tout>(),
+            (LIBXSMM_MELTW_FLAG_UNARY_GS_COLS |
+             (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
+                                : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES)),
+            LIBXSMM_MELTW_TYPE_UNARY_SCATTER) {}
+  void operator()(Tin* in, Tind* out1, Tout* out) {
+    kernel((void*)in, NULL, NULL, (void*)out, (void*)out1);
+  }
+  void ref(Tin* in, Tind* out1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      auto ind = out1[r];
+      for (int c = 0; c < cols; c++) {
+        out[ind * ldo + c] = in[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi = 0;
+  int ldo = 0;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout = Tin>
+class ExpTPP {
+  public:
+    ExpTPP() {}
+    ExpTPP(int rows, int cols) : ExpTPP(rows, cols, cols, cols) {}
+    ExpTPP(int rows, int cols, int ldi) : ExpTPP(rows, cols, cols, ldi, ldi) {}
+    ExpTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+	ldi(ldi),
+	ldo(ldo),
+	kernel(
+	    rows,
+	    cols,
+	    ldi,
+	    ldo,
+	    XsmmDtype<Tin>(),
+	    XsmmDtype<Tout>(),
+	    LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+	    LIBXSMM_MELTW_TYPE_UNARY_EXP) {}
+
+  void operator()(Tin* in, Tout* out) {
+    kernel((void*)in, (void*)out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int r = 0; r < rows; r++)
+      for (int c = 0; c < cols; c++)
+	out[r*ldo + c] = std::exp((float)in[r*ldi + c]);
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tind, typename Tout = Tin>
+class ReduceAddRowsIdxTPP {
+ public:
+  ReduceAddRowsIdxTPP() {}
+  ReduceAddRowsIdxTPP(int E)
+      : E(E),
+        reduce(
+            0,
+            E,
+            E,
+            E,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES 
+		    : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES) |  
+	    LIBXSMM_MELTW_FLAG_UNARY_REDUCE_INIT_ACC,
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_ADD) {}
+  void operator()(Tin* in, Tind* idx, Tind n, Tout* out) {
+    reduce((void*)in, (void*)idx, (void*)&n, (void*)out, NULL);
+  }
+  void ref(Tin* in, Tind* idx, Tind n, Tout* out) {
+    for (int c = 0; c < E; c++) {
+      out[c] = 0;
+      for(int r=0; r<n; r++) {
+	out[c] += (float)in[idx[r] * E + c];
+      }
+    }
+  }
+
+ private:
+  int E;
+  UnaryTPP reduce;
+};
+
+template <typename Tin, typename Tout = Tin>
+class ReduceAddRowsTPP {
+ public:
+  ReduceAddRowsTPP() {}
+  ReduceAddRowsTPP(int rows, int cols) : ReduceAddRowsTPP(rows, cols, cols, cols) {}
+  ReduceAddRowsTPP(int rows, int cols, int ldi) : ReduceAddRowsTPP(rows, cols, ldi, cols) {}
+  ReduceAddRowsTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        reduce(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS | LIBXSMM_MELTW_FLAG_UNARY_REDUCE_INIT_ACC,
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD) {}
+  void operator()(Tin* in, Tout* out) {
+    reduce(in, out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int c = 0; c < cols; c++) {
+      for (int r = 0; r < rows; r++) {
+        if (c == 0)
+          out[r] = 0;
+        out[r] += in[c * ldi + r];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi, ldo;
+
+  UnaryTPP reduce;
+};
+
+template <typename Tin, typename Tout = Tin>
+class ReduceAddColsTPP {
+ public:
+  ReduceAddColsTPP() {}
+  ReduceAddColsTPP(int rows, int cols) : ReduceAddColsTPP(rows, cols, cols, cols) {}
+  ReduceAddColsTPP(int rows, int cols, int ldi) : ReduceAddColsTPP(rows, cols, ldi, cols) {}
+  ReduceAddColsTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        reduce(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD) {}
+  void operator()(Tin* in, Tout* out) {
+    reduce(in, out);
+  }
+  void ref(Tin* in, Tout* out) {
+    for (int c = 0; c < cols; c++) {
+      for (int r = 0; r < rows; r++) {
+        if (c == 0)
+          out[r] = 0;
+        out[r] += in[c * ldi + r];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi, ldo;
+
+  UnaryTPP reduce;
+};
+
+template <typename Tin, typename Tout, typename Tind>
+class EmbBagFwdTPP {
+ public:
+  EmbBagFwdTPP() {}
+  EmbBagFwdTPP(int E)
+      : E(E),
+        kernel(
+            0,
+            E,
+            E,
+            E,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            (libxsmm_meltw_unary_flags)(
+                sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
+                                  : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES),
+            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_ADD) {}
+  void operator()(Tout* output, Tin* weight, Tind* input, int N) {
+    unsigned long long _N = N;
+    kernel((void*)weight, (void*)input, (void*)&_N, (void*)output, NULL);
+  }
+  void ref(Tout* output, Tin* weight, Tind* input, int N) {
+    for (long v = 0; v < E; v++)
+      output[v] = 0;
+    for (long s = 0; s < N; s++) {
+      auto ind = input[s];
+      for (long v = 0; v < E; v++)
+        output[v] += weight[ind * E + v];
+    }
+  }
+
+ private:
+  int E;
+  UnaryTPP kernel;
+};
+
+template <typename Tin, typename Tout>
+class EmbBagBwdTPP {
+ public:
+  EmbBagBwdTPP() {}
+  EmbBagBwdTPP(int E)
+      : E(E),
+        kernel(
+            0,
+            E,
+            E,
+            E,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_MELTW_FLAG_UNARY_NONE,
+            LIBXSMM_MELTW_TYPE_UNARY_REPLICATE_COL_VAR) {}
+  void operator()(Tin* in, Tout* out, uint64_t N) {
+    kernel((void*)in, NULL, NULL, (void*)&N, NULL, NULL, (void*)out, NULL);
+  }
+  void ref(Tin* in, Tout* out, uint64_t N) {
+    for (uint64_t i = 0; i < N; i++) {
+      for (int v = 0; v < E; v++) {
+        out[i * E + v] = in[v];
+      }
+    }
+  }
+
+ private:
+  int E;
+  UnaryTPP kernel;
+};
+
+template <typename T1, typename T2 = T1, typename T3 = T1>
+class MulReduceTPP : public BaseTPP {
+ public:
+  MulReduceTPP() {}
+  MulReduceTPP(int rows, int cols) : rows(rows), cols(cols) {
+    kernel = (libxsmm_meqn_function)get_kernel();
+    initialized = true;
+  }
+
+  void operator()(T1* in0, T2* in1, T3* out) {
+    if (!initialized)
+      return;
+    libxsmm_meqn_param eqn_param;
+    libxsmm_matrix_arg arg_array[2];
+    arg_array[0].primary = (void*)in0;
+    arg_array[1].primary = (void*)in1;
+    eqn_param.inputs = arg_array;
+    eqn_param.output.primary = (void*)out;
+
+    kernel(&eqn_param);
+  }
+
+  void ref(T1* in0, T2* in1, T3* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r] += (float)in0[r * cols + c] * (float)in1[r * cols + c];
+      }
+    }
+  }
+
+ protected:
+  std::string hash_str() override {
+    char hash[200];
+    snprintf(
+        hash,
+        200,
+        "mul_reduce_eqn_t%d_%d_%d_r%d_c%d",
+        XsmmDtype<T1>(),
+        XsmmDtype<T2>(),
+        XsmmDtype<T3>(),
+        rows, 
+        cols);
+    return std::string(hash);
+  }
+  void* build_kernel() override {
+    auto dt1 = XsmmDtype<T1>();
+    auto dt2 = XsmmDtype<T2>();
+    auto dt3 = XsmmDtype<T3>();
+    libxsmm_blasint ld = 1;
+    libxsmm_blasint my_eqn0 = libxsmm_meqn_create();
+    meqn_push_unary_op(
+        my_eqn0,
+        LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
+        LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
+        LIBXSMM_DATATYPE_F32);
+    meqn_push_binary_op(
+        my_eqn0,
+        LIBXSMM_MELTW_TYPE_BINARY_MUL,
+        LIBXSMM_MELTW_FLAG_BINARY_NONE,
+        LIBXSMM_DATATYPE_F32);
+    meqn_push_arg(my_eqn0, cols, rows, cols, 0, 0, dt1);
+    meqn_push_arg(my_eqn0, cols, rows, cols, 1, 0, dt2);
+    debug_print_eqn_tree(my_eqn0);
+    return (void*)meqn_dispatch(1, rows, &ld, dt3, my_eqn0);
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  libxsmm_meqn_function kernel = NULL;
+};
+
+}; // namespace dgl_tpp
+
+#endif // _XSMM_FUNCTORS_H_
diff --git a/src/runtime/ndarray.cc b/src/runtime/ndarray.cc
index abbe3b13..cf92fb9c 100644
--- a/src/runtime/ndarray.cc
+++ b/src/runtime/ndarray.cc
@@ -23,6 +23,7 @@ constexpr DGLDataType DGLDataTypeTraits<int32_t>::dtype;
 constexpr DGLDataType DGLDataTypeTraits<int64_t>::dtype;
 constexpr DGLDataType DGLDataTypeTraits<uint32_t>::dtype;
 constexpr DGLDataType DGLDataTypeTraits<uint64_t>::dtype;
+constexpr DGLDataType DGLDataTypeTraits<bfloat16>::dtype;
 #ifdef DGL_USE_CUDA
 constexpr DGLDataType DGLDataTypeTraits<__half>::dtype;
 #if BF16_ENABLED
@@ -311,6 +312,8 @@ template NDArray NDArray::FromVector<uint32_t>(
     const std::vector<uint32_t>&, DGLContext);
 template NDArray NDArray::FromVector<uint64_t>(
     const std::vector<uint64_t>&, DGLContext);
+template NDArray NDArray::FromVector<bfloat16>(
+    const std::vector<bfloat16>&, DGLContext);
 template NDArray NDArray::FromVector<float>(
     const std::vector<float>&, DGLContext);
 template NDArray NDArray::FromVector<double>(
@@ -334,8 +337,10 @@ std::vector<T> NDArray::ToVector() const {
 
 template std::vector<int32_t> NDArray::ToVector<int32_t>() const;
 template std::vector<int64_t> NDArray::ToVector<int64_t>() const;
+template std::vector<uint16_t> NDArray::ToVector<uint16_t>() const;
 template std::vector<uint32_t> NDArray::ToVector<uint32_t>() const;
 template std::vector<uint64_t> NDArray::ToVector<uint64_t>() const;
+template std::vector<bfloat16> NDArray::ToVector<bfloat16>() const;
 template std::vector<float> NDArray::ToVector<float>() const;
 template std::vector<double> NDArray::ToVector<double>() const;
 
