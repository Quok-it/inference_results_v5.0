# syntax = docker/dockerfile:experimental
# based onhttps://github.com/pytorch/pytorch/blob/master/Dockerfile
# 
# NOTE: To build this you will need a docker version > 18.06 with
#       experimental enabled and DOCKER_BUILDKIT=1
#
#       If you do not use buildkit you are not going to have a good time
#
#       For reference: 
#           https://docs.docker.com/develop/develop-images/build_enhancements/

ARG BASE_IMAGE=intel/intel-optimized-pytorch:mlperf-inference-4.1-retinanet
FROM ${BASE_IMAGE} AS dev-base
RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get autoclean && \
    apt-get autoremove -y && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    libnuma-dev \
    bc \
    && rm -rf /var/lib/apt/lists/*

#    numactl \

RUN echo "alias ll='ls -l'" >> /root/.bashrc
ENV PATH /opt/conda/bin:$PATH
RUN python -m pip install cmake==3.27.0

#intel-openmp==2024.2.1
#    /opt/conda/bin/conda install -c conda-forge mkl=2022.0.1 mkl-include=2022.0.1 -y && \
#    /opt/conda/bin/conda install -c conda-forge llvm-openmp=16.0.6 jemalloc=5.2.1 ninja==1.11.1 -y && \

RUN cp -r /workspace/retinanet-env /tmp/
#RUN cp -r /workspace/src/ckernels/3rdparty /tmp/
RUN rm -rf /workspace/*
COPY . /workspace
RUN mkdir -p /workspace/retinanet-env; cp -r /tmp/retinanet-env/* /workspace/retinanet-env/
#RUN mkdir -p /workspace/src/ckernels/3rdparty; cp -r /tmp/3rdparty/* /workspace/src/ckernels/3rdparty/

#ARG IPEX_VERSION=mlperf/retinanet
ENV CONDA_PREFIX "/opt/conda"
WORKDIR /workspace
#COPY . /workspace
RUN export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"} && \
    export CC=`which gcc` && export CXX=`which g++` && \
    rm -rf retinanet-env/mlperf_inference && \
    cd retinanet-env && \
    export IPEX_PATH=${PWD}/ipex-cpu-dev/build/Release/packages/intel_extension_for_pytorch && \
    git clone https://github.com/mlcommons/inference.git mlperf_inference && cd mlperf_inference && \
    git config user.email "test@example.com" && \
    git checkout master && \
    cd loadgen && mkdir build && cd build && cmake .. && make && cd .. && \
    CFLAGS="-std=c++14" python setup.py install && export LOADGEN_DIR=${PWD} && cd ../.. && \
    export OPENCV_DIR=${PWD}/opencv/build && \
    export RAPIDJSON_INCLUDE_DIR=${PWD}/rapidjson/include && \
    export GFLAGS_DIR=${PWD}/gflags/build && \
    export LIBRARY_PATH=${CONDA_PREFIX}/lib:${LIBRARY_PATH} && \
    rm -rf /opt/conda/lib/cmake/mkl && \
    export TORCH_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && \
    cd .. && \
    cmake -DCMAKE_PREFIX_PATH=${TORCH_PATH} \
        -DLOADGEN_DIR=${LOADGEN_DIR} \
        -DOpenCV_DIR=${OPENCV_DIR} \
        -DRapidJSON_INCLUDE_DIR=${RAPIDJSON_INCLUDE_DIR} \
        -Dgflags_DIR=${GFLAGS_DIR} \
        -DINTEL_EXTENSION_FOR_PYTORCH_PATH=${IPEX_PATH} \
        -B${PWD}/build \
        -H${PWD}/src && \
    cmake --build ${PWD}/build --config Release

#RUN rm -rf inference; git clone https://github.com/mlcommons/inference.git inference
RUN for FILE in $(cat /workspace/redactions.txt); do rm -rf /workspace/${FILE}; rm -rf /workspace/workload_code/${FILE}; done

ENV MALLOC_CONF "oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000"
ENV LD_PRELOAD "/opt/conda/lib/libjemalloc.so":"/opt/conda/lib/libiomp5.so":${LD_PRELOAD}
ENV CONDA_PREFIX "/opt/conda"
ENV MODEL_DIR=/model
ENV DATA_DIR=/data
