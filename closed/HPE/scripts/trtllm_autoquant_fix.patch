From f6cc9c9c79a617d5a102d4add0898b0a9ca69a94 Mon Sep 17 00:00:00 2001
From: Shengliang Xu <shengliangx@nvidia.com>
Date: Mon, 27 Jan 2025 23:08:17 +0000
Subject: [PATCH] fp4 mixed precision fixes

---
 tensorrt_llm/models/modeling_utils.py | 48 ++++++++++++++++++---------
 tensorrt_llm/quantization/quantize.py | 45 ++++++++++++++-----------
 2 files changed, 59 insertions(+), 34 deletions(-)

diff --git a/tensorrt_llm/models/modeling_utils.py b/tensorrt_llm/models/modeling_utils.py
index 5f36c326d6..dddf47e5cb 100644
--- a/tensorrt_llm/models/modeling_utils.py
+++ b/tensorrt_llm/models/modeling_utils.py
@@ -5,6 +5,7 @@ import json
 import os
 import re
 from enum import IntFlag, auto
+from fnmatch import fnmatch
 from functools import cached_property
 from pathlib import Path
 from typing import (TYPE_CHECKING, Callable, Dict, Generator, List, Optional,
@@ -208,19 +209,16 @@ class LayerQuantConfig(QuantConfig):
     quant_algo: Optional[QuantConfig] = None
     kv_cache_quant_algo: Optional[QuantConfig] = None
     quantized_layers: Optional[Dict[str, QuantConfig]] = None
-    exclude_modules: Optional[List[str]] = None

     def __init__(self,
                  *,
                  quant_algo: Optional[QuantConfig] = None,
                  kv_cache_quant_algo: Optional[QuantConfig] = None,
                  quantized_layers: Optional[Dict[str, QuantConfig]] = None,
-                 exclude_modules: Optional[List[str]] = None,
                  **kwargs):
         self.quant_algo = quant_algo
         self.quantized_layers = quantized_layers
         self.kv_cache_quant_algo = kv_cache_quant_algo
-        self.exclude_modules = exclude_modules
         self.auto_quant_mode = {}
         for name, layer_config in self.quantized_layers.items():
             self.auto_quant_mode.update({
@@ -240,9 +238,14 @@ class LayerQuantConfig(QuantConfig):
         quant_mode_list = list(set(self.auto_quant_mode.values()))
         return QuantModeWrapper(quant_mode_list)

-    @property
-    def layer_quant_mode(self) -> Dict[str, QuantMode]:
-        return self.auto_quant_mode
+    #@lru_cache(maxsize=None)
+    def layer_quant_mode(self, layer_name) -> QuantMode:
+
+        for name, quant_mode in self.auto_quant_mode.items():
+            if fnmatch(layer_name, name):
+                return quant_mode
+
+        return QuantMode(0)

     @cached_property
     def auto_quant_list(self):
@@ -263,11 +266,15 @@ class LayerQuantConfig(QuantConfig):
         obj = cls(quantized_layers=quantized_layers_dict, **config)
         return obj

+    #@lru_cache(maxsize=None)
     def get_quant_cfg(self, module_name):
-        if module_name in self.quantized_layers.keys():
-            return self.quantized_layers[module_name]
-        else:
-            return QuantConfig()
+        quant_res = QuantConfig()
+
+        for name, quant_cfg in self.quantized_layers.items():
+            if fnmatch(module_name, name):
+                quant_res = quant_cfg
+                break
+        return quant_res

     def get_modelopt_qformat(self):
         algo_to_modelopt_map = {
@@ -286,10 +293,8 @@ class LayerQuantConfig(QuantConfig):
         output = copy.deepcopy(self.__dict__)
         output.pop('auto_quant_mode', None)
         output.pop('quant_mode', None)
-        output.pop('exclude_modules', None)
         for name, per_layer_config in output['quantized_layers'].items():
             per_layer_config = per_layer_config.to_dict()
-            per_layer_config.pop('exclude_modules')
             output['quantized_layers'][name] = per_layer_config
         return output

@@ -454,6 +459,18 @@ class PretrainedConfig:
     def to_layer_quant_config(self, config_file: str):
         with open(config_file) as f:
             config = json.load(f)
+
+        if self.architecture == "MixtralForCausalLM":
+            for layer_name in list(config["quantized_layers"].keys()):
+                quant_cfg = config["quantized_layers"][layer_name]
+                if "mlp.fc" in layer_name or "mlp.proj" in layer_name:
+                    moe_name, _ = layer_name.rsplit('.', 1)
+                    if moe_name not in config["quantized_layers"]:
+                        config["quantized_layers"][moe_name] = quant_cfg
+                    else:
+                        assert quant_cfg == config["quantized_layers"][
+                            moe_name], "MoE module needs to have the same quantization format for non-rounter sub-modules"
+
         self.quantization = LayerQuantConfig.from_dict(config)

     @property
@@ -1746,11 +1763,12 @@ def preprocess_weights(weights: Dict[str, torch.Tensor],
         if quant_algo != QuantAlgo.MIXED_PRECISION:
             layer_quant_algo = quant_algo
         else:
-            if base_name not in quant_config.quantized_layers.keys():
+            quant_cfg = quant_config.get_quant_cfg(base_name)
+            if not quant_cfg.quant_algo:
                 new_weights.update(layer_weights)
                 continue
-            layer_quant_algo = quant_config.quantized_layers[
-                base_name].quant_algo
+
+            layer_quant_algo = quant_cfg.quant_algo

         preprocess_perlayer_weights(layer_weights, model_config,
                                     layer_quant_algo, from_pruned)
diff --git a/tensorrt_llm/quantization/quantize.py b/tensorrt_llm/quantization/quantize.py
index 149c39d488..11ee567ee1 100644
--- a/tensorrt_llm/quantization/quantize.py
+++ b/tensorrt_llm/quantization/quantize.py
@@ -40,21 +40,31 @@ def quantize_layers(
     for name, module, parent in model.named_modules_with_parent():
         module_name = name.rsplit('.', 1)[-1]
         is_excluded = False
+
+        # handle exclusion
         for exclude_module in exclude_modules:
             if fnmatch.fnmatchcase(name, exclude_module):
                 is_excluded = True
-                # MOE module will be quantize when initialization.
-                # We need to re-initialize a FP version of MOE module.
-                if isinstance(module, MixtureOfExperts):
-                    init_params = get_init_params(module, MixtureOfExperts)
-                    init_params["quant_mode"] = QuantMode(0)
-                    original_layer = MixtureOfExperts(**init_params)
-                    if parent is not None:
-                        setattr(parent, module_name, original_layer)
-                    else:
-                        model = original_layer
                 break
-        if not is_excluded:
+
+        # MOE module will be quantize when initialization.
+        # We need to handle it specially, we may want to redesign MoE implementation
+        if isinstance(module, MixtureOfExperts):
+            # We need to re-initialize a correct version of MOE module.
+            if is_excluded:
+                quant_mode = QuantMode(0)
+            else:
+                quant_mode = quant_config.quant_mode
+
+            init_params = get_init_params(module, MixtureOfExperts)
+            init_params["quant_mode"] = quant_mode
+
+            original_layer = MixtureOfExperts(**init_params)
+            if parent is not None:
+                setattr(parent, module_name, original_layer)
+            else:
+                model = original_layer
+        elif not is_excluded:
             quant_cls = None
             for cls in quant_map:
                 if isinstance(module, cls):
@@ -546,16 +556,13 @@ def kv_cache_quantize(model):


 def quantize(model, quant_config: Union[QuantConfig, LayerQuantConfig]):
-    quant_mode = quant_config.layer_quant_mode

     for name, module, parent in model.named_modules_with_parent():
+
         if quant_config.quant_algo == QuantAlgo.MIXED_PRECISION:
-            if name in quant_mode.keys():
-                layer_quant_mode = quant_mode[name]
-            else:
-                continue
+            layer_quant_mode = quant_config.layer_quant_mode(name)
         else:
-            layer_quant_mode = quant_mode
+            layer_quant_mode = quant_config.layer_quant_mode
         if layer_quant_mode == QuantMode(0):
             continue

@@ -566,9 +573,9 @@ def quantize(model, quant_config: Union[QuantConfig, LayerQuantConfig]):
         elif layer_quant_mode.has_fp8_rowwise():
             module = fp8_rowwise_quantize(module, layer_quant_cfg)
         elif layer_quant_mode.is_qserve_w4a8():
-            model = qserve_quantize(model, quant_config)
+            module = qserve_quantize(module, quant_config)
         elif layer_quant_mode.has_nvfp4():
-            model = fp4_quantize(model, layer_quant_cfg)
+            module = fp4_quantize(module, layer_quant_cfg)
         elif layer_quant_mode.has_act_and_weight_quant():
             module = smooth_quantize(module, layer_quant_cfg)
         elif layer_quant_mode.is_weight_only():
--
GitLab
